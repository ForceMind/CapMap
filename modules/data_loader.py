import pandas as pd
import akshare as ak
import os
import streamlit as st
import concurrent.futures
import threading
from datetime import datetime, timedelta
import random
import time

from .config import STOCK_POOLS
from .utils import with_retry, get_start_date, add_script_run_ctx, get_script_run_ctx

def fetch_history_data(pool_name="æ²ªæ·±300 (å¤§ç›˜)"):
    """
    è·å–æŒ‡å®šæˆåˆ†è‚¡è¿‡å»2å¹´çš„æ—¥çº¿æ•°æ®ã€‚
    é€»è¾‘å¤åˆ»è‡ª app1.py (ç¨³å®šç‰ˆ)ï¼Œæ”¯æŒå¤šæŒ‡æ•°æ± ã€‚
    """
    config = STOCK_POOLS.get(pool_name, STOCK_POOLS["æ²ªæ·±300 (å¤§ç›˜)"])
    cache_file = config["cache"]
    index_code = config["code"]

    cached_df = pd.DataFrame()
    last_cached_date = None

    # 1. å°è¯•åŠ è½½æœ¬åœ°ç¼“å­˜
    if os.path.exists(cache_file):
        try:
            cached_df = pd.read_parquet(cache_file)
            if not cached_df.empty:
                last_cached_date = cached_df['æ—¥æœŸ'].max().date()
                st.toast(f"âœ… å·²åŠ è½½æœ¬åœ°ç¼“å­˜ [{pool_name}]ï¼Œæœ€æ–°æ—¥æœŸ: {last_cached_date}")
        except Exception as e:
            st.error(f"è¯»å–ç¼“å­˜æ–‡ä»¶å¤±è´¥: {e}")

    # 2. è®¡ç®—éœ€è¦ä¸‹è½½çš„æ—¶é—´èŒƒå›´
    today = datetime.now().date()
    
    if last_cached_date:
        if last_cached_date >= today:
             return cached_df
        start_date_str = (last_cached_date + timedelta(days=1)).strftime("%Y%m%d")
    else:
        start_date_str = get_start_date(2)
        
    end_date_str = today.strftime("%Y%m%d")

    # å¦‚æœä¸éœ€è¦æ›´æ–°
    if start_date_str > end_date_str:
        return cached_df

    # çŠ¶æ€å®¹å™¨
    status_text = st.empty()
    progress_bar = st.progress(0)
    
    try:
        # å¦‚æœæ˜¯å¢é‡æ›´æ–°
        is_incremental = not cached_df.empty
        if not is_incremental:
            status_text.text(f"æ­£åœ¨åˆå§‹åŒ– [{pool_name}] å†å²æ•°æ®...")
        else:
            status_text.text(f"æ­£åœ¨æ£€æŸ¥å¢é‡æ•°æ® ({start_date_str} - {end_date_str})...")

        # è·å–æˆåˆ†è‚¡åˆ—è¡¨
        status_text.text(f"æ­£åœ¨è·å– [{pool_name}] æˆåˆ†è‚¡åˆ—è¡¨...")
        try:
            # å¢åŠ é‡è¯•
            cons_df = with_retry(lambda: ak.index_stock_cons(symbol=index_code), retries=5, delay=2.0)
        except:
             if not cached_df.empty:
                 st.warning("æˆåˆ†è‚¡åˆ—è¡¨è·å–å¤±è´¥ (ç½‘ç»œåŸå› )ï¼Œä½¿ç”¨ç¼“å­˜æ•°æ®")
                 return cached_df
             return pd.DataFrame()
        
        if cons_df is None or cons_df.empty:
             st.warning(f"æ— æ³•è·å– [{pool_name}] æˆåˆ†è‚¡åˆ—è¡¨ (å¯èƒ½æ˜¯ AkShare æ¥å£å˜åŠ¨æˆ–ç½‘ç»œè¶…æ—¶)")
             return cached_df if not cached_df.empty else pd.DataFrame()

        if 'variety' in cons_df.columns:
            code_col, name_col = 'variety', 'name'
        elif 'å“ç§ä»£ç ' in cons_df.columns:
            code_col, name_col = 'å“ç§ä»£ç ', 'å“ç§åç§°'
        else:
            code_col = cons_df.columns[0]
            name_col = cons_df.columns[1]
            
        # å¼ºè½¬ String
        stock_list = cons_df[code_col].apply(str).tolist() 
        stock_names = dict(zip(stock_list, cons_df[name_col]))
        
        # --- å°è¯•è·å–ä»Šæ—¥å®æ—¶æ•°æ® (Spot) ---
        today_spot_map = {}
        try:
            # Low frequency
            spot_df = ak.stock_zh_a_spot_em()
            if spot_df is not None and not spot_df.empty:
                spot_df['ä»£ç '] = spot_df['ä»£ç '].astype(str)
                
                # 1. æ›´æ–°åç§°æ˜ å°„
                new_names = dict(zip(spot_df['ä»£ç '], spot_df['åç§°']))
                stock_names.update(new_names)
                
                # 2. å‡†å¤‡ä»Šæ—¥æ•°æ®æ˜ å°„
                if end_date_str >= start_date_str:
                    today_spot_map = spot_df.set_index('ä»£ç ').to_dict('index')
        except Exception as e:
            # éè‡´å‘½é”™è¯¯
            print(f"Update spots failed: {e}")

        new_data_list = []
        total_stocks = len(stock_list)

        # å¾ªç¯è·å–å†å²
        def fetch_one_stock(code, name):
            try:
                # è·å–æ—¥çº¿
                df_hist = ak.stock_zh_a_hist(symbol=code, start_date=start_date_str, end_date=end_date_str, adjust="qfq")
                
                # æ£€æŸ¥æ˜¯å¦åŒ…å«ä»Šå¤©
                fetched_today = False
                if df_hist is not None and not df_hist.empty:
                    df_hist['æ—¥æœŸ'] = pd.to_datetime(df_hist['æ—¥æœŸ'])
                    if end_date_str in df_hist['æ—¥æœŸ'].dt.strftime("%Y%m%d").values:
                        fetched_today = True
                else:
                    df_hist = pd.DataFrame()

                # è¡¥å…¨ä»Šå¤©
                if (not fetched_today) and (end_date_str == datetime.now().strftime("%Y%m%d")):
                    if code in today_spot_map:
                        row = today_spot_map[code]
                        try:
                             new_row = pd.DataFrame([{
                                 'æ—¥æœŸ': pd.to_datetime(end_date_str),
                                 'æ”¶ç›˜': row['æœ€æ–°ä»·'],
                                 'æ¶¨è·Œå¹…': row['æ¶¨è·Œå¹…'],
                                 'æˆäº¤é¢': row['æˆäº¤é¢'],
                                 'ä»£ç ': code,
                                 'åç§°': name
                             }])
                             df_hist = pd.concat([df_hist, new_row], ignore_index=True)
                        except:
                            pass
                
                if df_hist is not None and not df_hist.empty:
                    # ç¡®ä¿åˆ—å­˜åœ¨
                    cols_needed = ['æ—¥æœŸ', 'æ”¶ç›˜', 'æ¶¨è·Œå¹…', 'æˆäº¤é¢']
                    for c in cols_needed:
                        if c not in df_hist.columns: return None
                    
                    df_hist = df_hist[cols_needed].copy()
                    df_hist['ä»£ç '] = code
                    df_hist['åç§°'] = name
                    return df_hist
            except Exception:
                pass
            return None

        # Use concurrency as in app1.py
        ctx = get_script_run_ctx()
        def fetch_one_stock_wrapper(code, name):
            if ctx:
                add_script_run_ctx(threading.current_thread(), ctx)
            return fetch_one_stock(code, name)

        # æ¢å¤ app1.py çš„ max_workers=10
        with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
             future_map = {executor.submit(fetch_one_stock_wrapper, c, stock_names.get(c, c)): c for c in stock_list}
             
             for i, future in enumerate(concurrent.futures.as_completed(future_map)):
                 # Update progress
                 if i % 10 == 0:
                     progress_bar.progress((i + 1) / total_stocks)
                     status_text.text(f"æ­£åœ¨åŒæ­¥æ•°æ® [{pool_name}]: {i+1}/{total_stocks}")
                 
                 res = future.result()
                 if res is not None:
                     new_data_list.append(res)
                
        status_text.empty()
        progress_bar.empty()
        
        # åˆå¹¶é€»è¾‘
        if new_data_list:
            new_df = pd.concat(new_data_list, ignore_index=True)
            # ç±»å‹è½¬æ¢
            new_df['æ—¥æœŸ'] = pd.to_datetime(new_df['æ—¥æœŸ'])
            new_df['æ¶¨è·Œå¹…'] = pd.to_numeric(new_df['æ¶¨è·Œå¹…'], errors='coerce')
            new_df['æˆäº¤é¢'] = pd.to_numeric(new_df['æˆäº¤é¢'], errors='coerce')
            new_df['æ”¶ç›˜'] = pd.to_numeric(new_df['æ”¶ç›˜'], errors='coerce')
            
            if cached_df.empty:
                final_df = new_df
            else:
                # åˆå¹¶æ—§æ•°æ®å’Œæ–°æ•°æ®ï¼Œå¹¶å»é‡
                st.toast(f"ğŸ“¥ æˆåŠŸè·å– {len(new_df)} æ¡æ–°è®°å½• ({pool_name})")
                final_df = pd.concat([cached_df, new_df], ignore_index=True)
                final_df.drop_duplicates(subset=['æ—¥æœŸ', 'ä»£ç '], keep='last', inplace=True)
        else:
            final_df = cached_df
            
        if final_df.empty:
            return pd.DataFrame()

        final_df = final_df.sort_values('æ—¥æœŸ')
        
        # ä½¿ç”¨æœ€æ–°çš„ stock_names æ›´æ–° DataFrame ä¸­çš„åç§°åˆ—
        if final_df is not None and not final_df.empty:
            final_df['åç§°'] = final_df['ä»£ç '].map(stock_names).fillna(final_df['åç§°'])
        
        # ä¿å­˜ç¼“å­˜
        if new_data_list or cached_df.empty:
            try:
                if not os.path.exists("data"):
                    os.makedirs("data")
                final_df.to_parquet(cache_file)
                if not cached_df.empty:
                    st.toast(f"ğŸ’¾ [{pool_name}] å¢é‡æ•°æ®å·²åˆå¹¶å¹¶ä¿å­˜")
                else:
                    st.success(f"ğŸ’¾ [{pool_name}] å…¨é‡æ•°æ®å·²åˆå§‹åŒ–")
            except Exception as e:
                st.warning(f"æ— æ³•ä¿å­˜ç¼“å­˜: {e}")

        return final_df

    except Exception as e:
        status_text.empty()
        progress_bar.empty()
        st.error(f"å…¨å±€æ•°æ®é”™è¯¯: {e}")
        return pd.DataFrame()

@st.cache_data(ttl=3600*24, show_spinner=False)
def fetch_cached_min_data(symbol, date_str, is_index=False, period='1'):
    """
    åŸå­åŒ–è·å–å•ä¸ªæ ‡çš„çš„åˆ†æ—¶æ•°æ®ï¼Œç‹¬ç«‹ç¼“å­˜ã€‚
    """
    start_time = f"{date_str} 09:30:00"
    end_time = f"{date_str} 15:00:00"
    
    # æŒ‡æ•°é€€é¿ç­–ç•¥å…¨å±€å˜é‡ (ç®€å•æ¨¡æ‹Ÿ)
    if not hasattr(fetch_cached_min_data, "current_backoff"):
        fetch_cached_min_data.current_backoff = 0
            
    # ç®€å•çš„é‡è¯•æœºåˆ¶
    max_retries = 3
    
    for attempt in range(max_retries):
        try:
            if is_index:
                df = ak.index_zh_a_hist_min_em(symbol=symbol, period=period, start_date=start_time, end_date=end_time)
            else:
                df = ak.stock_zh_a_hist_min_em(symbol=symbol, start_date=start_time, end_date=end_time, period=period, adjust='qfq')
            
            if df is not None and not df.empty:
                # ç»Ÿä¸€åˆ—å
                if 'æ—¶é—´' in df.columns:
                    df.rename(columns={'æ—¶é—´': 'time', 'å¼€ç›˜': 'open', 'æ”¶ç›˜': 'close'}, inplace=True)
                return df
                
        except Exception:
            time.sleep(1)
            
    return pd.DataFrame()

# --- åå°é¢„å–çº¿ç¨‹é€»è¾‘ ---
@st.cache_data(ttl=3600*24, show_spinner=False)
def fetch_cached_min_data_wrapper(symbol, date_str, is_index=False, period='1'):
    """Wrapper to be called by background thread"""
    # This is just a direct call to the cached function
    # In background thread, we can call this.
    return fetch_cached_min_data(symbol, date_str, is_index, period)

def background_prefetch_task(date_list, origin_df):
    """
    åå°çº¿ç¨‹ï¼šæ‰§è¡Œæ•°æ®é¢„å–ã€‚
    """
    total_dates = len(date_list)
    print(f"\n[åå°ä»»åŠ¡] å¼€å§‹é¢„å– {total_dates} å¤©çš„æ•°æ®ã€‚")
    
    current_backoff = 0 # ç§’
    
    indices_codes = ["000300", "000001", "399001"]
    
    for i, d in enumerate(date_list):
        d_str = d.strftime("%Y-%m-%d")
        print(f"[åå°ä»»åŠ¡] æ­£åœ¨å¤„ç†: {d_str} ({i+1}/{total_dates})")
        
        # ç­›é€‰
        daily = origin_df[origin_df['æ—¥æœŸ'].dt.date == d]
        if daily.empty: continue
        
        # Top 25
        top_stocks = daily.sort_values('æˆäº¤é¢', ascending=False).head(25)['ä»£ç '].tolist()
        
        # ä»»åŠ¡åˆ—è¡¨
        tasks = []
        for code in indices_codes: tasks.append((code, d_str, True))
        for code in top_stocks: tasks.append((code, d_str, False))
        
        # å†…å±‚é€ä¸ªæ‰§è¡Œ (ä¸ºäº†æ–¹ä¾¿æ§åˆ¶é€€é¿ï¼Œä¸”åå°ä»»åŠ¡ä¸æ€¥äºä¸€æ—¶çš„å¹¶å‘ï¼Œç¨³å®šç¬¬ä¸€)
        for t_code, t_date, t_is_index in tasks:
            
            # Indefinite retry loop with backoff
            while True:
                try:
                    # æ£€æŸ¥é€€é¿
                    if current_backoff > 0:
                        print(f"[åå°ä»»åŠ¡] å¤„äºå†·å´çŠ¶æ€ã€‚ç­‰å¾… {current_backoff} ç§’...")
                        time.sleep(current_backoff)
                        
                    fetch_cached_min_data(t_code, t_date, is_index=t_is_index, period='1')
                    
                    # Success
                    if current_backoff > 0:
                        print(f"[åå°ä»»åŠ¡] å·²æ¢å¤ã€‚é‡ç½®é€€é¿æ—¶é—´ã€‚")
                        current_backoff = 0
                    
                    time.sleep(0.1)
                    break # è·³å‡º whileï¼Œå¤„ç†ä¸‹ä¸€ä¸ª task

                except Exception as e:
                    print(f"[åå°ä»»åŠ¡] è·å– {t_code} ({t_date}) å¤±è´¥: {e}")
                    # è§¦å‘é€€é¿æœºåˆ¶
                    if current_backoff == 0:
                        current_backoff = 60
                    else:
                        current_backoff *= 2
                    
                    print(f"[åå°ä»»åŠ¡] é€€é¿æ—¶é—´å¢åŠ åˆ° {current_backoff}ç§’ã€‚æ­£åœ¨é‡è¯•åŒä¸€ä»»åŠ¡...")
    
    print("[åå°ä»»åŠ¡] æ‰€æœ‰ä»»åŠ¡å·²å®Œæˆã€‚")


def fetch_intraday_data_v2(stock_codes, target_date_str, period='1'):
    """
    è·å–æŒ‡å®šè‚¡ç¥¨åˆ—è¡¨ + ä¸‰å¤§æŒ‡æ•° çš„åˆ†é’Ÿçº§æ•°æ® (å¹¶å‘ç‰ˆ)ã€‚
    """
    results = [] 
    
    # å®šä¹‰éœ€è¦è·å–çš„æŒ‡æ•°
    indices_map = {
        "000300": "ğŸ“Š æ²ªæ·±300",
        "000001": "ğŸ“ˆ ä¸Šè¯æŒ‡æ•°",
        "399001": "ğŸ“‰ æ·±è¯æˆæŒ‡"
    }

    # ä»»åŠ¡åˆ—è¡¨
    tasks = []

    # 1. æäº¤æŒ‡æ•°ä»»åŠ¡
    for idx_code, idx_name in indices_map.items():
        tasks.append({
            'type': 'index',
            'code': idx_code,
            'name': idx_name,
            'to_val': 99999999999
        })

    # 2. æäº¤ä¸ªè‚¡ä»»åŠ¡
    for code, name, to_val in stock_codes:
        tasks.append({
            'type': 'stock',
            'code': code,
            'name': name,
            'to_val': to_val
        })
        
    def _worker(task):
        try:
            is_index = (task['type'] == 'index')
            data = fetch_cached_min_data(task['code'], target_date_str, is_index=is_index, period=period)
            if data is not None:
                return {
                    'code': task['code'],
                    'name': task['name'],
                    'data': data,
                    'turnover': task['to_val'],
                    'is_index': is_index
                }
        except Exception:
            pass
        return None

    # å¹¶å‘æ‰§è¡Œ
    ctx = get_script_run_ctx()
    def _worker_wrapper(t):
        if ctx:
            add_script_run_ctx(threading.current_thread(), ctx)
        return _worker(t)

    with concurrent.futures.ThreadPoolExecutor(max_workers=16) as executor:
        future_to_task = {executor.submit(_worker_wrapper, t): t for t in tasks}
        
        for future in concurrent.futures.as_completed(future_to_task):
            res = future.result()
            if res:
                results.append(res)
            
    return results
