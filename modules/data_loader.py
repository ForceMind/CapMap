import pandas as pd
import tushare as ts
import akshare as ak  # ä¿ç•™ akshare ä½œä¸ºå¤‡ä»½æˆ–è¾…åŠ©? æˆ–è€…å®Œå…¨ç§»é™¤? ç”¨æˆ·è¯´"æ›´æ¢"ï¼Œæˆ‘åº”å°½é‡ä½¿ç”¨ Tushare
import os
import streamlit as st
import concurrent.futures
import threading
from datetime import datetime, timedelta
import random
import time

from .config import STOCK_POOLS, TUSHARE_TOKEN
from .utils import with_retry, get_start_date, add_script_run_ctx, get_script_run_ctx

# åˆå§‹åŒ– Tushare
if TUSHARE_TOKEN == "YOUR_TUSHARE_TOKEN_HERE":
    st.warning("âš ï¸ è¯·åœ¨ modules/config.py ä¸­è®¾ç½®æœ‰æ•ˆçš„ Tushare Tokenï¼Œå¦åˆ™æ— æ³•è·å–æ•°æ®ã€‚")
else:
    try:
        ts.set_token(TUSHARE_TOKEN)
        pro = ts.pro_api()
    except Exception as e:
        st.error(f"Tushare åˆå§‹åŒ–å¤±è´¥: {e}")
        pro = None

def fetch_history_data(pool_name="æ²ªæ·±300 (å¤§ç›˜)", limit=None):
    """
    è·å–æŒ‡å®šæˆåˆ†è‚¡è¿‡å»2å¹´çš„æ—¥çº¿æ•°æ® (Tushareç‰ˆ)ã€‚
    """
    config = STOCK_POOLS.get(pool_name, STOCK_POOLS["æ²ªæ·±300 (å¤§ç›˜)"])
    cache_file = config["cache"]
    # ä¼˜å…ˆå°è¯•ä½¿ç”¨ code (Tushareæ ¼å¼), å¦‚æœæ²¡æœ‰åˆ™å›é€€åˆ° ak_code for display, actually we need Tushare index code
    index_code = config.get("code", "399300.SZ")

    cached_df = pd.DataFrame()
    last_cached_date = None

    # 1. å°è¯•åŠ è½½æœ¬åœ°ç¼“å­˜
    if os.path.exists(cache_file):
        try:
            cached_df = pd.read_parquet(cache_file)
            if not cached_df.empty:
                last_cached_date = cached_df['æ—¥æœŸ'].max().date()
                st.toast(f"âœ… å·²åŠ è½½æœ¬åœ°ç¼“å­˜ [{pool_name}]ï¼Œæœ€æ–°æ—¥æœŸ: {last_cached_date}")
        except Exception as e:
            st.error(f"è¯»å–ç¼“å­˜æ–‡ä»¶å¤±è´¥: {e}")

    # 2. è®¡ç®—éœ€è¦ä¸‹è½½çš„æ—¶é—´èŒƒå›´
    today = datetime.now().date()
    
    if last_cached_date:
        if last_cached_date >= today:
             return cached_df
        start_date_str = (last_cached_date + timedelta(days=1)).strftime("%Y%m%d")
    else:
        start_date_str = get_start_date(2)
        
    end_date_str = today.strftime("%Y%m%d")

    # å¦‚æœä¸éœ€è¦æ›´æ–°
    if start_date_str > end_date_str:
        return cached_df

    # çŠ¶æ€å®¹å™¨
    status_text = st.empty()
    progress_bar = st.progress(0)
    
    try:
        # å¦‚æœæ˜¯å¢é‡æ›´æ–°
        is_incremental = not cached_df.empty
        if not is_incremental:
            status_text.text(f"æ­£åœ¨åˆå§‹åŒ– [{pool_name}] å†å²æ•°æ® (Tushare)...")
        else:
            status_text.text(f"æ­£åœ¨æ£€æŸ¥å¢é‡æ•°æ® ({start_date_str} - {end_date_str})...")

        if pro is None:
             st.error("Tushare æœªåˆå§‹åŒ–ï¼Œè¯·æ£€æŸ¥ Token")
             return cached_df

        # è·å–æˆåˆ†è‚¡åˆ—è¡¨
        status_text.text(f"æ­£åœ¨è·å– [{pool_name}] æˆåˆ†è‚¡åˆ—è¡¨...")
        
        stock_list_data = [] # List of dict: {'code': '000001', 'name': 'å¹³å®‰é“¶è¡Œ', 'ts_code': '000001.SZ'}
        
        # ä¼˜å…ˆå°è¯• Tushare
        try:
            # Check rate limit or points issues
            cons_df = with_retry(lambda: pro.index_member(index_code=index_code), retries=2, delay=2.0)
            if cons_df is not None and not cons_df.empty:
                # cons_df columns: index_code, con_code, con_name
                # Note: valid columns might depend on permissions.
                for _, row in cons_df.iterrows():
                    ts_c = row['con_code']
                    nm = row['con_name'] if 'con_name' in row else ts_c
                    # symbol is code without suffix
                    sym = ts_c.split('.')[0]
                    stock_list_data.append({'code': sym, 'name': nm, 'ts_code': ts_c})
        except Exception as e:
            print(f"Tushare index_member failed: {e}")
            cons_df = pd.DataFrame()

        # Fallback to AkShare if Tushare failed or returned empty
        if not stock_list_data:
            st.warning("Tushare è·å–æˆåˆ†è‚¡å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨ AkShare ä½œä¸ºå¤‡ç”¨å…ƒæ•°æ®æº...")
            try:
                ak_code = config.get("ak_code", "000300")
                cons_df_ak = with_retry(lambda: ak.index_stock_cons(symbol=ak_code), retries=3, delay=2.0)
                if cons_df_ak is not None and not cons_df_ak.empty:
                    # AkShare standardizes: å“ç§ä»£ç , å“ç§åç§°
                    if 'variety' in cons_df_ak.columns:
                        c_col, n_col = 'variety', 'name'
                    elif 'å“ç§ä»£ç ' in cons_df_ak.columns:
                        c_col, n_col = 'å“ç§ä»£ç ', 'å“ç§åç§°'
                    else:
                        c_col, n_col = cons_df_ak.columns[0], cons_df_ak.columns[1]
                    
                    for _, row in cons_df_ak.iterrows():
                        sym = str(row[c_col])
                        nm = row[n_col]
                        # generate ts_code
                        if sym.startswith('6'): ts_c = f"{sym}.SH"
                        elif sym.startswith('8'): ts_c = f"{sym}.BJ"
                        else: ts_c = f"{sym}.SZ"
                        
                        stock_list_data.append({'code': sym, 'name': nm, 'ts_code': ts_c})
                        
            except Exception as e:
                 st.error(f"æ— æ³•è·å–æˆåˆ†è‚¡åˆ—è¡¨ (Tushare & AkShare failed): {e}")
                 return cached_df if not cached_df.empty else pd.DataFrame()
        
        if not stock_list_data:
             return cached_df if not cached_df.empty else pd.DataFrame()

        stock_map = {item['ts_code']: item for item in stock_list_data}
        stock_list = [item['ts_code'] for item in stock_list_data]
        if limit:
            stock_list = stock_list[:limit]
        total_stocks = len(stock_list)


        new_data_list = []
        total_stocks = len(stock_list)

        # å¾ªç¯è·å–å†å²
        def fetch_one_stock(ts_code):
            try:
                # è·å–æ—¥çº¿
                df_hist = ts.pro_bar(ts_code=ts_code, adj='qfq', start_date=start_date_str, end_date=end_date_str)
                
                if df_hist is not None and not df_hist.empty:
                    # Rename columns
                    df_hist = df_hist.rename(columns={
                        'trade_date': 'æ—¥æœŸ',
                        'close': 'æ”¶ç›˜',
                        'pct_chg': 'æ¶¨è·Œå¹…',
                        'amount': 'æˆäº¤é¢'
                    })
                    
                    df_hist['æ—¥æœŸ'] = pd.to_datetime(df_hist['æ—¥æœŸ'])
                    # Tushare amount is 'åƒå…ƒ', Akshare was 'å…ƒ'. Multiply by 1000
                    df_hist['æˆäº¤é¢'] = df_hist['æˆäº¤é¢'] * 1000
                    
                    cols_needed = ['æ—¥æœŸ', 'æ”¶ç›˜', 'æ¶¨è·Œå¹…', 'æˆäº¤é¢']
                    df_hist = df_hist[cols_needed].copy()
                    
                    # sym = symbol_map.get(ts_code, ts_code.split('.')[0])
                    # nm = name_map.get(ts_code, sym)
                    info = stock_map.get(ts_code, {})
                    sym = info.get('code', ts_code.split('.')[0])
                    nm = info.get('name', sym)
                    
                    df_hist['ä»£ç '] = sym
                    df_hist['åç§°'] = nm
                    
                    return df_hist

            except Exception:
                pass
            return None

        # Serial execution to respect Tushare rate limits (50 req/min => 1.2s delay)
        for i, ts_code in enumerate(stock_list):
             if i % 5 == 0:
                 progress_bar.progress((i + 1) / total_stocks)
                 status_text.text(f"æ­£åœ¨åŒæ­¥æ•°æ® [{pool_name}]: {i+1}/{total_stocks} (Tushareé™é€Ÿ fetching...)")
             
             # Enforce rate limit
             time.sleep(1.25)
             
             res = fetch_one_stock(ts_code)
             if res is not None:
                 new_data_list.append(res)
                
        status_text.empty()
        progress_bar.empty()
        
        # åˆå¹¶é€»è¾‘
        if new_data_list:
            new_df = pd.concat(new_data_list, ignore_index=True)
            # ç±»å‹è½¬æ¢
            new_df['æ—¥æœŸ'] = pd.to_datetime(new_df['æ—¥æœŸ'])
            new_df['æ¶¨è·Œå¹…'] = pd.to_numeric(new_df['æ¶¨è·Œå¹…'], errors='coerce')
            new_df['æˆäº¤é¢'] = pd.to_numeric(new_df['æˆäº¤é¢'], errors='coerce')
            new_df['æ”¶ç›˜'] = pd.to_numeric(new_df['æ”¶ç›˜'], errors='coerce')
            
            if cached_df.empty:
                final_df = new_df
            else:
                st.toast(f"ğŸ“¥ æˆåŠŸè·å– {len(new_df)} æ¡æ–°è®°å½• ({pool_name})")
                final_df = pd.concat([cached_df, new_df], ignore_index=True)
                final_df.drop_duplicates(subset=['æ—¥æœŸ', 'ä»£ç '], keep='last', inplace=True)
        else:
            final_df = cached_df
            
        if final_df.empty:
            return pd.DataFrame()

        final_df = final_df.sort_values('æ—¥æœŸ')
        
        # ä½¿ç”¨æœ€æ–°çš„ stock_names æ›´æ–° DataFrame ä¸­çš„åç§°åˆ—
        # Map symbol -> name
        final_stock_names = {item['code']: item['name'] for item in stock_list_data}
        if final_df is not None and not final_df.empty:
            final_df['åç§°'] = final_df['ä»£ç '].map(final_stock_names).fillna(final_df['åç§°'])
        
        # ä¿å­˜ç¼“å­˜
        if new_data_list or cached_df.empty:
            try:
                if not os.path.exists("data"):
                    os.makedirs("data")
                final_df.to_parquet(cache_file)
                if not cached_df.empty:
                    st.toast(f"ğŸ’¾ [{pool_name}] å¢é‡æ•°æ®å·²åˆå¹¶å¹¶ä¿å­˜")
                else:
                    st.success(f"ğŸ’¾ [{pool_name}] å…¨é‡æ•°æ®å·²åˆå§‹åŒ–")
            except Exception as e:
                st.warning(f"æ— æ³•ä¿å­˜ç¼“å­˜: {e}")

        return final_df

    except Exception as e:
        status_text.empty()
        progress_bar.empty()
        st.error(f"å…¨å±€æ•°æ®é”™è¯¯: {e}")
        return pd.DataFrame()

@st.cache_data(ttl=3600*24, show_spinner=False)
def fetch_cached_min_data(symbol, date_str, is_index=False, period='1'):
    """
    åŸå­åŒ–è·å–å•ä¸ªæ ‡çš„çš„åˆ†æ—¶æ•°æ® (Tushareç‰ˆ)ï¼Œç‹¬ç«‹ç¼“å­˜ã€‚
    """
    start_time = f"{date_str} 09:30:00"
    end_time = f"{date_str} 15:00:00"
    
    # ç®€å•çš„é‡è¯•æœºåˆ¶
    max_retries = 3

    # Helper for ts_code
    def get_ts_code(sym, is_idx):
        if is_idx:
            if sym == "000300": return "399300.SZ"
            if sym == "000001": return "000001.SH"
            if sym == "399001": return "399001.SZ"
            return sym
        else:
            if sym.startswith('6'): return f"{sym}.SH"
            if sym.startswith('0') or sym.startswith('3'): return f"{sym}.SZ"
            if sym.startswith('8'): return f"{sym}.BJ"
            return f"{sym}.SH"

    ts_code = get_ts_code(symbol, is_index)
    
    for attempt in range(max_retries):
        try:
            # period '1' -> '1min'
            freq = '1min' if period == '1' else period
            
            # ts.pro_bar handles min data via 'ft_mins' or 'stk_mins'
            # Requires start_date and end_date as strings.
            # Tushare needs date string for pro_bar if it's daily, 
            # but for minutes, it might vary.
            # Using pro_bar is safest wrapper.
            
            # Warning: Tushare min data consumes points.
            df = ts.pro_bar(ts_code=ts_code, freq=freq, start_date=start_time, end_date=end_time)
            
            # If fail (e.g. Rate Limit 2/min or No Points), fallback to AkShare
            if df is None or df.empty:
                 try:
                     parts = ts_code.split('.')
                     code_val = parts[0]
                     suffix = parts[1] if len(parts) > 1 else 'SZ'
                     prefix = 'sz' if suffix == 'SZ' else 'sh' if suffix == 'SH' else 'bj'
                     
                     df_ak = pd.DataFrame()
                     if is_index:
                        symbol_ak = f"{prefix}{code_val}"
                        df_ak = ak.index_zh_a_hist_min_em(symbol=symbol_ak, period=period)
                        if not df_ak.empty:
                            df_ak.rename(columns={'æ—¶é—´': 'time', 'å¼€ç›˜': 'open', 'æ”¶ç›˜': 'close', 'æœ€é«˜': 'high', 'æœ€ä½': 'low', 'æˆäº¤é‡': 'vol'}, inplace=True)
                     else:
                        symbol_ak = f"{prefix}{code_val}"
                        df_ak = ak.stock_zh_a_minute(symbol=symbol_ak, period=period, adjust='qfq')
                        if not df_ak.empty:
                            df_ak.rename(columns={'day': 'time'}, inplace=True)
                            
                     if not df_ak.empty:
                         # Filter for the specific date
                         df_ak['time'] = pd.to_datetime(df_ak['time'])
                         mask = (df_ak['time'] >= pd.to_datetime(start_time)) & (df_ak['time'] <= pd.to_datetime(end_time))
                         df = df_ak.loc[mask].copy()
                 except Exception as e:
                     # print(f"AkShare fallback failed: {e}")
                     pass
            
            if df is not None and not df.empty:
                # Rename columns
                # Tushare: trade_time, open, close, high, low, vol, amount
                # Akshare: æ—¶é—´, å¼€ç›˜, æ”¶ç›˜, æœ€é«˜, æœ€ä½, æˆäº¤é‡, æˆäº¤é¢
                
                # App expects: 'time', 'open', 'close'
                if 'trade_time' in df.columns:
                    df.rename(columns={'trade_time': 'time', 'open': 'open', 'close': 'close'}, inplace=True)
                
                # Check column mapping
                if 'time' not in df.columns: 
                    # fallback if Tushare returns trade_date and trade_time split?
                    pass
                
                # Sort by time
                df = df.sort_values('time')
                return df
                
        except Exception:
            time.sleep(1)
            
    return pd.DataFrame()

# --- åå°é¢„å–çº¿ç¨‹é€»è¾‘ ---
@st.cache_data(ttl=3600*24, show_spinner=False)
def fetch_cached_min_data_wrapper(symbol, date_str, is_index=False, period='1'):
    """Wrapper to be called by background thread"""
    # This is just a direct call to the cached function
    # In background thread, we can call this.
    return fetch_cached_min_data(symbol, date_str, is_index, period)

def background_prefetch_task(date_list, origin_df):
    """
    åå°çº¿ç¨‹ï¼šæ‰§è¡Œæ•°æ®é¢„å–ã€‚
    """
    total_dates = len(date_list)
    print(f"\n[åå°ä»»åŠ¡] å¼€å§‹é¢„å– {total_dates} å¤©çš„æ•°æ®ã€‚")
    
    current_backoff = 0 # ç§’
    
    indices_codes = ["000300", "000001", "399001"]
    
    for i, d in enumerate(date_list):
        d_str = d.strftime("%Y-%m-%d")
        print(f"[åå°ä»»åŠ¡] æ­£åœ¨å¤„ç†: {d_str} ({i+1}/{total_dates})")
        
        # ç­›é€‰
        daily = origin_df[origin_df['æ—¥æœŸ'].dt.date == d]
        if daily.empty: continue
        
        # Top 25
        top_stocks = daily.sort_values('æˆäº¤é¢', ascending=False).head(25)['ä»£ç '].tolist()
        
        # ä»»åŠ¡åˆ—è¡¨
        tasks = []
        for code in indices_codes: tasks.append((code, d_str, True))
        for code in top_stocks: tasks.append((code, d_str, False))
        
        # å†…å±‚é€ä¸ªæ‰§è¡Œ (ä¸ºäº†æ–¹ä¾¿æ§åˆ¶é€€é¿ï¼Œä¸”åå°ä»»åŠ¡ä¸æ€¥äºä¸€æ—¶çš„å¹¶å‘ï¼Œç¨³å®šç¬¬ä¸€)
        for t_code, t_date, t_is_index in tasks:
            
            # Indefinite retry loop with backoff
            while True:
                try:
                    # æ£€æŸ¥é€€é¿
                    if current_backoff > 0:
                        print(f"[åå°ä»»åŠ¡] å¤„äºå†·å´çŠ¶æ€ã€‚ç­‰å¾… {current_backoff} ç§’...")
                        time.sleep(current_backoff)
                        
                    fetch_cached_min_data(t_code, t_date, is_index=t_is_index, period='1')
                    
                    # Success
                    if current_backoff > 0:
                        print(f"[åå°ä»»åŠ¡] å·²æ¢å¤ã€‚é‡ç½®é€€é¿æ—¶é—´ã€‚")
                        current_backoff = 0
                    
                    time.sleep(0.1)
                    break # è·³å‡º whileï¼Œå¤„ç†ä¸‹ä¸€ä¸ª task

                except Exception as e:
                    print(f"[åå°ä»»åŠ¡] è·å– {t_code} ({t_date}) å¤±è´¥: {e}")
                    # è§¦å‘é€€é¿æœºåˆ¶
                    if current_backoff == 0:
                        current_backoff = 60
                    else:
                        current_backoff *= 2
                    
                    print(f"[åå°ä»»åŠ¡] é€€é¿æ—¶é—´å¢åŠ åˆ° {current_backoff}ç§’ã€‚æ­£åœ¨é‡è¯•åŒä¸€ä»»åŠ¡...")
    
    print("[åå°ä»»åŠ¡] æ‰€æœ‰ä»»åŠ¡å·²å®Œæˆã€‚")


def fetch_intraday_data_v2(stock_codes, target_date_str, period='1'):
    """
    è·å–æŒ‡å®šè‚¡ç¥¨åˆ—è¡¨ + ä¸‰å¤§æŒ‡æ•° çš„åˆ†é’Ÿçº§æ•°æ® (å¹¶å‘ç‰ˆ)ã€‚
    """
    results = [] 
    
    # å®šä¹‰éœ€è¦è·å–çš„æŒ‡æ•°
    indices_map = {
        "000300": "ğŸ“Š æ²ªæ·±300",
        "000001": "ğŸ“ˆ ä¸Šè¯æŒ‡æ•°",
        "399001": "ğŸ“‰ æ·±è¯æˆæŒ‡"
    }

    # ä»»åŠ¡åˆ—è¡¨
    tasks = []

    # 1. æäº¤æŒ‡æ•°ä»»åŠ¡
    for idx_code, idx_name in indices_map.items():
        tasks.append({
            'type': 'index',
            'code': idx_code,
            'name': idx_name,
            'to_val': 99999999999
        })

    # 2. æäº¤ä¸ªè‚¡ä»»åŠ¡
    for code, name, to_val in stock_codes:
        tasks.append({
            'type': 'stock',
            'code': code,
            'name': name,
            'to_val': to_val
        })
        
    def _worker(task):
        try:
            is_index = (task['type'] == 'index')
            data = fetch_cached_min_data(task['code'], target_date_str, is_index=is_index, period=period)
            if data is not None:
                return {
                    'code': task['code'],
                    'name': task['name'],
                    'data': data,
                    'turnover': task['to_val'],
                    'is_index': is_index
                }
        except Exception:
            pass
        return None

    # å¹¶å‘æ‰§è¡Œ
    ctx = get_script_run_ctx()
    def _worker_wrapper(t):
        if ctx:
            add_script_run_ctx(threading.current_thread(), ctx)
        return _worker(t)

    with concurrent.futures.ThreadPoolExecutor(max_workers=16) as executor:
        future_to_task = {executor.submit(_worker_wrapper, t): t for t in tasks}
        
        for future in concurrent.futures.as_completed(future_to_task):
            res = future.result()
            if res:
                results.append(res)
            
    return results
