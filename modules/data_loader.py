import pandas as pd
import akshare as ak
import os
import streamlit as st
import concurrent.futures
import threading
from datetime import datetime, timedelta
import time

from .config import STOCK_POOLS, DATA_DIR
from .utils import with_retry, get_start_date, add_script_run_ctx, get_script_run_ctx


def log_info(message):
    ts = datetime.now().strftime("%H:%M:%S")
    print(f"[{ts}] {message}")


_PROXY_DISABLED_LOGGED = False


def _disable_proxy_env():
    """
    é»˜è®¤ç¦ç”¨ç³»ç»Ÿä»£ç†ï¼Œé¿å… Eastmoney æ¥å£è§¦å‘ ProxyErrorã€‚
    å¦‚éœ€å¯ç”¨ä»£ç†ï¼šè®¾ç½®ç¯å¢ƒå˜é‡ CAPMAP_USE_PROXY=1 æˆ–æ³¨é‡Šæ­¤å‡½æ•°è°ƒç”¨ã€‚
    """
    global _PROXY_DISABLED_LOGGED
    if os.environ.get("CAPMAP_USE_PROXY") == "1":
        return False
    for key in ("HTTP_PROXY", "HTTPS_PROXY", "ALL_PROXY", "http_proxy", "https_proxy", "all_proxy"):
        if key in os.environ:
            os.environ[key] = ""
    os.environ["NO_PROXY"] = "push2his.eastmoney.com,82.push2.eastmoney.com,*.eastmoney.com"
    if not _PROXY_DISABLED_LOGGED:
        log_info("å·²ç¦ç”¨ç³»ç»Ÿä»£ç†(é»˜è®¤)ã€‚å¦‚éœ€å¯ç”¨ä»£ç†ï¼Œè¯·è®¾ç½® CAPMAP_USE_PROXY=1")
        _PROXY_DISABLED_LOGGED = True
    return True


def _stop_requested():
    try:
        return bool(st.session_state.get("stop_fetch_requested"))
    except Exception:
        return False


def build_fetch_plan(pool_name, max_workers, request_delay, fetch_spot):
    _disable_proxy_env()

    config = STOCK_POOLS.get(pool_name, STOCK_POOLS["æ²ªæ·±300 (å¤§ç›˜)"])
    cache_file = config["cache"]
    index_code = config["code"]

    cached_df = pd.DataFrame()
    last_cached_date = None
    cached_rows = 0

    if os.path.exists(cache_file):
        try:
            cached_df = pd.read_parquet(cache_file)
            if not cached_df.empty:
                last_cached_date = cached_df['æ—¥æœŸ'].max().date()
                cached_rows = len(cached_df)
        except Exception:
            pass

    today = datetime.now().date()
    if last_cached_date:
        start_date_str = (last_cached_date + timedelta(days=1)).strftime("%Y%m%d")
    else:
        start_date_str = get_start_date(months_back=3)
    end_date_str = today.strftime("%Y%m%d")

    total_stocks = None
    try:
        cons_df = with_retry(lambda: ak.index_stock_cons(symbol=index_code), retries=3, delay=1.0)
        if cons_df is not None and not cons_df.empty:
            if 'variety' in cons_df.columns:
                code_col = 'variety'
            elif 'å“ç§ä»£ç ' in cons_df.columns:
                code_col = 'å“ç§ä»£ç '
            else:
                code_col = cons_df.columns[0]
            total_stocks = len(cons_df[code_col].tolist())
    except Exception:
        total_stocks = None

    needs_update = start_date_str <= end_date_str
    avg_req_seconds = 0.4
    est_seconds = None
    if total_stocks:
        est_seconds = (total_stocks * (request_delay + avg_req_seconds)) / max(1, max_workers)

    return {
        "pool_name": pool_name,
        "index_code": index_code,
        "cache_file": cache_file,
        "has_cache": not cached_df.empty,
        "cached_rows": cached_rows,
        "last_cached_date": last_cached_date,
        "start_date_str": start_date_str,
        "end_date_str": end_date_str,
        "total_stocks": total_stocks,
        "needs_update": needs_update,
        "max_workers": max_workers,
        "request_delay": request_delay,
        "fetch_spot": fetch_spot,
        "est_seconds": est_seconds
    }

def fetch_history_data(
    pool_name="æ²ªæ·±300 (å¤§ç›˜)",
    allow_download=True,
    max_workers=3,
    request_delay=0.5,
    fetch_spot=True
):
    """
    è·å–æŒ‡å®šæˆåˆ†è‚¡è¿‘ 3 ä¸ªæœˆçš„æ—¥çº¿æ•°æ®ï¼ˆå¯é…ç½®ï¼‰ã€‚
    é€»è¾‘å¤åˆ»è‡ª app1.py (ç¨³å®šç‰ˆ)ï¼Œæ”¯æŒå¤šæŒ‡æ•°æ± ã€‚
    """
    _disable_proxy_env()

    config = STOCK_POOLS.get(pool_name, STOCK_POOLS["æ²ªæ·±300 (å¤§ç›˜)"])
    cache_file = config["cache"]
    index_code = config["code"]

    cached_df = pd.DataFrame()
    last_cached_date = None

    # 1. å°è¯•åŠ è½½æœ¬åœ°ç¼“å­˜
    cache_min_codes = 50
    if os.path.exists(cache_file):
        try:
            cached_df = pd.read_parquet(cache_file)
            if not cached_df.empty:
                last_cached_date = cached_df['æ—¥æœŸ'].max().date()
                st.toast(f"âœ… å·²åŠ è½½æœ¬åœ°ç¼“å­˜ [{pool_name}]ï¼Œæœ€æ–°æ—¥æœŸ: {last_cached_date}")
                log_info(f"è¯»å–ç¼“å­˜æˆåŠŸ: {pool_name} | æœ€æ–°æ—¥æœŸ {last_cached_date} | è¡Œæ•° {len(cached_df)}")
        except Exception as e:
            st.error(f"è¯»å–ç¼“å­˜æ–‡ä»¶å¤±è´¥: {e}")
            log_info(f"è¯»å–ç¼“å­˜å¤±è´¥: {pool_name} | {e}")

    if not cached_df.empty:
        try:
            unique_codes = cached_df['ä»£ç '].astype(str).nunique()
        except Exception:
            unique_codes = 0
        if unique_codes < cache_min_codes:
            st.warning(f"æ£€æµ‹åˆ°ç¼“å­˜æ ·æœ¬è¿‡å°‘({unique_codes}åª)ï¼Œå°†å¿½ç•¥è¯¥ç¼“å­˜å¹¶é‡æ–°æ‹‰å–ã€‚")
            log_info(f"ç¼“å­˜å¯èƒ½ä¸å®Œæ•´: {pool_name} | å”¯ä¸€ç  {unique_codes}")
            try:
                os.remove(cache_file)
                log_info(f"å·²åˆ é™¤ä¸å®Œæ•´ç¼“å­˜: {cache_file}")
            except Exception:
                pass
            cached_df = pd.DataFrame()
            last_cached_date = None

    if not allow_download:
        log_info(f"å·²å…³é—­è‡ªåŠ¨æ‹‰å–: {pool_name} | ä»…ä½¿ç”¨ç¼“å­˜")
        return cached_df

    if _stop_requested():
        log_info("æ£€æµ‹åˆ°ä¸­æ–­è¯·æ±‚ï¼Œå·²å–æ¶ˆæ‹‰å–")
        return cached_df

    max_workers = max(1, int(max_workers))
    request_delay = max(0.0, float(request_delay))
    # 2. è®¡ç®—éœ€è¦ä¸‹è½½çš„æ—¶é—´èŒƒå›´
    today = datetime.now().date()
    
    if last_cached_date:
        if last_cached_date >= today:
             return cached_df
        start_date_str = (last_cached_date + timedelta(days=1)).strftime("%Y%m%d")
    else:
        start_date_str = get_start_date(months_back=3)
        
    end_date_str = today.strftime("%Y%m%d")

    # å¦‚æœä¸éœ€è¦æ›´æ–°
    if start_date_str > end_date_str:
        return cached_df

    # çŠ¶æ€å®¹å™¨
    status_text = st.empty()
    progress_bar = st.progress(0)
    
    try:
        # å¦‚æœæ˜¯å¢é‡æ›´æ–°
        is_incremental = not cached_df.empty
        if not is_incremental:
            status_text.text(f"æ­£åœ¨åˆå§‹åŒ– [{pool_name}] å†å²æ•°æ®...")
        else:
            status_text.text(f"æ­£åœ¨æ£€æŸ¥å¢é‡æ•°æ® ({start_date_str} - {end_date_str})...")

        # è·å–æˆåˆ†è‚¡åˆ—è¡¨
        log_info(f"å¼€å§‹è·å–æˆåˆ†è‚¡åˆ—è¡¨: {pool_name} | æ¥å£ index_stock_cons({index_code})")
        status_text.text(f"æ­£åœ¨è·å– [{pool_name}] æˆåˆ†è‚¡åˆ—è¡¨...")
        try:
            # å¢åŠ é‡è¯•
            cons_df = with_retry(lambda: ak.index_stock_cons(symbol=index_code), retries=5, delay=2.0)
        except:
             if not cached_df.empty:
                 st.warning("æˆåˆ†è‚¡åˆ—è¡¨è·å–å¤±è´¥ (ç½‘ç»œåŸå› )ï¼Œä½¿ç”¨ç¼“å­˜æ•°æ®")
                 return cached_df
             return pd.DataFrame()
        
        if cons_df is None or cons_df.empty:
             st.warning(f"æ— æ³•è·å– [{pool_name}] æˆåˆ†è‚¡åˆ—è¡¨ (å¯èƒ½æ˜¯ AkShare æ¥å£å˜åŠ¨æˆ–ç½‘ç»œè¶…æ—¶)")
             return cached_df if not cached_df.empty else pd.DataFrame()

        if 'variety' in cons_df.columns:
            code_col, name_col = 'variety', 'name'
        elif 'å“ç§ä»£ç ' in cons_df.columns:
            code_col, name_col = 'å“ç§ä»£ç ', 'å“ç§åç§°'
        else:
            code_col = cons_df.columns[0]
            name_col = cons_df.columns[1]
            
        # å¼ºè½¬ä¸º 6 ä½è‚¡ç¥¨ä»£ç 
        code_series = cons_df[code_col].astype(str)
        code_series = code_series.str.extract(r'(\d{6})', expand=False).fillna(code_series)
        code_series = code_series.str.zfill(6)
        stock_names = dict(zip(code_series.tolist(), cons_df[name_col].astype(str)))
        stock_list = list(dict.fromkeys(code_series.tolist()))
        
        # --- å°è¯•è·å–ä»Šæ—¥å®æ—¶æ•°æ® (Spot) ---
        today_spot_map = {}
        if fetch_spot:
            try:
                log_info(f"å¼€å§‹è·å–ç›˜ä¸­è¡¥å…¨: {pool_name} | æ¥å£ stock_zh_a_spot_em")
                # Low frequency
                spot_df = ak.stock_zh_a_spot_em()
                if spot_df is not None and not spot_df.empty:
                    spot_df['ä»£ç '] = spot_df['ä»£ç '].astype(str)
                    spot_df['ä»£ç '] = spot_df['ä»£ç '].str.extract(r'(\d{6})', expand=False).fillna(spot_df['ä»£ç '])
                    spot_df['ä»£ç '] = spot_df['ä»£ç '].str.zfill(6)
                    
                    # 1. æ›´æ–°åç§°æ˜ å°„
                    new_names = dict(zip(spot_df['ä»£ç '], spot_df['åç§°']))
                    stock_names.update(new_names)
                    
                    # 2. å‡†å¤‡ä»Šæ—¥æ•°æ®æ˜ å°„
                    if end_date_str >= start_date_str:
                        today_spot_map = spot_df.set_index('ä»£ç ').to_dict('index')
            except Exception as e:
                # éè‡´å‘½é”™è¯¯
                print(f"Update spots failed: {e}")

        new_data_list = []
        total_stocks = len(stock_list)
        success_count = 0
        fail_count = 0
        fail_samples = []
        empty_samples = []
        proxy_error_seen = False
        stop_triggered = False
        fail_lock = threading.Lock()
        log_info(f"å¼€å§‹è·å–æ—¥çº¿: {pool_name} | è‚¡ç¥¨æ•° {total_stocks} | çº¿ç¨‹ {max_workers} | å»¶è¿Ÿ {request_delay}s")

        def _record_sample(bucket, message):
            with fail_lock:
                if len(bucket) < 5:
                    bucket.append(message)

        def _record_proxy_error():
            nonlocal proxy_error_seen
            with fail_lock:
                proxy_error_seen = True

        # å¾ªç¯è·å–å†å²
        def fetch_one_stock(code, name):
            try:
                if request_delay > 0:
                    time.sleep(request_delay)
                # è·å–æ—¥çº¿
                df_hist = ak.stock_zh_a_hist(symbol=code, start_date=start_date_str, end_date=end_date_str, adjust="qfq")
                
                # æ£€æŸ¥æ˜¯å¦åŒ…å«ä»Šå¤©
                fetched_today = False
                if df_hist is not None and not df_hist.empty:
                    df_hist['æ—¥æœŸ'] = pd.to_datetime(df_hist['æ—¥æœŸ'])
                    if end_date_str in df_hist['æ—¥æœŸ'].dt.strftime("%Y%m%d").values:
                        fetched_today = True
                else:
                    df_hist = pd.DataFrame()

                # è¡¥å…¨ä»Šå¤©
                if (not fetched_today) and (end_date_str == datetime.now().strftime("%Y%m%d")):
                    if code in today_spot_map:
                        row = today_spot_map[code]
                        try:
                             new_row = pd.DataFrame([{
                                 'æ—¥æœŸ': pd.to_datetime(end_date_str),
                                 'æ”¶ç›˜': row['æœ€æ–°ä»·'],
                                 'æ¶¨è·Œå¹…': row['æ¶¨è·Œå¹…'],
                                 'æˆäº¤é¢': row['æˆäº¤é¢'],
                                 'ä»£ç ': code,
                                 'åç§°': name
                             }])
                             df_hist = pd.concat([df_hist, new_row], ignore_index=True)
                        except Exception:
                            pass
                
                if df_hist is not None and not df_hist.empty:
                    # ç¡®ä¿åˆ—å­˜åœ¨
                    cols_needed = ['æ—¥æœŸ', 'æ”¶ç›˜', 'æ¶¨è·Œå¹…', 'æˆäº¤é¢']
                    for c in cols_needed:
                        if c not in df_hist.columns:
                            _record_sample(fail_samples, f"{code} ç¼ºåˆ—:{c}")
                            return None
                    
                    df_hist = df_hist[cols_needed].copy()
                    df_hist['ä»£ç '] = code
                    df_hist['åç§°'] = name
                    return df_hist

                _record_sample(empty_samples, code)
            except Exception as e:
                msg = str(e)
                if "proxy" in msg.lower():
                    _record_proxy_error()
                _record_sample(fail_samples, f"{code} {msg}")
            return None
        # Use concurrency as in app1.py
        ctx = get_script_run_ctx()
        def fetch_one_stock_wrapper(code, name):
            if ctx:
                add_script_run_ctx(threading.current_thread(), ctx)
            return fetch_one_stock(code, name)
        if max_workers <= 1:
            for i, code in enumerate(stock_list):
                if _stop_requested():
                    stop_triggered = True
                    log_info("æ£€æµ‹åˆ°ä¸­æ–­è¯·æ±‚ï¼Œåœæ­¢æ‹‰å–")
                    break
                name = stock_names.get(code, code)
                res = fetch_one_stock(code, name)
                if res is not None:
                    new_data_list.append(res)
                    success_count += 1
                else:
                    fail_count += 1
                if i % 10 == 0:
                    progress_bar.progress((i + 1) / total_stocks)
                    status_text.text(f"æ­£åœ¨è·å–æ—¥çº¿ [{pool_name}]: {i+1}/{total_stocks}")
        else:
            with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
                 future_map = {executor.submit(fetch_one_stock_wrapper, c, stock_names.get(c, c)): c for c in stock_list}
                 
                 for i, future in enumerate(concurrent.futures.as_completed(future_map)):
                     if _stop_requested():
                         stop_triggered = True
                         log_info("æ£€æµ‹åˆ°ä¸­æ–­è¯·æ±‚ï¼Œåœæ­¢æ‹‰å–")
                         executor.shutdown(cancel_futures=True)
                         break
                     # Update progress
                     if i % 10 == 0:
                         progress_bar.progress((i + 1) / total_stocks)
                         status_text.text(f"æ­£åœ¨è·å–æ—¥çº¿ [{pool_name}]: {i+1}/{total_stocks}")
                     
                     res = future.result()
                     if res is not None:
                         new_data_list.append(res)
                         success_count += 1
                     else:
                         fail_count += 1
        status_text.empty()
        progress_bar.empty()
        log_info(f"å®Œæˆæ—¥çº¿è·å–: {pool_name} | æˆåŠŸ {success_count} | å¤±è´¥ {fail_count}")
        if proxy_error_seen:
            log_info("æ£€æµ‹åˆ°ä»£ç†é”™è¯¯: å·²é»˜è®¤ç¦ç”¨ä»£ç†ã€‚å¦‚éœ€å¯ç”¨ï¼Œè¯·è®¾ç½® CAPMAP_USE_PROXY=1")
        if stop_triggered:
            st.warning("å·²æ”¶åˆ°ä¸­æ–­è¯·æ±‚ï¼Œæœ¬æ¬¡æ‹‰å–å·²åœæ­¢ã€‚")
        if fail_samples:
            log_info("å¤±è´¥æ ·ä¾‹: " + " | ".join(fail_samples))
        if empty_samples:
            log_info("ç©ºæ•°æ®æ ·ä¾‹: " + ", ".join(empty_samples))
        if total_stocks:
            min_success = max(5, int(total_stocks * 0.1))
            if success_count < min_success:
                st.warning(f"æ—¥çº¿æˆåŠŸç‡è¿‡ä½: {success_count}/{total_stocks}ï¼Œç–‘ä¼¼è¢«é™é¢‘æˆ–ç½‘ç»œå¼‚å¸¸ã€‚å»ºè®®å°†å¹¶å‘è°ƒä¸º1ï¼Œé—´éš”â‰¥2ç§’åé‡è¯•ã€‚")
                if cached_df.empty:
                    return pd.DataFrame()
                return cached_df
        if not new_data_list and cached_df.empty:
            st.error("æ—¥çº¿æ‹‰å–å…¨éƒ¨å¤±è´¥ï¼Œå¯èƒ½æ˜¯ç½‘ç»œ/ä»£ç†/é™é¢‘å¯¼è‡´ã€‚è¯·é™ä½å¹¶å‘ã€å¢å¤§é—´éš”åé‡è¯•ã€‚")
            return pd.DataFrame()

        # åˆå¹¶é€»è¾‘
        if new_data_list:
            new_df = pd.concat(new_data_list, ignore_index=True)
            # ç±»å‹è½¬æ¢
            new_df['æ—¥æœŸ'] = pd.to_datetime(new_df['æ—¥æœŸ'])
            new_df['æ¶¨è·Œå¹…'] = pd.to_numeric(new_df['æ¶¨è·Œå¹…'], errors='coerce')
            new_df['æˆäº¤é¢'] = pd.to_numeric(new_df['æˆäº¤é¢'], errors='coerce')
            new_df['æ”¶ç›˜'] = pd.to_numeric(new_df['æ”¶ç›˜'], errors='coerce')
            
            if cached_df.empty:
                final_df = new_df
            else:
                # åˆå¹¶æ—§æ•°æ®å’Œæ–°æ•°æ®ï¼Œå¹¶å»é‡
                st.toast(f"ğŸ“¥ æˆåŠŸè·å– {len(new_df)} æ¡æ–°è®°å½• ({pool_name})")
                final_df = pd.concat([cached_df, new_df], ignore_index=True)
                final_df.drop_duplicates(subset=['æ—¥æœŸ', 'ä»£ç '], keep='last', inplace=True)
        else:
            final_df = cached_df
            
        if final_df.empty:
            return pd.DataFrame()

        final_df = final_df.sort_values('æ—¥æœŸ')
        
        # ä½¿ç”¨æœ€æ–°çš„ stock_names æ›´æ–° DataFrame ä¸­çš„åç§°åˆ—
        if final_df is not None and not final_df.empty:
            final_df['åç§°'] = final_df['ä»£ç '].map(stock_names).fillna(final_df['åç§°'])
        
        # ä¿å­˜ç¼“å­˜
        if new_data_list or cached_df.empty:
            try:
                cache_dir = os.path.dirname(cache_file)
                if cache_dir and not os.path.exists(cache_dir):
                    os.makedirs(cache_dir, exist_ok=True)
                final_df.to_parquet(cache_file)
                if not cached_df.empty:
                    st.toast(f"ğŸ’¾ [{pool_name}] å¢é‡æ•°æ®å·²åˆå¹¶å¹¶ä¿å­˜")
                else:
                    st.success(f"ğŸ’¾ [{pool_name}] å…¨é‡æ•°æ®å·²åˆå§‹åŒ–")
            except Exception as e:
                st.warning(f"æ— æ³•ä¿å­˜ç¼“å­˜: {e}")

        return final_df

    except Exception as e:
        status_text.empty()
        progress_bar.empty()
        st.error(f"å…¨å±€æ•°æ®é”™è¯¯: {e}")
        return pd.DataFrame()

MIN_CACHE_DIR = str(DATA_DIR / "min_cache")


def _min_cache_path(symbol, date_str, period, is_index):
    safe_symbol = str(symbol).replace("/", "_")
    safe_date = str(date_str).replace(":", "").replace(" ", "_")
    suffix = "idx" if is_index else "stk"
    filename = f"{safe_symbol}_{safe_date}_{period}_{suffix}.parquet"
    return os.path.join(MIN_CACHE_DIR, filename)


@st.cache_data(ttl=3600*24, show_spinner=False)
def fetch_cached_min_data(symbol, date_str, is_index=False, period='1'):
    """
    åŸå­åŒ–è·å–å•ä¸ªæ ‡çš„çš„åˆ†æ—¶æ•°æ®ï¼Œç‹¬ç«‹ç¼“å­˜ã€‚
    é¿å…å› è‚¡ç¥¨åˆ—è¡¨ç»„åˆå˜åŒ–å¯¼è‡´æ•´ä¸ªç¼“å­˜å¤±æ•ˆã€‚
    params:
    period: '1', '5', '15', '30', '60'
    """
    _disable_proxy_env()
    cache_path = _min_cache_path(symbol, date_str, period, is_index)
    if os.path.exists(cache_path):
        try:
            cached_df = pd.read_parquet(cache_path)
            if cached_df is not None and not cached_df.empty:
                if 'time' in cached_df.columns:
                    cached_df['time'] = pd.to_datetime(cached_df['time'])
                return cached_df
        except Exception:
            pass

    start_time = f"{date_str} 09:30:00"
    end_time = f"{date_str} 15:00:00"
    
    # æŒ‡æ•°é€€é¿ç­–ç•¥å…¨å±€å˜é‡ (ç®€å•æ¨¡æ‹Ÿ)
    if not hasattr(fetch_cached_min_data, "current_backoff"):
        fetch_cached_min_data.current_backoff = 0
            
    # ç®€å•çš„é‡è¯•æœºåˆ¶
    max_retries = 3
    
    for attempt in range(max_retries):
        try:
            if is_index:
                # æŒ‡æ•°æ¥å£
                df = ak.index_zh_a_hist_min_em(symbol=symbol, period=period, start_date=start_time, end_date=end_time)
            else:
                # ä¸ªè‚¡æ¥å£
                df = ak.stock_zh_a_hist_min_em(symbol=symbol, start_date=start_time, end_date=end_time, period=period, adjust='qfq')
            
            if df is not None and not df.empty:
                # æˆåŠŸ - é‡ç½®é€€é¿
                if fetch_cached_min_data.current_backoff > 0:
                     print(f"[{datetime.now().time()}] API æ¢å¤ã€‚é‡ç½®é€€é¿æ—¶é—´ã€‚")
                     fetch_cached_min_data.current_backoff = 0

                # ç»Ÿä¸€åˆ—å
                if 'æ—¶é—´' in df.columns:
                    df.rename(columns={'æ—¶é—´': 'time', 'å¼€ç›˜': 'open', 'æ”¶ç›˜': 'close'}, inplace=True)
                
                # ç®€å•æ¸…æ´—
                df['time'] = pd.to_datetime(df['time'])
                
                # è®¡ç®—æ¶¨è·Œå¹…(ç›¸å¯¹äºå½“æ—¥å¼€ç›˜)
                base_price = df['open'].iloc[0]
                df['pct_chg'] = (df['close'] - base_price) / base_price * 100
                
                result_df = df[['time', 'pct_chg', 'close']]
                try:
                    os.makedirs(os.path.dirname(cache_path), exist_ok=True)
                    result_df.to_parquet(cache_path)
                except Exception:
                    pass
                return result_df
                
        except Exception:
            # å¤±è´¥å¤„ç†é€»è¾‘
            if fetch_cached_min_data.current_backoff == 0:
                fetch_cached_min_data.current_backoff = 60 # åˆå§‹ 1 åˆ†é’Ÿ
            else:
                fetch_cached_min_data.current_backoff *= 2 # ç¿»å€
            pass

    return None

# --- åå°é¢„å–çº¿ç¨‹é€»è¾‘ ---
@st.cache_data(ttl=3600*24, show_spinner=False)
def fetch_cached_min_data_wrapper(symbol, date_str, is_index=False, period='1'):
    """Wrapper to be called by background thread"""
    # This is just a direct call to the cached function
    # In background thread, we can call this.
    return fetch_cached_min_data(symbol, date_str, is_index, period)

def background_prefetch_task(date_list, origin_df):
    """
    åå°çº¿ç¨‹ï¼šæ‰§è¡Œæ•°æ®é¢„å–ã€‚
    """
    total_dates = len(date_list)
    print(f"\n[åå°ä»»åŠ¡] å¼€å§‹é¢„å– {total_dates} å¤©çš„æ•°æ®ã€‚")
    
    current_backoff = 0 # ç§’
    
    indices_codes = ["000300", "000001", "399001"]
    
    for i, d in enumerate(date_list):
        d_str = d.strftime("%Y-%m-%d")
        print(f"[åå°ä»»åŠ¡] æ­£åœ¨å¤„ç†: {d_str} ({i+1}/{total_dates})")
        
        # ç­›é€‰
        daily = origin_df[origin_df['æ—¥æœŸ'].dt.date == d]
        if daily.empty: continue
        
        # Top 25
        top_stocks = daily.sort_values('æˆäº¤é¢', ascending=False).head(25)['ä»£ç '].tolist()
        
        # ä»»åŠ¡åˆ—è¡¨
        tasks = []
        for code in indices_codes: tasks.append((code, d_str, True))
        for code in top_stocks: tasks.append((code, d_str, False))
        
        # å†…å±‚é€ä¸ªæ‰§è¡Œ (ä¸ºäº†æ–¹ä¾¿æ§åˆ¶é€€é¿ï¼Œä¸”åå°ä»»åŠ¡ä¸æ€¥äºä¸€æ—¶çš„å¹¶å‘ï¼Œç¨³å®šç¬¬ä¸€)
        for t_code, t_date, t_is_index in tasks:
            
            # Indefinite retry loop with backoff
            while True:
                try:
                    # æ£€æŸ¥é€€é¿
                    if current_backoff > 0:
                        print(f"[åå°ä»»åŠ¡] å¤„äºå†·å´çŠ¶æ€ã€‚ç­‰å¾… {current_backoff} ç§’...")
                        time.sleep(current_backoff)
                        
                    fetch_cached_min_data(t_code, t_date, is_index=t_is_index, period='1')
                    
                    # Success
                    if current_backoff > 0:
                        print(f"[åå°ä»»åŠ¡] å·²æ¢å¤ã€‚é‡ç½®é€€é¿æ—¶é—´ã€‚")
                        current_backoff = 0
                    
                    time.sleep(0.1)
                    break # è·³å‡º whileï¼Œå¤„ç†ä¸‹ä¸€ä¸ª task

                except Exception as e:
                    print(f"[åå°ä»»åŠ¡] è·å– {t_code} ({t_date}) å¤±è´¥: {e}")
                    # è§¦å‘é€€é¿æœºåˆ¶
                    if current_backoff == 0:
                        current_backoff = 60
                    else:
                        current_backoff *= 2
                    
                    print(f"[åå°ä»»åŠ¡] é€€é¿æ—¶é—´å¢åŠ åˆ° {current_backoff}ç§’ã€‚æ­£åœ¨é‡è¯•åŒä¸€ä»»åŠ¡...")
    
    print("[åå°ä»»åŠ¡] æ‰€æœ‰ä»»åŠ¡å·²å®Œæˆã€‚")


def fetch_intraday_data_v2(stock_codes, target_date_str, period='1', max_workers=1, request_delay=0.0):
    """
    åˆ†æ—¶æ•°æ® + æŒ‡æ•°åˆ†æ—¶èµ°åŠ¿åˆå¹¶ (æ–°ç‰ˆ)
    """
    results = []
    log_info(f"å¼€å§‹è·å–åˆ†æ—¶: {target_date_str} | æ ‡çš„æ•° {len(stock_codes)} | å‘¨æœŸ {period} | çº¿ç¨‹ {max_workers} | å»¶è¿Ÿ {request_delay}s")
    
    indices_map = {
        '000300': 'æ²ªæ·±300',
        '000001': 'ä¸Šè¯æŒ‡æ•°',
        '399001': 'æ·±è¯æˆæŒ‡'
    }

    tasks = []

    for idx_code, idx_name in indices_map.items():
        tasks.append({
            'type': 'index',
            'code': idx_code,
            'name': idx_name,
            'to_val': 99999999999
        })

    for code, name, to_val in stock_codes:
        tasks.append({
            'type': 'stock',
            'code': code,
            'name': name,
            'to_val': to_val
        })

    def _worker(task):
        try:
            is_index = (task['type'] == 'index')
            if request_delay > 0:
                time.sleep(request_delay)
            data = fetch_cached_min_data(task['code'], target_date_str, is_index=is_index, period=period)
            if data is not None:
                return {
                    'code': task['code']
                    , 'name': task['name']
                    , 'data': data
                    , 'turnover': task['to_val']
                    , 'is_index': is_index
                }
        except Exception:
            pass
        return None

    ctx = get_script_run_ctx()
    def _worker_wrapper(t):
        if ctx:
            add_script_run_ctx(threading.current_thread(), ctx)
        return _worker(t)

    if max_workers <= 1:
        for t in tasks:
            res = _worker(t)
            if res:
                results.append(res)
    else:
        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_task = {executor.submit(_worker_wrapper, t): t for t in tasks}
            
            for future in concurrent.futures.as_completed(future_to_task):
                res = future.result()
                if res:
                    results.append(res)

    return results
