import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
import akshare as ak
from datetime import datetime, timedelta
import time
import random
import os

# é…ç½®é¡µé¢ä¿¡æ¯
st.set_page_config(
    page_title="Aè‚¡å†å²ç›˜é¢å›æ”¾ç³»ç»Ÿ",
    page_icon="âª",
    layout="wide"
)

# -----------------------------------------------------------------------------
# 1. æ ¸å¿ƒæ•°æ®é€»è¾‘
# -----------------------------------------------------------------------------

CACHE_FILE = "data/csi300_history_cache.parquet"

def get_start_date(years_back=2):
    """è®¡ç®— N å¹´å‰çš„æ—¥æœŸï¼Œè¿”å› YYYYMMDD å­—ç¬¦ä¸²"""
    target = datetime.now() - timedelta(days=365 * years_back)
    return target.strftime("%Y%m%d")

def fetch_history_data():
    """
    è·å–æ²ªæ·±300æˆåˆ†è‚¡è¿‡å»2å¹´çš„æ—¥çº¿æ•°æ®ã€‚
    å¢é‡æ›´æ–°é€»è¾‘ï¼š
    1. å°è¯•è¯»å–æœ¬åœ°ç¼“å­˜ã€‚
    2. å¦‚æœæœ‰ç¼“å­˜ï¼Œæ£€æŸ¥ç¼“å­˜ä¸­æœ€æ–°çš„æ—¥æœŸã€‚
    3. å¦‚æœ æœ€æ–°æ—¥æœŸ < æ˜¨å¤© (æˆ–ä»Šå¤©æ”¶ç›˜å)ï¼Œåˆ™åªä¸‹è½½å¢é‡æ•°æ®ï¼ˆä¸ºäº†ç®€å•å¯é ï¼ŒAkShareæ—¥çº¿æ¥å£é€šå¸¸æ˜¯æŒ‰æ®µä¸‹è½½ï¼Œæˆ–è€…å…¨é‡ä¸‹è½½ï¼‰ã€‚
       * ä¿®æ­£ç­–ç•¥ï¼šç”±äº ak.stock_zh_a_hist æ¥å£å‚æ•°æ˜¯ start_date å’Œ end_dateï¼Œ
         æˆ‘ä»¬å¯ä»¥åªä¸‹è½½ [ç¼“å­˜æœ€æ–°æ—¥æœŸ+1, ä»Šå¤©] çš„æ•°æ®ï¼Œç„¶å append åˆ°ç¼“å­˜ä¸­ã€‚
    """
    cached_df = pd.DataFrame()
    last_cached_date = None

    # 1. å°è¯•åŠ è½½æœ¬åœ°ç¼“å­˜
    if os.path.exists(CACHE_FILE):
        try:
            cached_df = pd.read_parquet(CACHE_FILE)
            if not cached_df.empty:
                last_cached_date = cached_df['æ—¥æœŸ'].max().date()
                st.toast(f"âœ… å·²åŠ è½½æœ¬åœ°ç¼“å­˜ï¼Œæœ€æ–°æ—¥æœŸ: {last_cached_date}")
        except Exception as e:
            st.error(f"è¯»å–ç¼“å­˜æ–‡ä»¶å¤±è´¥: {e}")

    # 2. è®¡ç®—éœ€è¦ä¸‹è½½çš„æ—¶é—´èŒƒå›´
    today = datetime.now().date()
    
    # å¦‚æœå·²ç»æœ‰ä»Šå¤©çš„æœ€æ–°æ•°æ®ï¼ˆå‡è®¾ä¸‹åˆ3ç‚¹åæ‰ç®—ä»Šå¤©ç»“æŸï¼Œç®€å•èµ·è§åªè¦ç¼“å­˜æ—¥æœŸ>=ä»Šå¤© æˆ–è€… >=æ˜¨å¤©ä¸”ç°åœ¨æ²¡æ”¶ç›˜ï¼Œå°±ä¸æ›´æ–°äº†? 
    # ä¸ºäº†ä¸¥è°¨ï¼Œåªè¦ç¼“å­˜æ—¥æœŸ < ä»Šå¤©ï¼Œå°±å°è¯•è·å–å¢é‡ã€‚ä½†è¿™åœ¨ç›˜ä¸­å¯èƒ½ä¼šå¯¼è‡´é‡å¤è·å–æ˜¨å¤©çš„æ•°æ®å¦‚æœæ˜¨å¤©æ˜¯ä¼‘å¸‚æ—¥?
    # ç®€åŒ–é€»è¾‘ï¼šå¦‚æœç¼“å­˜ä¸ºç©ºï¼Œä¸‹è½½è¿‡å»3å¹´ã€‚å¦‚æœç¼“å­˜éç©ºï¼Œä¸‹è½½ [last_date + 1 day, today]ã€‚
    
    if last_cached_date:
        start_date_str = (last_cached_date + timedelta(days=1)).strftime("%Y%m%d")
        # å¦‚æœç¼“å­˜æ˜¯æœ€æ–°çš„ï¼ˆæ¯”å¦‚ä»Šå¤©æ˜¯å‘¨æ—¥ï¼Œç¼“å­˜æ˜¯å‘¨äº”ï¼Œtodayæ˜¯å‘¨æ—¥ï¼Œstart_dateæ˜¯å‘¨å…­ã€‚ä¸‹è½½å‘¨å…­åˆ°å‘¨æ—¥çš„æ•°æ®ä¸ºç©ºï¼Œè¿™æ˜¯okçš„ï¼‰
        if last_cached_date >= today:
            return cached_df
    else:
        start_date_str = get_start_date(2)
        
    end_date_str = today.strftime("%Y%m%d")

    # å¦‚æœä¸éœ€è¦æ›´æ–° (start > end)
    if start_date_str > end_date_str:
        return cached_df

    # çŠ¶æ€å®¹å™¨
    status_text = st.empty()
    progress_bar = st.progress(0)
    
    try:
        # å¦‚æœæ˜¯å¢é‡æ›´æ–°ï¼Œå°±ä¸æ˜¾ç¤ºå¤ªå“äººçš„"æ­£åœ¨è·å–åˆ—è¡¨..."ï¼Œé™¤éèŒƒå›´å¾ˆå¤§
        is_incremental = not cached_df.empty
        if not is_incremental:
            status_text.text("æ­£åœ¨åˆå§‹åŒ–å…¨é‡å†å²æ•°æ®...")
        else:
            status_text.text(f"æ­£åœ¨æ£€æŸ¥å¢é‡æ•°æ® ({start_date_str} - {end_date_str})...")

        # è·å–åˆ—è¡¨
        try:
            cons_df = ak.index_stock_cons(symbol="000300")
        except:
             # å¦‚æœè·å–æˆåˆ†è‚¡åˆ—è¡¨å¤±è´¥ï¼Œä¸”æˆ‘ä»¬æœ‰ç¼“å­˜ï¼Œå°±ç›´æ¥ç”¨ç¼“å­˜ç®—äº†
             if not cached_df.empty:
                 st.warning("ç½‘ç»œè¿æ¥ä¸ç¨³å®šï¼Œæ— æ³•è·å–æœ€æ–°æˆåˆ†è‚¡ï¼Œä»…æ˜¾ç¤ºæœ¬åœ°å†å²æ•°æ®ã€‚")
                 return cached_df
             else:
                 return pd.DataFrame()
        
        # ... å¤„ç†åˆ—å ...
        if cons_df is None or cons_df.empty:
             if not cached_df.empty: return cached_df
             return pd.DataFrame()

        if 'variety' in cons_df.columns:
            code_col, name_col = 'variety', 'name'
        elif 'å“ç§ä»£ç ' in cons_df.columns:
            code_col, name_col = 'å“ç§ä»£ç ', 'å“ç§åç§°'
        else:
            code_col = cons_df.columns[0]
            name_col = cons_df.columns[1]
            
        stock_list = cons_df[code_col].tolist()
        stock_names = dict(zip(cons_df[code_col], cons_df[name_col]))
        
        new_data_list = []
        total_stocks = len(stock_list)
        
        # å¾ªç¯è·å–
        for idx, code in enumerate(stock_list):
            if not is_incremental:
                current_progress = (idx + 1) / total_stocks
                progress_bar.progress(current_progress)
                name = stock_names.get(code, code)
                status_text.text(f"æ­£åœ¨ä¸‹è½½ ({idx+1}/{total_stocks}): {name} ...")
            else:
                # å¢é‡æ›´æ–°æ—¶ä¸æ˜¾ç¤ºé‚£ä¹ˆç»†çš„è¿›åº¦æ¡ï¼Œæˆ–è€…åªåœ¨æ¯10ä¸ªæ˜¾ç¤ºä¸€æ¬¡
                if idx % 10 == 0:
                    status_text.text(f"å¢é‡æ›´æ–°ä¸­: {idx}/{total_stocks} å®Œæˆ...")
            
            try:
                df_hist = ak.stock_zh_a_hist(symbol=code, start_date=start_date_str, end_date=end_date_str, adjust="qfq")
                if df_hist is not None and not df_hist.empty:
                    df_hist = df_hist[['æ—¥æœŸ', 'æ”¶ç›˜', 'æ¶¨è·Œå¹…', 'æˆäº¤é¢']].copy()
                    df_hist['ä»£ç '] = code
                    df_hist['åç§°'] = name
                    new_data_list.append(df_hist)
            except Exception:
                pass
                
        status_text.empty()
        progress_bar.empty()
        
        # åˆå¹¶é€»è¾‘
        if new_data_list:
            new_df = pd.concat(new_data_list, ignore_index=True)
            # ç±»å‹è½¬æ¢
            new_df['æ—¥æœŸ'] = pd.to_datetime(new_df['æ—¥æœŸ'])
            new_df['æ¶¨è·Œå¹…'] = pd.to_numeric(new_df['æ¶¨è·Œå¹…'], errors='coerce')
            new_df['æˆäº¤é¢'] = pd.to_numeric(new_df['æˆäº¤é¢'], errors='coerce')
            new_df['æ”¶ç›˜'] = pd.to_numeric(new_df['æ”¶ç›˜'], errors='coerce')
            
            if cached_df.empty:
                final_df = new_df
            else:
                # åˆå¹¶æ—§æ•°æ®å’Œæ–°æ•°æ®ï¼Œå¹¶å»é‡
                st.toast(f"ğŸ“¥ æˆåŠŸè·å– {len(new_df)} æ¡æ–°è®°å½•")
                final_df = pd.concat([cached_df, new_df], ignore_index=True)
                # æŒ‰ 'æ—¥æœŸ' + 'ä»£ç ' å»é‡ï¼Œä¿ç•™æ–°çš„ï¼ˆå¦‚æœé‡å ï¼‰
                final_df.drop_duplicates(subset=['æ—¥æœŸ', 'ä»£ç '], keep='last', inplace=True)
        else:
            # æ²¡ä¸‹è½½åˆ°æ–°æ•°æ®ï¼ˆå¯èƒ½æ˜¯å‡æœŸï¼‰
            final_df = cached_df
            
        if final_df.empty:
            return pd.DataFrame()

        final_df = final_df.sort_values('æ—¥æœŸ')
        
        # åªæœ‰å½“æœ‰æ–°æ•°æ® æˆ–è€… æ˜¯é¦–æ¬¡ä¸‹è½½æ—¶ï¼Œæ‰ä¿å­˜
        if new_data_list or cached_df.empty:
            try:
                if not os.path.exists("data"):
                    os.makedirs("data")
                final_df.to_parquet(CACHE_FILE)
                if not cached_df.empty:
                    st.toast("ğŸ’¾ å¢é‡æ•°æ®å·²åˆå¹¶å¹¶ä¿å­˜")
                else:
                    st.success("ğŸ’¾ å…¨é‡æ•°æ®å·²åˆå§‹åŒ–")
            except Exception as e:
                st.warning(f"æ— æ³•ä¿å­˜ç¼“å­˜: {e}")

        return final_df

    except Exception as e:
        st.error(f"å…¨å±€æ•°æ®é”™è¯¯: {e}")
        status_text.empty()
        progress_bar.empty()
        return pd.DataFrame()

# -----------------------------------------------------------------------------
@st.cache_data(ttl=3600*24)
def fetch_cached_min_data(symbol, date_str, is_index=False):
    """
    åŸå­åŒ–è·å–å•ä¸ªæ ‡çš„çš„åˆ†æ—¶æ•°æ®ï¼Œç‹¬ç«‹ç¼“å­˜ã€‚
    é¿å…å› è‚¡ç¥¨åˆ—è¡¨ç»„åˆå˜åŒ–å¯¼è‡´æ•´ä¸ªç¼“å­˜å¤±æ•ˆã€‚
    """
    start_time = f"{date_str} 09:30:00"
    end_time = f"{date_str} 15:00:00"
    
    # ç®€å•çš„é‡è¯•æœºåˆ¶
    for _ in range(3):
        try:
            if is_index:
                # æŒ‡æ•°æ¥å£
                df = ak.index_zh_a_hist_min_em(symbol=symbol, period="1", start_date=start_time, end_date=end_time)
            else:
                # ä¸ªè‚¡æ¥å£
                df = ak.stock_zh_a_hist_min_em(symbol=symbol, start_date=start_time, end_date=end_time, period='1', adjust='qfq')
            
            if df is not None and not df.empty:
                # ç»Ÿä¸€åˆ—å
                if 'æ—¶é—´' in df.columns:
                    df.rename(columns={'æ—¶é—´': 'time', 'å¼€ç›˜': 'open', 'æ”¶ç›˜': 'close'}, inplace=True)
                
                # ç®€å•æ¸…æ´—
                df['time'] = pd.to_datetime(df['time'])
                
                # è®¡ç®—æ¶¨è·Œå¹… (ç›¸å¯¹äºå½“æ—¥å¼€ç›˜)
                base_price = df['open'].iloc[0]
                df['pct_chg'] = (df['close'] - base_price) / base_price * 100
                
                return df[['time', 'pct_chg', 'close']]
                
        except Exception:
            time.sleep(0.3 + random.random() * 0.5)
            
    return None

def fetch_intraday_data_v2(stock_codes, target_date_str):
    """
    è·å–æŒ‡å®šè‚¡ç¥¨åˆ—è¡¨ + ä¸‰å¤§æŒ‡æ•° çš„åˆ†é’Ÿçº§æ•°æ®ã€‚
    v2: å¢åŠ ä¸Šè¯ã€æ·±è¯æŒ‡æ•°ï¼Œä¼˜åŒ–ç¼“å­˜ï¼ŒåŸå­åŒ–è°ƒç”¨ã€‚
    æ³¨æ„ï¼šæ­¤å‡½æ•°æœ¬èº«ä¸å†ç¼“å­˜ï¼Œå› ä¸ºå®ƒåªæ˜¯ç»„è£…è€…ï¼Œä¸”è¾“å…¥åˆ—è¡¨ç»å¸¸å˜åŒ–ã€‚
    """
    results = [] 
    
    # å®šä¹‰éœ€è¦è·å–çš„æŒ‡æ•°
    indices_map = {
        "000300": "ğŸ“Š æ²ªæ·±300",
        "000001": "ğŸ“ˆ ä¸Šè¯æŒ‡æ•°",
        "399001": "ğŸ“‰ æ·±è¯æˆæŒ‡"
    }

    # 1. è·å–æŒ‡æ•°æ•°æ®
    for idx_code, idx_name in indices_map.items():
        try:
            idx_data = fetch_cached_min_data(idx_code, target_date_str, is_index=True)
            
            if idx_data is not None:
                results.append({
                    'code': idx_code,
                    'name': idx_name,
                    'data': idx_data,
                    'turnover': 99999999999, # Sort order
                    'is_index': True
                })
        except Exception as e:
            print(f"Index {idx_code} fetch failed: {e}")

    # 2. è·å–ä¸ªè‚¡æ•°æ®
    for code, name, to_val in stock_codes:
        try:
            stk_data = fetch_cached_min_data(code, target_date_str, is_index=False)
            
            if stk_data is not None:
                results.append({
                    'code': code,
                    'name': name,
                    'data': stk_data,
                    'turnover': to_val,
                    'is_index': False
                })
        except Exception:
            pass 
            
    return results

# 2. UI å¸ƒå±€
# -----------------------------------------------------------------------------

st.title("Aè‚¡å†å²ç›˜é¢å›æ”¾ç³»ç»Ÿ (æ²ªæ·±300 Market Replay)")

st.markdown("""
> ğŸ•¹ï¸ **æ“ä½œæŒ‡å—**ï¼š
> 1. ç­‰å¾…æ•°æ®åˆå§‹åŒ–å®Œæˆï¼ˆåˆæ¬¡è¿è¡Œå¯èƒ½éœ€è¦ 2-3 åˆ†é’Ÿï¼‰ã€‚
> 2. æ‹–åŠ¨ä¸‹æ–¹æ»‘å—é€‰æ‹©å†å²æ—¥æœŸã€‚
> 3. è§‚å¯Ÿå½“æ—¥ç›˜é¢çš„èµ„é‡‘æµå‘ä¸çƒ­åº¦ã€‚
""")

# ä¾§è¾¹æ 
with st.sidebar:
    st.header("æ§åˆ¶å°")
    if st.button("ğŸ”„ å¼ºåˆ¶åˆ·æ–°æ•°æ®"):
        if os.path.exists(CACHE_FILE):
            os.remove(CACHE_FILE)
            st.toast("å·²åˆ é™¤æœ¬åœ°ç¼“å­˜ï¼Œå³å°†é‡æ–°è·å–...")
        st.cache_data.clear()
        st.rerun()
    st.info("æ•°æ®æºï¼šæ²ªæ·±300æˆåˆ†è‚¡ (AkShare)")
    st.caption("æ³¨ï¼šæ–¹å—å¤§å°ä½¿ç”¨'æˆäº¤é¢'ä»£æ›¿'å¸‚å€¼'ï¼Œ\nåæ˜ å½“æ—¥äº¤æ˜“çƒ­åº¦ã€‚")

    st.markdown("---")
    st.markdown("### ğŸ› ï¸ æ¿å—è¿‡æ»¤")
    filter_cyb = st.checkbox("å±è”½åˆ›ä¸šæ¿ (300å¼€å¤´)", value=False)
    filter_kcb = st.checkbox("å±è”½ç§‘åˆ›æ¿ (688å¼€å¤´)", value=False)

# åŠ è½½æ•°æ®
with st.spinner("æ­£åœ¨åˆå§‹åŒ–å†å²æ•°æ®ä»“åº“..."):
    origin_df = fetch_history_data()

if not origin_df.empty:
    # --- å…¨å±€è¿‡æ»¤é€»è¾‘ ---
    df = origin_df.copy()
    if filter_cyb:
        df = df[~df['ä»£ç '].astype(str).str.startswith('300')]
    if filter_kcb:
        df = df[~df['ä»£ç '].astype(str).str.startswith('688')]

    if df.empty:
        st.warning("è¿‡æ»¤åæ²¡æœ‰å‰©ä½™è‚¡ç¥¨æ•°æ®ï¼Œè¯·å–æ¶ˆå‹¾é€‰è¿‡æ»¤é€‰é¡¹ã€‚")
        st.stop()

    # --- æ—¶é—´é€‰æ‹©å™¨é€»è¾‘ (Session State ç®¡ç†) ---
    available_dates = sorted(df['æ—¥æœŸ'].dt.date.unique())
    
    if 'selected_date_idx' not in st.session_state:
        st.session_state.selected_date_idx = len(available_dates) - 1

    #ç¡®ä¿ç´¢å¼•ä¸è¶Šç•Œ
    if st.session_state.selected_date_idx >= len(available_dates):
        st.session_state.selected_date_idx = len(available_dates) - 1
    if st.session_state.selected_date_idx < 0:
        st.session_state.selected_date_idx = 0

    # å¸ƒå±€ï¼šå‰ä¸€å¤© | æ»‘å— | åä¸€å¤©
    st.markdown("### ğŸ“… é€‰æ‹©å›æ”¾æ—¥æœŸ")
    
    # æ¨¡å¼é€‰æ‹©
    mode_col1, mode_col2 = st.columns([1, 3])
    with mode_col1:
        playback_mode = st.radio("å›æ”¾æ¨¡å¼", ["å•æ—¥å¤ç›˜", "å¤šæ—¥èµ°åŠ¿æ‹¼æ¥"], horizontal=True)

    if playback_mode == "å•æ—¥å¤ç›˜":
        col_prev, col_slider, col_next = st.columns([1, 6, 1])
        
        with col_prev:
            st.write("") 
            st.write("")
            if st.button("â¬…ï¸ å‰ä¸€å¤©"):
                if st.session_state.selected_date_idx > 0:
                    st.session_state.selected_date_idx -= 1
                    st.rerun()

        with col_next:
            st.write("")
            st.write("")
            if st.button("åä¸€å¤© â¡ï¸"):
                if st.session_state.selected_date_idx < len(available_dates) - 1:
                    st.session_state.selected_date_idx += 1
                    st.rerun()

        with col_slider:
            # åŸ select_slider æ›¿æ¢ä¸º date_input ä»¥æ”¯æŒå¿«é€Ÿå¹´ä»½é€‰æ‹©
            current_date_val = available_dates[st.session_state.selected_date_idx]
            
            picked_date = st.date_input(
                "æ—¥æœŸ",
                value=current_date_val,
                min_value=available_dates[0],
                max_value=available_dates[-1],
                label_visibility="collapsed"
            )
            
            # å¦‚æœæ—¥æœŸå‘ç”Ÿå˜åŒ–
            if picked_date != current_date_val:
                if picked_date in available_dates:
                    st.session_state.selected_date_idx = available_dates.index(picked_date)
                else:
                    # å¦‚æœé€‰ä¸­çš„æ˜¯éäº¤æ˜“æ—¥ï¼Œå¯»æ‰¾æœ€è¿‘çš„äº¤æ˜“æ—¥
                    closest_date = min(available_dates, key=lambda d: abs(d - picked_date))
                    st.session_state.selected_date_idx = available_dates.index(closest_date)
                    st.toast(f"ğŸ“… ä¼‘å¸‚æ—¥ï¼Œå·²è‡ªåŠ¨å®šä½åˆ°æœ€è¿‘äº¤æ˜“æ—¥: {closest_date}")
                st.rerun()
        
        target_dates = [available_dates[st.session_state.selected_date_idx]] # ä½¿ç”¨ state ä¸­çš„æ—¥æœŸ
        selected_date = target_dates[0]
        
    else: # å¤šæ—¥èµ°åŠ¿æ‹¼æ¥
        with mode_col2:
            date_range = st.date_input(
                "é€‰æ‹©æ—¶é—´èŒƒå›´ (å»ºè®®ä¸è¶…è¿‡5å¤©ï¼Œå¦åˆ™åŠ è½½è¾ƒæ…¢)",
                value=[available_dates[-5] if len(available_dates)>5 else available_dates[0], available_dates[-1]],
                min_value=available_dates[0],
                max_value=available_dates[-1]
            )
        
        if len(date_range) == 2:
            start_d, end_d = date_range
            # ç­›é€‰å‡ºèŒƒå›´å†…çš„äº¤æ˜“æ—¥
            target_dates = [d for d in available_dates if start_d <= d <= end_d]
            if not target_dates: # å¦‚æœé€‰å®šçš„èŒƒå›´å†…æ²¡æœ‰äº¤æ˜“æ—¥ (ä¾‹å¦‚å…¨é€‰äº†å‡æœŸ)
                 st.warning("âš ï¸ é€‰å®šèŒƒå›´å†…æ— äº¤æ˜“æ•°æ®ï¼Œå·²è‡ªåŠ¨é‡ç½®ä¸ºæœ€è¿‘äº¤æ˜“æ—¥")
                 target_dates = [available_dates[-1]]
            
            st.info(f"å·²é€‰æ‹© {len(target_dates)} ä¸ªäº¤æ˜“æ—¥è¿›è¡Œæ‹¼æ¥å±•ç¤º")
            selected_date = target_dates[-1] # ç”¨äºä¸‹æ–¹æ˜¾ç¤ºç»Ÿè®¡é¢æ¿çš„åŸºå‡†
        else:
            st.warning("è¯·é€‰æ‹©å®Œæ•´çš„å¼€å§‹å’Œç»“æŸæ—¥æœŸ")
            target_dates = [available_dates[-1]]
            selected_date = available_dates[-1]

    # --- æ•°æ®åˆ‡ç‰‡ä¸ç»Ÿè®¡ (ä»¥æœ€åä¸€å¤©æˆ–é€‰ä¸­æ—¥ä¸ºå‡†) ---
    daily_df = df[df['æ—¥æœŸ'].dt.date == selected_date].copy()
    
    if daily_df.empty:
        st.warning(f"{selected_date} å½“æ—¥æ— äº¤æ˜“æ•°æ®ï¼ˆå¯èƒ½æ˜¯éäº¤æ˜“æ—¥æˆ–æ•°æ®ç¼ºå¤±ï¼‰ã€‚")
    else:
        # å½“æ—¥ç»Ÿè®¡æŒ‡æ ‡
        median_chg = daily_df['æ¶¨è·Œå¹…'].median()
        total_turnover = daily_df['æˆäº¤é¢'].sum() / 1e8 # äº¿å…ƒ
        top_gainer = daily_df.loc[daily_df['æ¶¨è·Œå¹…'].idxmax()]
        top_loser = daily_df.loc[daily_df['æ¶¨è·Œå¹…'].idxmin()]
        
        # æ˜¾ç¤ºæŒ‡æ ‡è¡Œ
        col1, col2, col3, col4 = st.columns(4)
        col1.metric("å½“å‰å›æ”¾æ—¥æœŸ", selected_date.strftime("%Y-%m-%d"))
        col2.metric("æˆåˆ†è‚¡ä¸­ä½æ•°æ¶¨è·Œ", f"{median_chg:.2f}%", 
                    delta=f"{median_chg:.2f}%", delta_color="normal") # Aè‚¡ä¹ æƒ¯éœ€ç»“åˆ Streamlit theme, ç”¨ normal éœ€è‡ªè¡Œè„‘è¡¥çº¢ç»¿
        col3.metric("æˆåˆ†è‚¡æ€»æˆäº¤", f"{total_turnover:.1f} äº¿")
        col4.metric("é¢†æ¶¨é¾™å¤´", f"{top_gainer['åç§°']} ({top_gainer['æ¶¨è·Œå¹…']:.2f}%)")
    # --- æ–°å¢åŠŸèƒ½ï¼šåˆ†æ—¶èµ°åŠ¿å åŠ  ---
    st.markdown("---")
    st.subheader("ğŸ“ˆ æ ¸å¿ƒèµ„äº§åˆ†æ—¶èµ°åŠ¿å åŠ ")
    
    # æ¨¡å¼é€‰æ‹©
    chart_mode = st.radio("é€‰è‚¡æ¨¡å¼", ["æˆäº¤é¢ Top 10 (æ´»è·ƒåº¦)", "æŒ‡æ•°è´¡çŒ® Top 20 (å½±å“å¤§ç›˜)"], horizontal=True)
    st.caption("æ³¨ï¼šæŒ‡æ•°è´¡çŒ® = æ¶¨è·Œå¹… Ã— æƒé‡(è¿‘ä¼¼ä¸ºæˆäº¤é¢/å¸‚å€¼å æ¯”)ã€‚æ­¤æ¨¡å¼èƒ½çœ‹åˆ°æ˜¯è°åœ¨æ‹‰åŠ¨æˆ–ç ¸ç›˜ã€‚")

    show_intraday = st.checkbox("åŠ è½½åˆ†æ—¶èµ°åŠ¿ (éœ€ä»ç½‘ç»œå®æ—¶æ‹‰å–)", value=False)
    
    if show_intraday:
        with st.spinner(f"æ­£åœ¨æ‹‰å– {len(target_dates)} å¤©çš„åˆ†é’Ÿçº¿æ•°æ® (èŒƒå›´: {target_dates[0]} ~ {target_dates[-1]})..."):
            
            if "æˆäº¤é¢" in chart_mode:
                # åŸé€»è¾‘ï¼šæˆäº¤é¢æœ€é«˜
                top_stocks_df = daily_df.sort_values('æˆäº¤é¢', ascending=False).head(10)
            else:
                # æ–°é€»è¾‘ï¼šæŒ‡æ•°è´¡çŒ®åº¦ Top 20
                # Impact = abs(æ¶¨è·Œå¹… * æˆäº¤é¢) 
                # è¿™é‡Œç”¨ æˆäº¤é¢ è¿‘ä¼¼ å¸‚å€¼æƒé‡ (å› ä¸ºæˆ‘ä»¬æ²¡æœ‰å†å²å¸‚å€¼æ•°æ®ï¼Œæˆäº¤é¢é«˜çš„é€šå¸¸ä¹Ÿæ˜¯æƒé‡å¤§çš„)
                # æ›´ç²¾ç»†ä¸€ç‚¹ï¼šImpact = æ¶¨è·Œå¹… * æˆäº¤é¢ (åŒºåˆ†æ­£è´Ÿ)
                # æˆ‘ä»¬å– ç»å¯¹å€¼æœ€å¤§çš„å‰20ï¼Œå³ æ¶¨å¾—æœ€çŒ›çš„æƒé‡è‚¡ å’Œ è·Œå¾—æœ€çŒ›çš„æƒé‡è‚¡
                daily_df['abs_impact'] = (daily_df['æ¶¨è·Œå¹…'] * daily_df['æˆäº¤é¢']).abs()
                top_stocks_df = daily_df.sort_values('abs_impact', ascending=False).head(20)

            # å‡†å¤‡å‚æ•°åˆ—è¡¨
            target_stocks_list = []
            for _, row in top_stocks_df.iterrows():
                target_stocks_list.append((row['ä»£ç '], row['åç§°'], 0)) # Turnover temporarily 0, unused in fetch
            
            # å¾ªç¯è·å–æ‰€æœ‰ç›®æ ‡æ—¥æœŸçš„æ•°æ®å¹¶åˆå¹¶
            all_intraday_data = [] # List of results
            
            # ç”¨æˆ·è¦æ±‚ç§»é™¤é™åˆ¶ï¼Œä½†ä¸ºäº†é˜²æ­¢æ— å“åº”ï¼Œä»…åœ¨æ•°é‡æå¤šæ—¶æç¤º
            if len(target_dates) > 10 and playback_mode == "å¤šæ—¥èµ°åŠ¿æ‹¼æ¥":
                st.toast(f"âš ï¸ æ‚¨é€‰æ‹©äº† {len(target_dates)} å¤©çš„æ•°æ®ï¼Œæ‹‰å–å’Œæ¸²æŸ“å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ï¼Œè¯·è€å¿ƒç­‰å¾…...")
            
            target_dates_to_fetch = target_dates

            # è¿›åº¦æ¡
            fetch_progress = st.progress(0)
            
            for i, d_date in enumerate(target_dates_to_fetch):
                fetch_progress.progress((i + 1) / len(target_dates_to_fetch))
                d_str = d_date.strftime("%Y-%m-%d")
                
                # è·å–è¯¥æ—¥æ‰€æœ‰æ•°æ®
                # æ³¨æ„ï¼šturnover éœ€è¦ä¼ å…¥è¯¥æ—¥å®é™…çš„ turnoverï¼Œè¿™é‡Œæˆ‘ä»¬åšä¸€ä¸ªç®€åŒ–ï¼š
                # ä¾ç„¶ç”¨ fetch_intraday_data_v2ï¼Œä½†å®ƒè¿”å›çš„ turnover æ˜¯è¾“å…¥å‚æ•°ã€‚
                # å®é™…ä¸Šç”»å›¾æ—¶æˆ‘ä»¬å¸Œæœ›çº¿å®½éšã€å½“æ—¥ã€‘æˆäº¤é¢å˜åŒ–ï¼Ÿæˆ–è€…ä¿æŒä¸€è‡´ï¼Ÿ
                # å¦‚æœæ˜¯å¤šæ—¥æ‹¼æ¥ï¼Œå»ºè®®çº¿å®½å›ºå®šæˆ–å–å¹³å‡ã€‚ç®€å•èµ·è§ï¼Œçº¿å®½ä½¿ç”¨æœ€åä¸€å¤©çš„æˆäº¤é¢å®šçº§ã€‚
                
                day_results = fetch_intraday_data_v2(target_stocks_list, d_str)
                
                # ä¸ºæ•°æ®æ·»åŠ  'date_str' æ ‡è¯†
                for res in day_results:
                     res['data']['date_col'] = d_str
                     res['real_date'] = d_date
                
                all_intraday_data.extend(day_results)
            
            fetch_progress.empty()
            
            if not all_intraday_data:
                st.warning("æœªèƒ½è·å–åˆ°åˆ†æ—¶æ•°æ®")
            else:
                # --- æ•°æ®é‡ç»„ ---
                # å°†åˆ†æ•£çš„æ•°æ®åˆå¹¶ä¸ºï¼š { 'code': { 'name':..., 'is_index':..., 'full_data': DataFrame } }
                combined_series = {}
                
                for item in all_intraday_data:
                    code = item['code']
                    if code not in combined_series:
                        # æŸ¥æ‰¾è¯¥è‚¡ç¥¨åœ¨é€‰å®šæ—¥(æœ€åä¸€æ—¥)çš„æˆäº¤é¢ï¼Œç”¨äºå®šçº¿å®½
                        to_val = 0
                        if not item.get('is_index'):
                            matches = daily_df[daily_df['ä»£ç '] == code]
                            if not matches.empty:
                                to_val = matches.iloc[0]['æˆäº¤é¢']
                        
                        combined_series[code] = {
                            'name': item['name'],
                            'code': code,
                            'is_index': item.get('is_index', False),
                            'turnover': to_val, # ä½¿ç”¨æœ€åä¸€å¤©çš„æˆäº¤é¢
                            'dfs': []
                        }
                    combined_series[code]['dfs'].append(item['data'])
                
                # åˆå¹¶ DataFrame å¹¶æ„å»ºè¿ç»­æ—¶é—´è½´
                # ç­–ç•¥ï¼šä¸ä½¿ç”¨çœŸå®æ—¶é—´è½´ï¼Œè€Œæ˜¯ä½¿ç”¨ "äº¤æ˜“åˆ†é’Ÿåºåˆ—" (Trading Minute Index)
                # æ¯å¤© 240 åˆ†é’Ÿã€‚Day 1: 0-239, Day 2: 240-479...
                # éœ€è¦ç”Ÿæˆä¸€ä¸ª Xè½´ Label Map
                
                final_plot_data = [] # List of {idx, pct_chg, ...}
                x_tick_vals = []
                x_tick_text = []
                
                # ä¸ºæ¯ä¸€å¤©ç”Ÿæˆæ ‡å‡†æ—¶é—´åºåˆ— (é¿å…ç¼ºå¤±åˆ†é’Ÿå¯¼è‡´çš„é”™ä½)
                # 09:30 - 11:30 (121 points usually inclusive? Aè‚¡åˆ†é’Ÿçº¿é€šå¸¸åŒ…å« 11:30 å’Œ 15:00)
                # åˆ†é’Ÿçº¿é€šå¸¸æ˜¯ 09:31 - 11:30 (120 mins) 13:01 - 15:00 (120 mins). 
                # AkShare è¿”å›çš„æ•°æ®æ—¶é—´å¦‚æœæ˜¯ 09:30 é€šå¸¸ä»£è¡¨å¼€ç›˜?
                # è®©æˆ‘ä»¬é€šè¿‡è§‚å¯Ÿç¬¬ä¸€æ¡æ•°æ®æ¥å†³å®šé€»è¾‘ï¼Œé€šå¸¸ç›´æ¥ concat å³å¯
                # ä¸ºäº†è§£å†³ GAPï¼Œæˆ‘ä»¬åœ¨ UI ç»˜å›¾å±‚å¼ºåˆ¶æŠŠ x æ˜ å°„ä¸º 0, 1, 2...
                
                # æå–çœŸæ­£è·å–åˆ°æ•°æ®çš„æ—¥æœŸåˆ—è¡¨ (å»é™¤èŠ‚å‡æ—¥/æ— æ•°æ®æ—¥)
                valid_dates = set()
                for item in all_intraday_data:
                     if 'real_date' in item:
                         valid_dates.add(item['real_date'].strftime("%Y-%m-%d"))
                
                days_list = sorted(list(valid_dates))
                
                if not days_list:
                    # å¦‚æœè¿‡æ»¤åå±…ç„¶æ²¡äº†ï¼ˆç†è®ºä¸Šå¤–å±‚ checked not emptyï¼‰ï¼Œå›é€€å…œåº•
                     days_list = sorted(list(set([x.strftime("%Y-%m-%d") for x in target_dates_to_fetch])))
                
                # æ„å»ºåŸºå‡†æ—¶é—´ç½‘æ ¼ (Template) - æ¯å¤© 240/241 ä¸ªç‚¹
                # 09:30 - 11:30, 13:00 - 15:00
                dummy_date = "2000-01-01"
                morning_range = pd.date_range(f"{dummy_date} 09:30", f"{dummy_date} 11:30", freq="1min")
                afternoon_range = pd.date_range(f"{dummy_date} 13:00", f"{dummy_date} 15:00", freq="1min")
                daily_time_template = morning_range.union(afternoon_range) # Size approx 242
                
                # è®¡ç®—æ€»åç§»é‡
                points_per_day = len(daily_time_template)
                
                for code, info in combined_series.items():
                    # åˆå¹¶ã€æ’åº
                    full_df = pd.concat(info['dfs']).sort_values(['date_col', 'time'])
                    
                    # é‡æ–°æ„é€  X è½´ (Int)
                    # ç®—æ³•ï¼šå¯¹äºæ¯ä¸€è¡Œï¼Œæ‰¾åˆ°å®ƒæ˜¯ ç¬¬å‡ å¤© çš„ ç¬¬å‡ åˆ†é’Ÿ
                    # x_int = day_index * points_per_day + minute_index_in_day
                    
                    full_df['time_str'] = full_df['time'].dt.strftime("%H:%M:%S")
                    
                    x_values = []
                    
                    for idx, row in full_df.iterrows():
                        d_str = row['date_col']
                        t_str = row['time_str'] # å®Œæ•´æ—¶é—´å¯¹è±¡
                        
                        # ç¡®å®šæ˜¯ç¬¬å‡ å¤©
                        day_idx = days_list.index(d_str) if d_str in days_list else 0
                        
                        # ç¡®å®šæ˜¯å½“å¤©çš„ç¬¬å‡ åˆ†é’Ÿ
                        # ç®€å•è½¬æ¢ï¼šå°æ—¶*60 + åˆ†é’Ÿ
                        t_obj = row['time'] # timestamp
                        mins_from_midnight = t_obj.hour * 60 + t_obj.minute
                        
                        # æŠŠä¸­åˆä¼‘å¸‚çš„æ—¶é—´å‹æ‰
                        # 9:30 (570) -> 11:30 (690). Length 120.
                        # 13:00 (780) -> 15:00 (900). 
                        
                        if mins_from_midnight <= 690: # Morning
                            offset = mins_from_midnight - 570 # 09:30 is 0
                        else: # Afternoon
                            offset = 120 + (mins_from_midnight - 780) # 13:00 starts at 120+
                            
                        final_x = day_idx * (240 + 20) + offset # åŠ 20ä¸ªå•ä½çš„é—´éš”è®©å¤©ä¸å¤©ä¹‹é—´æœ‰ç‚¹ç©ºéš™
                        x_values.append(final_x)
                        
                    full_df['x_int'] = x_values
                    
                    # è®¡ç®—ç´¯è®¡æ¶¨è·Œå¹… (å¦‚æœæ˜¯å¤šæ—¥ï¼Œéœ€è¦é“¾å¼è®¡ç®—ï¼Ÿ)
                    # ç®€å•æ–¹æ¡ˆï¼šæ¯å¤©é‡æ–°å½’é›¶ï¼Ÿè¿˜æ˜¯å¤šæ—¥è¿è´¯ï¼Ÿ
                    # ç”¨æˆ·è¯´ "æ‹¼èµ·æ¥å½¢æˆå®Œæ•´çš„å›¾"ï¼Œé€šå¸¸æ„å‘³ç€è¿è´¯è¶‹åŠ¿ã€‚
                    # ä»¥ç¬¬ä¸€å¤©å¼€ç›˜ä»·ä¸ºåŸºå‡†
                    base_price = full_df['close'].iloc[0]
                    # å¦‚æœä¸­é—´æœ‰æ–­ç‚¹ï¼Œç®€å•çš„ (close - base) / base å¯èƒ½å¤±çœŸï¼ˆå› ä¸ºæ˜¨æ”¶...ï¼‰
                    # å‡†ç¡®åšæ³•ï¼šç´¯ä¹˜æ¯å¤©çš„æ¶¨è·Œå¹…ã€‚
                    # ä½†è¿™é‡Œåªè¦å¤§æ¦‚è¶‹åŠ¿ã€‚å¦‚æœç”¨ (Px - P0) / P0ï¼Œé‚£è·¨æ—¥ç¼ºå£ä¼šä½“ç°ä¸ºç›´çº¿çš„è·³è·ƒã€‚
                    # è¿™ç¬¦åˆ "çœŸå®ä»·æ ¼èµ°åŠ¿"ã€‚
                    
                    full_df['cumulative_pct'] = (full_df['close'] - base_price) / base_price * 100
                    
                    info['plot_data'] = full_df
                
                # ç”Ÿæˆ X è½´æ ‡ç­¾ (åªæ˜¾ç¤ºæ¯å¤©çš„ 9:30, 10:30, 11:30/13:00, 14:00, 15:00)
                # æˆ–è€…åªæ˜¾ç¤ºæ—¥æœŸ + å…³é”®ç‚¹
                for i, d_str in enumerate(days_list):
                    base_x = i * (240 + 20)
                    day_label = d_str[5:] # MM-DD
                    # 09:30
                    x_tick_vals.append(base_x)
                    x_tick_text.append(f"{day_label}\n09:30")
                    # 11:30
                    x_tick_vals.append(base_x + 120)
                    x_tick_text.append("11:30/13:00")
                    # 15:00
                    x_tick_vals.append(base_x + 240)
                    x_tick_text.append("15:00")

                # åˆ†ç±»
                idx_data = [v for k,v in combined_series.items() if v['is_index']]
                stk_data = [v for k,v in combined_series.items() if not v['is_index']]
                
                sh_stocks = [v for v in stk_data if v['code'].startswith('6')]
                sz_stocks = [v for v in stk_data if not v['code'].startswith('6')]
                
                sh_index = [v for v in idx_data if v['code'] in ['000001', '000300']]
                sz_index = [v for v in idx_data if v['code'] in ['399001', '000300']]

                # ç»˜å›¾å‡½æ•° v3
                def plot_intraday_v3(stocks, indices, title_suffix):
                    fig = go.Figure()
                    
                    # ä¸ªè‚¡
                    if stocks:
                        max_to = max([s['turnover'] for s in stocks]) if stocks else 1
                        min_to = min([s['turnover'] for s in stocks]) if stocks else 0
                        
                        for s in stocks:
                            if max_to == min_to: width=2
                            else: width = 1 + 3*(s['turnover'] - min_to)/(max_to - min_to)
                            
                            df_p = s['plot_data']
                            last_val = df_p['cumulative_pct'].iloc[-1]
                            color = 'rgba(214, 39, 40, 0.4)' if last_val > 0 else 'rgba(44, 160, 44, 0.4)'
                            
                            fig.add_trace(go.Scatter(
                                x=df_p['x_int'],
                                y=df_p['cumulative_pct'],
                                mode='lines',
                                name=s['name'],
                                line=dict(width=width, color=color),
                                hovertemplate=f"<b>{s['name']}</b><br>æ¶¨è·Œ: %{{y:.2f}}%<br>æ—¶é—´: %{{customdata}}",
                                customdata=df_p['date_col'] + ' ' + df_p['time_str']
                            ))
                            
                    # æŒ‡æ•°
                    idx_colors = {'000300': 'black', '000001': '#d62728', '399001': '#1f77b4'}
                    for idx in indices:
                        df_p = idx['plot_data']
                        c_code = idx.get('code', '000300')
                        
                        fig.add_trace(go.Scatter(
                            x=df_p['x_int'],
                            y=df_p['cumulative_pct'],
                            mode='lines',
                            name=idx['name'],
                            line=dict(width=3, color=idx_colors.get(c_code, 'black')),
                            hovertemplate=f"<b>{idx['name']}</b><br>æ¶¨è·Œ: %{{y:.2f}}%"
                        ))

                    # 3. åˆ†å‰²çº¿ (å¦‚æœæ˜¯å¤šæ—¥)
                    if len(days_list) > 1:
                        for i in range(1, len(days_list)):
                            # åœ¨æ¯ä¸€å¤©å¼€å§‹å‰ç”»ç«–çº¿
                            boundary = i * (240 + 20) - 10
                            fig.add_vline(x=boundary, line_width=1, line_dash="dash", line_color="gray")

                    fig.update_layout(
                        title=f"åˆ†æ—¶èµ°åŠ¿å åŠ  ({'å¤šæ—¥æ‹¼æ¥' if len(days_list)>1 else days_list[0]}) - {title_suffix}",
                        xaxis=dict(
                            tickmode='array',
                            tickvals=x_tick_vals,
                            ticktext=x_tick_text,
                            showgrid=True,
                            showspikes=True, # æ˜¾ç¤ºå‚ç›´è¾…åŠ©çº¿
                            spikemode='across',
                            spikesnap='cursor',
                            showline=True, 
                            linewidth=1, 
                            linecolor='black',
                            mirror=True
                        ),
                        yaxis=dict(
                            showspikes=True # Yè½´ä¹Ÿæ˜¾ç¤ºè¾…åŠ©çº¿ï¼Œæ–¹ä¾¿çœ‹ç‚¹ä½
                        ),
                        yaxis_title="ç´¯è®¡æ¶¨è·Œå¹… (%)",
                        hovermode="x unified", # å¼€å¯ç»Ÿä¸€ Hoverï¼Œæ˜¾ç¤ºè¯¥æ—¶é—´ç‚¹æ‰€æœ‰æ•°æ®
                        height=700, # ç¨å¾®è°ƒé«˜é«˜åº¦ä»¥å®¹çº³æ›´å¤š Hover ä¿¡æ¯
                        legend=dict(orientation="h", y=1.02, x=1, xanchor='right')
                    )
                    return fig

                tab1, tab2 = st.tabs(["æ²ªå¸‚ (SH)", "æ·±å¸‚ (SZ)"])
                
                with tab1:
                    st.plotly_chart(plot_intraday_v3(sh_stocks, sh_index, "æ²ªå¸‚æƒé‡è‚¡"), use_container_width=True)
                with tab2:
                    st.plotly_chart(plot_intraday_v3(sz_stocks, sz_index, "æ·±å¸‚æƒé‡è‚¡"), use_container_width=True)


        st.divider()
        
        # --- å¯è§†åŒ– ---
        st.subheader(f"ğŸ“Š {selected_date.strftime('%Yå¹´%mæœˆ%dæ—¥')} å¸‚åœºå…¨æ™¯çƒ­åŠ›å›¾")
        
        # Aè‚¡ä¸“ç”¨è‰²è°±
        max_limit = 7
        min_limit = -7
        
        fig = px.treemap(
            daily_df,
            path=['åç§°'],
            values='æˆäº¤é¢', # ç”¨æˆäº¤é¢ä»£è¡¨çƒ­åº¦/æƒé‡ (å› ä¸ºå†å²å¸‚å€¼éš¾è·å–)
            color='æ¶¨è·Œå¹…',
            color_continuous_scale=['#00a65a', '#ffffff', '#dd4b39'], # ç»¿ -> ç™½ -> çº¢
            range_color=[min_limit, max_limit],
            hover_data={
                'åç§°': True,
                'ä»£ç ': True,
                'æ”¶ç›˜': True,
                'æ¶¨è·Œå¹…': ':.2f',
                'æˆäº¤é¢': True
            },
            height=650
        )
        
        # ä¼˜åŒ–æ˜¾ç¤º
        fig.update_traces(
            textinfo="label+value+percent entry",
            hovertemplate="<b>%{label}</b><br>æ”¶ç›˜ä»·: %{customdata[2]}<br>æ¶¨è·Œå¹…: %{color:.2f}%<br>æˆäº¤é¢: %{value:.2s}"
        )
        fig.update_layout(
            margin=dict(t=10, l=10, r=10, b=10),
            coloraxis_colorbar=dict(title="æ¶¨è·Œå¹…(%)")
        )
        
        st.plotly_chart(fig, use_container_width=True)
        
        # å¯é€‰ï¼šæ˜¾ç¤ºè¯¦ç»†æ•°æ®è¡¨
        with st.expander("æŸ¥çœ‹å½“æ—¥è¯¦ç»†æ•°æ®"):
            st.dataframe(
                daily_df[['ä»£ç ', 'åç§°', 'æ”¶ç›˜', 'æ¶¨è·Œå¹…', 'æˆäº¤é¢']].style.format({
                    'æ”¶ç›˜': '{:.2f}',
                    'æ¶¨è·Œå¹…': '{:.2f}%',
                    'æˆäº¤é¢': '{:,.0f}'
                })
            )

else:
    st.error("æ•°æ®åŠ è½½å¤±è´¥ï¼Œè¯·åˆ·æ–°é‡è¯•ã€‚")
