import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
import akshare as ak
from datetime import datetime, timedelta
import time
import random
import os
import concurrent.futures
import threading
import sys

# é…ç½®é¡µé¢ä¿¡æ¯
st.set_page_config(
    page_title="Aè‚¡å†å²ç›˜é¢å›æ”¾ç³»ç»Ÿ",
    page_icon="âª",
    layout="wide"
)

# -----------------------------------------------------------------------------
# 1. æ ¸å¿ƒæ•°æ®é€»è¾‘
# -----------------------------------------------------------------------------

CACHE_FILE = "data/csi300_history_cache.parquet"

def get_start_date(years_back=2):
    """è®¡ç®— N å¹´å‰çš„æ—¥æœŸï¼Œè¿”å› YYYYMMDD å­—ç¬¦ä¸²"""
    target = datetime.now() - timedelta(days=365 * years_back)
    return target.strftime("%Y%m%d")

def fetch_history_data():
    """
    è·å–æ²ªæ·±300æˆåˆ†è‚¡è¿‡å»2å¹´çš„æ—¥çº¿æ•°æ®ã€‚
    å¢é‡æ›´æ–°é€»è¾‘ï¼š
    1. å°è¯•è¯»å–æœ¬åœ°ç¼“å­˜ã€‚
    2. å¦‚æœæœ‰ç¼“å­˜ï¼Œæ£€æŸ¥ç¼“å­˜ä¸­æœ€æ–°çš„æ—¥æœŸã€‚
    3. å¦‚æœ æœ€æ–°æ—¥æœŸ < æ˜¨å¤© (æˆ–ä»Šå¤©æ”¶ç›˜å)ï¼Œåˆ™åªä¸‹è½½å¢é‡æ•°æ®ï¼ˆä¸ºäº†ç®€å•å¯é ï¼ŒAkShareæ—¥çº¿æ¥å£é€šå¸¸æ˜¯æŒ‰æ®µä¸‹è½½ï¼Œæˆ–è€…å…¨é‡ä¸‹è½½ï¼‰ã€‚
       * ä¿®æ­£ç­–ç•¥ï¼šç”±äº ak.stock_zh_a_hist æ¥å£å‚æ•°æ˜¯ start_date å’Œ end_dateï¼Œ
         æˆ‘ä»¬å¯ä»¥åªä¸‹è½½ [ç¼“å­˜æœ€æ–°æ—¥æœŸ+1, ä»Šå¤©] çš„æ•°æ®ï¼Œç„¶å append åˆ°ç¼“å­˜ä¸­ã€‚
    """
    cached_df = pd.DataFrame()
    last_cached_date = None

    # 1. å°è¯•åŠ è½½æœ¬åœ°ç¼“å­˜
    if os.path.exists(CACHE_FILE):
        try:
            cached_df = pd.read_parquet(CACHE_FILE)
            if not cached_df.empty:
                last_cached_date = cached_df['æ—¥æœŸ'].max().date()
                st.toast(f"âœ… å·²åŠ è½½æœ¬åœ°ç¼“å­˜ï¼Œæœ€æ–°æ—¥æœŸ: {last_cached_date}")
        except Exception as e:
            st.error(f"è¯»å–ç¼“å­˜æ–‡ä»¶å¤±è´¥: {e}")

    # 2. è®¡ç®—éœ€è¦ä¸‹è½½çš„æ—¶é—´èŒƒå›´
    today = datetime.now().date()
    
    # å¦‚æœç¼“å­˜é‡Œçš„æ—¥æœŸå·²ç»æ˜¯ä»Šå¤©ï¼Œä¸”ç°åœ¨æ˜¯ç›˜ä¸­ï¼Œå¯èƒ½ç”¨æˆ·æƒ³åˆ·æ–°
    # ä½†ç®€å•èµ·è§ï¼Œæˆ‘ä»¬è®¾å®šï¼šå¦‚æœç¼“å­˜æœ€æ–°æ—¥æœŸ < ä»Šå¤©ï¼Œè‚¯å®šè¦å°è¯•ä¸‹è½½ã€‚
    # å¦‚æœç¼“å­˜æœ€æ–°æ—¥æœŸ == ä»Šå¤©ï¼Œåªæœ‰å½“å¼ºåˆ¶åˆ·æ–°æ—¶æ‰é€šè¿‡(å¤–éƒ¨æ§åˆ¶)ï¼Œè¿™é‡Œå‡½æ•°å†…éƒ¨å…ˆå‡è®¾"å·²æ˜¯æœ€æ–°"
    # ä½†ä¸ºäº†æ”¯æŒç›˜ä¸­åˆ·æ–°ï¼Œå¦‚æœ last_cached_date == todayï¼Œæˆ‘ä»¬å…¶å®å¯ä»¥é‡æ‹‰ä»Šå¤©çš„ã€‚
    # è¿™é‡Œæˆ‘ä»¬åªå¤„ç† last_cached_date < today çš„è‡ªåŠ¨å¢é‡, æˆ–è€… force refresh (caller clears cache)
    
    if last_cached_date:
        if last_cached_date >= today:
             # å¦‚æœå·²ç»æœ‰ä»Šå¤©çš„æ•°æ®ï¼Œæš‚æ—¶ç›´æ¥è¿”å› (ç”¨æˆ·éœ€ç‚¹å‡»å¼ºåˆ¶åˆ·æ–°æ¥æ›´æ–°ä»Šæ—¥ç›˜ä¸­æ•°æ®)
             # ä½†ä¸ºäº†èƒ½å¤Ÿ"è‡ªåŠ¨"æ‹‰å–ç›˜ä¸­ï¼Œå¦‚æœ last_cached_date == todayï¼Œæˆ‘ä»¬åšä¸ªåˆ¤æ–­ï¼Ÿ
             # ç°åœ¨çš„é€»è¾‘æ˜¯ï¼šå¦‚æœç¼“å­˜æ–‡ä»¶å­˜åœ¨ä¸”æ—¥æœŸ>=ä»Šå¤©ï¼Œå°±ä¸åŠ¨äº†ã€‚
             # è¿™å¯¼è‡´å¦‚æœæ—©ä¸Š9ç‚¹è·‘äº†ä¸€æ¬¡ï¼ˆæœ‰æ•°æ®ï¼‰ï¼Œä¸‹åˆ3ç‚¹å†è·‘ï¼Œè¿˜æ˜¯æ—§çš„ã€‚
             # æ”¹è¿›ï¼šå¦‚æœæ˜¯ä»Šå¤©ï¼Œä¸”ç°åœ¨è¿˜æ²¡æ”¶ç›˜ï¼Œæˆ–è€…åˆšæ”¶ç›˜ï¼Œå…è®¸è¦†ç›–ï¼Ÿ
             # æš‚ä¿ç•™åŸé€»è¾‘é˜²æ­¢é¢‘ç¹è¯·æ±‚ï¼Œä¾é  "å¼ºåˆ¶åˆ·æ–°" æŒ‰é’®æ¥æ¸…ç©ºç¼“å­˜ã€‚
             return cached_df
        
        start_date_str = (last_cached_date + timedelta(days=1)).strftime("%Y%m%d")
    else:
        start_date_str = get_start_date(2)
        
    end_date_str = today.strftime("%Y%m%d")

    # å¦‚æœä¸éœ€è¦æ›´æ–°
    if start_date_str > end_date_str:
        return cached_df

    # çŠ¶æ€å®¹å™¨
    status_text = st.empty()
    progress_bar = st.progress(0)
    
    try:
        # å¦‚æœæ˜¯å¢é‡æ›´æ–°
        is_incremental = not cached_df.empty
        if not is_incremental:
            status_text.text("æ­£åœ¨åˆå§‹åŒ–å…¨é‡å†å²æ•°æ®...")
        else:
            status_text.text(f"æ­£åœ¨æ£€æŸ¥å¢é‡æ•°æ® ({start_date_str} - {end_date_str})...")

        # è·å–æˆåˆ†è‚¡åˆ—è¡¨
        try:
            cons_df = ak.index_stock_cons(symbol="000300")
        except:
             if not cached_df.empty:
                 st.warning("æˆåˆ†è‚¡åˆ—è¡¨è·å–å¤±è´¥ï¼Œä½¿ç”¨ç¼“å­˜æ•°æ®")
                 return cached_df
             return pd.DataFrame()
        
        if cons_df is None or cons_df.empty:
             return cached_df if not cached_df.empty else pd.DataFrame()

        if 'variety' in cons_df.columns:
            code_col, name_col = 'variety', 'name'
        elif 'å“ç§ä»£ç ' in cons_df.columns:
            code_col, name_col = 'å“ç§ä»£ç ', 'å“ç§åç§°'
        else:
            code_col = cons_df.columns[0]
            name_col = cons_df.columns[1]
            
        stock_list = cons_df[code_col].tolist()
        stock_names = dict(zip(cons_df[code_col], cons_df[name_col]))
        
        new_data_list = []
        total_stocks = len(stock_list)
        
        # --- å°è¯•è·å–ä»Šæ—¥å®æ—¶æ•°æ® (Spot) ä½œä¸ºè¡¥å…… ---
        # å¾ˆå¤šæ—¶å€™ stock_zh_a_hist åœ¨ç›˜ä¸­ä¸è¿”å›å½“æ—¥æ•°æ®ï¼Œæˆ–è€…æœ‰äº›æºä¸è¿”å›ã€‚
        # æˆ‘ä»¬å¯ä»¥æ‹‰å– ak.stock_zh_a_spot_em() è·å–æ‰€æœ‰Aè‚¡å®æ—¶è¡Œæƒ…ï¼Œç„¶åè¿‡æ»¤å‡º CSI300
        # ä»…å½“æˆ‘ä»¬éœ€è¦ "ä»Šå¤©" çš„æ•°æ®æ—¶ (start_date_str <= today_str)
        today_spot_map = {}
        has_today_hist = False # æ ‡è®°æ˜¯å¦é€šè¿‡ hist æ¥å£æ‹¿åˆ°äº†ä»Šå¤©æ•°æ®
        
        if end_date_str >= start_date_str:
             try:
                 spot_df = ak.stock_zh_a_spot_em()
                 if spot_df is not None and not spot_df.empty:
                     # spot_df columns: ä»£ç , åç§°, æœ€æ–°ä»·, æ¶¨è·Œå¹…, æˆäº¤é¢ ...
                     # å»ºç«‹æ˜ å°„: code -> row
                     spot_df['ä»£ç '] = spot_df['ä»£ç '].astype(str)
                     today_spot_map = spot_df.set_index('ä»£ç ').to_dict('index')
             except Exception as e:
                 print(f"Spot fetch failed: {e}")

        # å¾ªç¯è·å–å†å²
        # ä½¿ç”¨ ThreadPoolExecutor åŠ é€Ÿå¢é‡å†å²ä¸‹è½½ (å¦‚æœéœ€è¦ä¸‹è½½å¾ˆå¤šå¤©)
        # ä½† akshare æ¥å£é¢‘ç¹è°ƒç”¨å¯èƒ½å—é™ï¼Œé€‚åº¦å¹¶å‘
        
        def fetch_one_stock(code, name):
            try:
                # è·å–æ—¥çº¿
                df_hist = ak.stock_zh_a_hist(symbol=code, start_date=start_date_str, end_date=end_date_str, adjust="qfq")
                
                # æ£€æŸ¥æ˜¯å¦åŒ…å«ä»Šå¤©
                # å¦‚æœ df_hist ä¸åŒ…å«ä»Šå¤©ï¼Œä½†æˆ‘ä»¬æœ‰ today_spot_mapï¼Œåˆ™äººå·¥è¡¥ä¸€è¡Œ
                fetched_today = False
                if df_hist is not None and not df_hist.empty:
                    df_hist['æ—¥æœŸ'] = pd.to_datetime(df_hist['æ—¥æœŸ'])
                    if end_date_str in df_hist['æ—¥æœŸ'].dt.strftime("%Y%m%d").values:
                        fetched_today = True
                else:
                    df_hist = pd.DataFrame()

                # å¦‚æœæ²¡æœ‰æ‹‰åˆ°ä»Šå¤©çš„æ•°æ®ï¼Œä¸”æˆ‘ä»¬éœ€è¦ä»Šå¤© (end_date_str == today)ï¼Œè¡¥å…¨
                if (not fetched_today) and (end_date_str == datetime.now().strftime("%Y%m%d")):
                    if code in today_spot_map:
                        row = today_spot_map[code]
                        # æ„é€ ä¸€è¡Œ
                        # å¿…é¡»å­—æ®µ: æ—¥æœŸ, æ”¶ç›˜, æ¶¨è·Œå¹…, æˆäº¤é¢, ä»£ç , åç§°
                        # spot row keys: 'æœ€æ–°ä»·', 'æ¶¨è·Œå¹…', 'æˆäº¤é¢'
                        try:
                             new_row = pd.DataFrame([{
                                 'æ—¥æœŸ': pd.to_datetime(end_date_str),
                                 'æ”¶ç›˜': row['æœ€æ–°ä»·'],
                                 'æ¶¨è·Œå¹…': row['æ¶¨è·Œå¹…'],
                                 'æˆäº¤é¢': row['æˆäº¤é¢'],
                                 'ä»£ç ': code,
                                 'åç§°': name
                             }])
                             df_hist = pd.concat([df_hist, new_row], ignore_index=True)
                        except:
                            pass
                
                if df_hist is not None and not df_hist.empty:
                    # ç¡®ä¿åˆ—å­˜åœ¨
                    if 'æ—¥æœŸ' not in df_hist.columns: return None
                    cols_needed = ['æ—¥æœŸ', 'æ”¶ç›˜', 'æ¶¨è·Œå¹…', 'æˆäº¤é¢']
                    for c in cols_needed:
                        if c not in df_hist.columns: return None
                    
                    df_hist = df_hist[cols_needed].copy()
                    df_hist['ä»£ç '] = code
                    df_hist['åç§°'] = name
                    return df_hist
            except Exception:
                pass
            return None

        # å¦‚æœæ˜¯å¢é‡åªå·®1å¤©ï¼Œå…¶å®å•çº¿ç¨‹ä¹Ÿå¿«ã€‚å¦‚æœæ˜¯åˆå§‹åŒ–ï¼Œå¹¶å‘ã€‚
        # Use concurrency
        with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
             future_map = {executor.submit(fetch_one_stock, c, stock_names.get(c, c)): c for c in stock_list}
             
             for i, future in enumerate(concurrent.futures.as_completed(future_map)):
                 # Update progress
                 if i % 10 == 0:
                     progress_bar.progress((i + 1) / total_stocks)
                     status_text.text(f"æ­£åœ¨åŒæ­¥æ•°æ®: {i+1}/{total_stocks}")
                 
                 res = future.result()
                 if res is not None:
                     new_data_list.append(res)
                
        status_text.empty()
        progress_bar.empty()
        
        # åˆå¹¶é€»è¾‘
        if new_data_list:
            new_df = pd.concat(new_data_list, ignore_index=True)
            # ç±»å‹è½¬æ¢
            new_df['æ—¥æœŸ'] = pd.to_datetime(new_df['æ—¥æœŸ'])
            new_df['æ¶¨è·Œå¹…'] = pd.to_numeric(new_df['æ¶¨è·Œå¹…'], errors='coerce')
            new_df['æˆäº¤é¢'] = pd.to_numeric(new_df['æˆäº¤é¢'], errors='coerce')
            new_df['æ”¶ç›˜'] = pd.to_numeric(new_df['æ”¶ç›˜'], errors='coerce')
            
            if cached_df.empty:
                final_df = new_df
            else:
                # åˆå¹¶æ—§æ•°æ®å’Œæ–°æ•°æ®ï¼Œå¹¶å»é‡
                st.toast(f"ğŸ“¥ æˆåŠŸè·å– {len(new_df)} æ¡æ–°è®°å½•")
                final_df = pd.concat([cached_df, new_df], ignore_index=True)
                # æŒ‰ 'æ—¥æœŸ' + 'ä»£ç ' å»é‡ï¼Œä¿ç•™æ–°çš„ï¼ˆå¦‚æœé‡å ï¼‰
                final_df.drop_duplicates(subset=['æ—¥æœŸ', 'ä»£ç '], keep='last', inplace=True)
        else:
            # æ²¡ä¸‹è½½åˆ°æ–°æ•°æ®ï¼ˆå¯èƒ½æ˜¯å‡æœŸï¼‰
            final_df = cached_df
            
        if final_df.empty:
            return pd.DataFrame()

        final_df = final_df.sort_values('æ—¥æœŸ')
        
        # åªæœ‰å½“æœ‰æ–°æ•°æ® æˆ–è€… æ˜¯é¦–æ¬¡ä¸‹è½½æ—¶ï¼Œæ‰ä¿å­˜
        if new_data_list or cached_df.empty:
            try:
                if not os.path.exists("data"):
                    os.makedirs("data")
                final_df.to_parquet(CACHE_FILE)
                if not cached_df.empty:
                    st.toast("ğŸ’¾ å¢é‡æ•°æ®å·²åˆå¹¶å¹¶ä¿å­˜")
                else:
                    st.success("ğŸ’¾ å…¨é‡æ•°æ®å·²åˆå§‹åŒ–")
            except Exception as e:
                st.warning(f"æ— æ³•ä¿å­˜ç¼“å­˜: {e}")

        return final_df

    except Exception as e:
        st.error(f"å…¨å±€æ•°æ®é”™è¯¯: {e}")
        status_text.empty()
        progress_bar.empty()
        return pd.DataFrame()

# -----------------------------------------------------------------------------
@st.cache_data(ttl=3600*24)
def fetch_cached_min_data(symbol, date_str, is_index=False, period='1'):
    """
    åŸå­åŒ–è·å–å•ä¸ªæ ‡çš„çš„åˆ†æ—¶æ•°æ®ï¼Œç‹¬ç«‹ç¼“å­˜ã€‚
    é¿å…å› è‚¡ç¥¨åˆ—è¡¨ç»„åˆå˜åŒ–å¯¼è‡´æ•´ä¸ªç¼“å­˜å¤±æ•ˆã€‚
    params:
    period: '1', '5', '15', '30', '60'
    """
    start_time = f"{date_str} 09:30:00"
    end_time = f"{date_str} 15:00:00"
    
    # æŒ‡æ•°é€€é¿ç­–ç•¥å…¨å±€å˜é‡ (ç®€å•æ¨¡æ‹Ÿï¼Œå®é™…ç¯å¢ƒåº”ç”¨ç±»å°è£…)
    # ä½¿ç”¨å‡½æ•°å±æ€§æš‚å­˜çŠ¶æ€
    if not hasattr(fetch_cached_min_data, "current_backoff"):
        fetch_cached_min_data.current_backoff = 0
            
    # ç®€å•çš„é‡è¯•æœºåˆ¶
    max_retries = 3
    
    # å¦‚æœå¤„äº"å†·å´æœŸ"å†…? è¿™é‡Œç®€åŒ–ä¸ºï¼šæ¯æ¬¡å¤±è´¥åå¢åŠ ç­‰å¾…æ—¶é—´ï¼ŒæˆåŠŸåˆ™é‡ç½®
    
    for attempt in range(max_retries):
        try:
            if is_index:
                # æŒ‡æ•°æ¥å£
                df = ak.index_zh_a_hist_min_em(symbol=symbol, period=period, start_date=start_time, end_date=end_time)
            else:
                # ä¸ªè‚¡æ¥å£
                df = ak.stock_zh_a_hist_min_em(symbol=symbol, start_date=start_time, end_date=end_time, period=period, adjust='qfq')
            
            if df is not None and not df.empty:
                # æˆåŠŸ - é‡ç½®é€€é¿
                if fetch_cached_min_data.current_backoff > 0:
                     print(f"[{datetime.now().time()}] API Recovered. Resetting backoff.")
                     fetch_cached_min_data.current_backoff = 0

                # ç»Ÿä¸€åˆ—å
                if 'æ—¶é—´' in df.columns:
                    df.rename(columns={'æ—¶é—´': 'time', 'å¼€ç›˜': 'open', 'æ”¶ç›˜': 'close'}, inplace=True)
                
                # ç®€å•æ¸…æ´—
                df['time'] = pd.to_datetime(df['time'])
                
                # è®¡ç®—æ¶¨è·Œå¹… (ç›¸å¯¹äºå½“æ—¥å¼€ç›˜)
                base_price = df['open'].iloc[0]
                df['pct_chg'] = (df['close'] - base_price) / base_price * 100
                
                return df[['time', 'pct_chg', 'close']]
                
        except Exception as e:
            # å¤±è´¥å¤„ç†é€»è¾‘
            # å¦‚æœæ˜¯ç‰¹å®šçš„ API é™åˆ¶é”™è¯¯ (éœ€åˆ†æ eï¼Œè¿™é‡Œç®€å•å‡è®¾æ‰€æœ‰å¼‚å¸¸éƒ½å¯èƒ½ç”±é¢‘ç‡å¯¼è‡´)
            # å¢åŠ é€€é¿æ—¶é—´
            if fetch_cached_min_data.current_backoff == 0:
                fetch_cached_min_data.current_backoff = 60 # åˆå§‹ 1 åˆ†é’Ÿ
            else:
                fetch_cached_min_data.current_backoff *= 2 # ç¿»å€
            
            wait_time = fetch_cached_min_data.current_backoff
            
            # åªæœ‰å½“è¿™æ˜¯åå°é¢„å–ä»»åŠ¡æ—¶æ‰è¿›è¡Œé•¿æ—¶é—´ç­‰å¾…? 
            # å‰å°å®æ—¶æ‹‰å–ä¸å®œç­‰å¾…å¤ªä¹…ã€‚è¿™é‡Œæˆ‘ä»¬æ·»åŠ ä¸€ä¸ªä¸Šä¸‹æ–‡åˆ¤æ–­æ˜¯ä¸ç°å®çš„ã€‚
            # ä½†æ—¢ç„¶ç”¨æˆ·æåˆ°äº†"ç¿»å€ç­‰å¾…"ï¼Œè¿™é€šå¸¸æ˜¯é’ˆå¯¹åå°çˆ¬è™«ã€‚
            # å¯¹äºå‰å°äº¤äº’ï¼Œç­‰å¾…1åˆ†é’Ÿç”¨æˆ·æ—©è·‘äº†ã€‚
            # ä¸ºäº†å…¼å®¹ï¼Œæˆ‘ä»¬åªåœ¨ "é¢„å–/çˆ¬è™«" æ¨¡å¼ä¸‹å¯ç”¨æ­¤é€»è¾‘ï¼Ÿ 
            # ä½† fetch_cached_min_data æ˜¯é€šç”¨å‡½æ•°ã€‚
            # å¦¥åï¼šå¦‚æœç­‰å¾…æ—¶é—´å¾ˆé•¿ (>5s)ï¼Œåˆ™å¯ä»¥è®¤ä¸ºè¿™æ˜¯ä¸€ä¸ªéœ€è¦é•¿æ—¶é—´æ¢å¤çš„é”™è¯¯ï¼Œ
            # åœ¨å‰å°ç›´æ¥å¤±è´¥æ¯”è¾ƒå¥½ã€‚åœ¨åå°åˆ™ sleepã€‚
            # ä½†è¿™é‡Œæ— æ³•åŒºåˆ†ã€‚æˆ‘ä»¬å‡è®¾æ­¤ä¸¥æ ¼çš„é€€é¿ç­–ç•¥åªåœ¨å¤–éƒ¨æ§åˆ¶å¾ªç¯ä¸­ç”Ÿæ•ˆæ¯”è¾ƒå¥½ã€‚
            # ä¿®æ”¹ï¼šå°†ä¸¥æ ¼çš„é€€é¿é€»è¾‘ç§»åˆ°è°ƒç”¨æ–¹çš„ loop ä¸­ (Task Worker)ï¼Œ
            # è¿™é‡Œçš„ fetch_cached_min_data åªè´Ÿè´£å•æ¬¡å°è¯•ã€‚
            pass

    return None

# --- æ–°å¢ï¼šåå°é¢„å–çº¿ç¨‹é€»è¾‘ ---
def background_prefetch_task(date_list, origin_df):
    """
    åå°çº¿ç¨‹ï¼šæ‰§è¡Œæ•°æ®é¢„å–ã€‚
    """
    total_dates = len(date_list)
    print(f"\n[Background Worker] Started prefetch for {total_dates} days.")
    
    current_backoff = 0 # ç§’
    
    indices_codes = ["000300", "000001", "399001"]
    
    for i, d in enumerate(date_list):
        d_str = d.strftime("%Y-%m-%d")
        print(f"[Background Worker] Processing: {d_str} ({i+1}/{total_dates})")
        
        # ç­›é€‰
        daily = origin_df[origin_df['æ—¥æœŸ'].dt.date == d]
        if daily.empty: continue
        
        # Top 25
        top_stocks = daily.sort_values('æˆäº¤é¢', ascending=False).head(25)['ä»£ç '].tolist()
        
        # ä»»åŠ¡åˆ—è¡¨
        tasks = []
        for code in indices_codes: tasks.append((code, d_str, True))
        for code in top_stocks: tasks.append((code, d_str, False))
        
        # å†…å±‚é€ä¸ªæ‰§è¡Œ (ä¸ºäº†æ–¹ä¾¿æ§åˆ¶é€€é¿ï¼Œä¸”åå°ä»»åŠ¡ä¸æ€¥äºä¸€æ—¶çš„å¹¶å‘ï¼Œç¨³å®šç¬¬ä¸€)
        # å¦‚æœè¦å¹¶å‘ï¼Œä¹Ÿå¿…é¡»åœ¨å¹¶å‘å‘ç”Ÿå¼‚å¸¸æ—¶æ•è·å¹¶è§¦å‘é€€é¿ã€‚
        # ç®€å•èµ·è§ï¼Œè¿™é‡ŒæŒ‰é¡ºåºæˆ–å°æ‰¹æ¬¡æ‰§è¡Œã€‚
        
        for t_code, t_date, t_is_index in tasks:
            
            # Indefinite retry loop with backoff
            while True:
                try:
                    # æ£€æŸ¥é€€é¿
                    if current_backoff > 0:
                        print(f"[Background Worker] In cool-down state. Waiting {current_backoff} seconds...")
                        time.sleep(current_backoff)
                        
                    # å°è¯•æ‹‰å– (fetch_cached_min_data å†…éƒ¨æœ‰ç¼“å­˜ï¼Œå¦‚æœå·²å­˜åœ¨ä¼šç›´æ¥è¿”å›)
                    # ä¸ºäº†æµ‹è¯• API è¿æ¥ï¼Œå¦‚æœç¼“å­˜å·²å­˜åœ¨ï¼Œå…¶å®ä¸ä¼šè§¦å‘ç½‘ç»œè¯·æ±‚ã€‚
                    # æˆ‘ä»¬éœ€è¦å‡è®¾ fetch_cached_min_data ä¼šå¤„ç†ç½‘ç»œã€‚
                    # æ³¨æ„ï¼šfetch_cached_min_data è¢« @st.cache_data è£…é¥°ã€‚
                    # åœ¨åå°çº¿ç¨‹è°ƒç”¨ st.cache_data è£…é¥°çš„å‡½æ•°é€šå¸¸æ˜¯æ²¡é—®é¢˜çš„ã€‚
                    
                    fetch_cached_min_data(t_code, t_date, is_index=t_is_index, period='1')
                    # åªæœ‰å½“æˆ‘ä»¬éœ€è¦æ›´å¤šæ•°æ®æ—¶æ‰æ‹‰5åˆ†é’Ÿ
                    # fetch_cached_min_data(t_code, t_date, is_index=t_is_index, period='5') 
                    
                    # Success
                    if current_backoff > 0:
                        print(f"[Background Worker] Recovered. Resetting backoff.")
                        current_backoff = 0
                    
                    # æ‹‰å–æˆåŠŸåç¨å¾® sleep ä¸€ä¸‹é¿å…è¿‡äºé¢‘ç¹ (0.1s)
                    time.sleep(0.1)
                    break # è·³å‡º whileï¼Œå¤„ç†ä¸‹ä¸€ä¸ª task

                except Exception as e:
                    print(f"[Background Worker] Error fetching {t_code} on {t_date}: {e}")
                    # è§¦å‘é€€é¿æœºåˆ¶
                    if current_backoff == 0:
                        current_backoff = 60
                    else:
                        current_backoff *= 2
                    
                    print(f"[Background Worker] Backoff increased to {current_backoff}s. Retrying same task...")
                    # Loop continues, will sleep at start of next iteration
    
    print("[Background Worker] All tasks completed.")


def fetch_intraday_data_v2(stock_codes, target_date_str, period='1'):
    """
    è·å–æŒ‡å®šè‚¡ç¥¨åˆ—è¡¨ + ä¸‰å¤§æŒ‡æ•° çš„åˆ†é’Ÿçº§æ•°æ® (å¹¶å‘ç‰ˆ)ã€‚
    v2: å¢åŠ ä¸Šè¯ã€æ·±è¯æŒ‡æ•°ï¼Œä¼˜åŒ–ç¼“å­˜ï¼ŒåŸå­åŒ–è°ƒç”¨ã€‚
    v3: å¼•å…¥å¤šçº¿ç¨‹å¹¶å‘åŠ é€Ÿ
    """
    results = [] 
    
    # å®šä¹‰éœ€è¦è·å–çš„æŒ‡æ•°
    indices_map = {
        "000300": "ğŸ“Š æ²ªæ·±300",
        "000001": "ğŸ“ˆ ä¸Šè¯æŒ‡æ•°",
        "399001": "ğŸ“‰ æ·±è¯æˆæŒ‡"
    }

    # ä»»åŠ¡åˆ—è¡¨
    tasks = []

    # 1. æäº¤æŒ‡æ•°ä»»åŠ¡
    for idx_code, idx_name in indices_map.items():
        tasks.append({
            'type': 'index',
            'code': idx_code,
            'name': idx_name,
            'to_val': 99999999999
        })

    # 2. æäº¤ä¸ªè‚¡ä»»åŠ¡
    for code, name, to_val in stock_codes:
        tasks.append({
            'type': 'stock',
            'code': code,
            'name': name,
            'to_val': to_val
        })
        
    def _worker(task):
        try:
            is_index = (task['type'] == 'index')
            data = fetch_cached_min_data(task['code'], target_date_str, is_index=is_index, period=period)
            if data is not None:
                return {
                    'code': task['code'],
                    'name': task['name'],
                    'data': data,
                    'turnover': task['to_val'],
                    'is_index': is_index
                }
        except Exception:
            pass
        return None

    # å¹¶å‘æ‰§è¡Œ
    # çº¿ç¨‹æ•°ä¸å®œè¿‡å¤šï¼Œä»¥å…è§¦å‘åçˆ¬é™åˆ¶ï¼Œ10-20å·¦å³è¾ƒä¸ºå®‰å…¨
    with concurrent.futures.ThreadPoolExecutor(max_workers=16) as executor:
        future_to_task = {executor.submit(_worker, t): t for t in tasks}
        
        for future in concurrent.futures.as_completed(future_to_task):
            res = future.result()
            if res:
                results.append(res)
            
    return results

# 2. UI å¸ƒå±€
# -----------------------------------------------------------------------------

st.title("Aè‚¡å†å²ç›˜é¢å›æ”¾ç³»ç»Ÿ (æ²ªæ·±300 Market Replay)")

st.markdown("""
> ğŸ•¹ï¸ **æ“ä½œæŒ‡å—**ï¼š
> 1. ç­‰å¾…æ•°æ®åˆå§‹åŒ–å®Œæˆï¼ˆåˆæ¬¡è¿è¡Œå¯èƒ½éœ€è¦ 2-3 åˆ†é’Ÿï¼‰ã€‚
> 2. æ‹–åŠ¨ä¸‹æ–¹æ»‘å—é€‰æ‹©å†å²æ—¥æœŸã€‚
> 3. è§‚å¯Ÿå½“æ—¥ç›˜é¢çš„èµ„é‡‘æµå‘ä¸çƒ­åº¦ã€‚
""")

# ä¾§è¾¹æ 
with st.sidebar:
    st.header("æ§åˆ¶å°")
    if st.button("ğŸ”„ å¼ºåˆ¶åˆ·æ–°æ•°æ®"):
        if os.path.exists(CACHE_FILE):
            os.remove(CACHE_FILE)
            st.toast("å·²åˆ é™¤æœ¬åœ°ç¼“å­˜ï¼Œå³å°†é‡æ–°è·å–...")
        st.cache_data.clear()
        st.rerun()
    st.info("æ•°æ®æºï¼šæ²ªæ·±300æˆåˆ†è‚¡ (AkShare)")
    st.caption("æ³¨ï¼šæ–¹å—å¤§å°ä½¿ç”¨'æˆäº¤é¢'ä»£æ›¿'å¸‚å€¼'ï¼Œ\nåæ˜ å½“æ—¥äº¤æ˜“çƒ­åº¦ã€‚")

    st.markdown("---")
    st.markdown("### ğŸ› ï¸ æ¿å—è¿‡æ»¤")
    filter_cyb = st.checkbox("å±è”½åˆ›ä¸šæ¿ (300å¼€å¤´)", value=False)
    filter_kcb = st.checkbox("å±è”½ç§‘åˆ›æ¿ (688å¼€å¤´)", value=False)
    
# åŠ è½½æ•°æ®
with st.spinner("æ­£åœ¨åˆå§‹åŒ–å†å²æ•°æ®ä»“åº“..."):
    origin_df = fetch_history_data()

# --- åå°ä»»åŠ¡æ£€æµ‹ä¸æ§åˆ¶ ---
# æ£€æŸ¥æ˜¯å¦æœ‰åä¸º "PrefetchWorker" çš„åå°çº¿ç¨‹
bg_thread = None
for t in threading.enumerate():
    if t.name == "PrefetchWorker":
        bg_thread = t
        break

# æ›´æ–° Sidebar UI
with st.sidebar:
    st.markdown("---")
    with st.expander("ğŸ“¥ åå°æ•°æ®é¢„å–", expanded=False):
        st.caption("åå°é™é»˜ä¸‹è½½æœ€è¿‘ N å¤©åˆ†æ—¶æ•°æ®")
        prefetch_days = st.number_input("é¢„å–å¤©æ•°", min_value=5, max_value=200, value=30, step=10)
        
        if bg_thread and bg_thread.is_alive():
            st.info(f"ğŸŸ¢ åå°ä»»åŠ¡è¿è¡Œä¸­...\nè¯·å…³æ³¨æ§åˆ¶å°(Console)æ—¥å¿—")
            # æ— æ³•é€šè¿‡ Button åœæ­¢çº¿ç¨‹ï¼Œé™¤éä½¿ç”¨ Eventã€‚æš‚ä¸å®ç°åœæ­¢ã€‚
        else:
            if st.button("ğŸš€ å¯åŠ¨åå°ä¸‹è½½"):
                if not origin_df.empty:
                    # è·å–æ—¥æœŸåˆ—è¡¨
                    all_dates = sorted(origin_df['æ—¥æœŸ'].dt.date.unique())
                    target_prefetch_dates = all_dates[-prefetch_days:]
                    
                    # å¯åŠ¨çº¿ç¨‹
                    t = threading.Thread(
                        target=background_prefetch_task,
                        args=(target_prefetch_dates, origin_df),
                        name="PrefetchWorker",
                        daemon=True
                    )
                    t.start()
                    st.rerun()
                else:
                    st.error("å†å²æ•°æ®å°šæœªå°±ç»ª")

if not origin_df.empty:
    # --- å…¨å±€è¿‡æ»¤é€»è¾‘ ---
    df = origin_df.copy()
    if filter_cyb:
        df = df[~df['ä»£ç '].astype(str).str.startswith('300')]
    if filter_kcb:
        df = df[~df['ä»£ç '].astype(str).str.startswith('688')]

    if df.empty:
        st.warning("è¿‡æ»¤åæ²¡æœ‰å‰©ä½™è‚¡ç¥¨æ•°æ®ï¼Œè¯·å–æ¶ˆå‹¾é€‰è¿‡æ»¤é€‰é¡¹ã€‚")
        st.stop()

    # --- æ—¶é—´é€‰æ‹©å™¨é€»è¾‘ (Session State ç®¡ç†) ---
    available_dates = sorted(df['æ—¥æœŸ'].dt.date.unique())
    
    if 'selected_date_idx' not in st.session_state:
        st.session_state.selected_date_idx = len(available_dates) - 1

    #ç¡®ä¿ç´¢å¼•ä¸è¶Šç•Œ
    if st.session_state.selected_date_idx >= len(available_dates):
        st.session_state.selected_date_idx = len(available_dates) - 1
    if st.session_state.selected_date_idx < 0:
        st.session_state.selected_date_idx = 0

    # å¸ƒå±€ï¼šå‰ä¸€å¤© | æ»‘å— | åä¸€å¤©
    st.markdown("### ğŸ“… é€‰æ‹©å›æ”¾æ—¥æœŸ")
    
    # æ¨¡å¼é€‰æ‹©
    mode_col1, mode_col2 = st.columns([1, 3])
    with mode_col1:
        playback_mode = st.radio("å›æ”¾æ¨¡å¼", ["å•æ—¥å¤ç›˜", "å¤šæ—¥èµ°åŠ¿æ‹¼æ¥"], horizontal=True)

    if playback_mode == "å•æ—¥å¤ç›˜":
        col_prev, col_slider, col_next = st.columns([1, 6, 1])
        
        with col_prev:
            st.write("") 
            st.write("")
            if st.button("â¬…ï¸ å‰ä¸€å¤©"):
                if st.session_state.selected_date_idx > 0:
                    st.session_state.selected_date_idx -= 1
                    st.rerun()

        with col_next:
            st.write("")
            st.write("")
            if st.button("åä¸€å¤© â¡ï¸"):
                if st.session_state.selected_date_idx < len(available_dates) - 1:
                    st.session_state.selected_date_idx += 1
                    st.rerun()

        with col_slider:
            # åŸ select_slider æ›¿æ¢ä¸º date_input ä»¥æ”¯æŒå¿«é€Ÿå¹´ä»½é€‰æ‹©
            current_date_val = available_dates[st.session_state.selected_date_idx]
            
            picked_date = st.date_input(
                "æ—¥æœŸ",
                value=current_date_val,
                min_value=available_dates[0],
                max_value=available_dates[-1],
                label_visibility="collapsed"
            )
            
            # å¦‚æœæ—¥æœŸå‘ç”Ÿå˜åŒ–
            if picked_date != current_date_val:
                if picked_date in available_dates:
                    st.session_state.selected_date_idx = available_dates.index(picked_date)
                else:
                    # å¦‚æœé€‰ä¸­çš„æ˜¯éäº¤æ˜“æ—¥ï¼Œå¯»æ‰¾æœ€è¿‘çš„äº¤æ˜“æ—¥
                    closest_date = min(available_dates, key=lambda d: abs(d - picked_date))
                    st.session_state.selected_date_idx = available_dates.index(closest_date)
                    st.toast(f"ğŸ“… ä¼‘å¸‚æ—¥ï¼Œå·²è‡ªåŠ¨å®šä½åˆ°æœ€è¿‘äº¤æ˜“æ—¥: {closest_date}")
                st.rerun()
        
        target_dates = [available_dates[st.session_state.selected_date_idx]] # ä½¿ç”¨ state ä¸­çš„æ—¥æœŸ
        selected_date = target_dates[0]
        
    else: # å¤šæ—¥èµ°åŠ¿æ‹¼æ¥
        with mode_col2:
            date_range = st.date_input(
                "é€‰æ‹©æ—¶é—´èŒƒå›´ (å»ºè®®ä¸è¶…è¿‡5å¤©ï¼Œå¦åˆ™åŠ è½½è¾ƒæ…¢)",
                value=[available_dates[-5] if len(available_dates)>5 else available_dates[0], available_dates[-1]],
                min_value=available_dates[0],
                max_value=available_dates[-1]
            )
        
        if len(date_range) == 2:
            start_d, end_d = date_range
            # ç­›é€‰å‡ºèŒƒå›´å†…çš„äº¤æ˜“æ—¥
            target_dates = [d for d in available_dates if start_d <= d <= end_d]
            if not target_dates: # å¦‚æœé€‰å®šçš„èŒƒå›´å†…æ²¡æœ‰äº¤æ˜“æ—¥ (ä¾‹å¦‚å…¨é€‰äº†å‡æœŸ)
                 st.warning("âš ï¸ é€‰å®šèŒƒå›´å†…æ— äº¤æ˜“æ•°æ®ï¼Œå·²è‡ªåŠ¨é‡ç½®ä¸ºæœ€è¿‘äº¤æ˜“æ—¥")
                 target_dates = [available_dates[-1]]
            
            st.info(f"å·²é€‰æ‹© {len(target_dates)} ä¸ªäº¤æ˜“æ—¥è¿›è¡Œæ‹¼æ¥å±•ç¤º")
            selected_date = target_dates[-1] # ç”¨äºä¸‹æ–¹æ˜¾ç¤ºç»Ÿè®¡é¢æ¿çš„åŸºå‡†
        else:
            st.warning("è¯·é€‰æ‹©å®Œæ•´çš„å¼€å§‹å’Œç»“æŸæ—¥æœŸ")
            target_dates = [available_dates[-1]]
            selected_date = available_dates[-1]

    # --- æ•°æ®åˆ‡ç‰‡ä¸ç»Ÿè®¡ (ä»¥æœ€åä¸€å¤©æˆ–é€‰ä¸­æ—¥ä¸ºå‡†) ---
    daily_df = df[df['æ—¥æœŸ'].dt.date == selected_date].copy()
    
    if daily_df.empty:
        st.warning(f"{selected_date} å½“æ—¥æ— äº¤æ˜“æ•°æ®ï¼ˆå¯èƒ½æ˜¯éäº¤æ˜“æ—¥æˆ–æ•°æ®ç¼ºå¤±ï¼‰ã€‚")
    else:
        # å½“æ—¥ç»Ÿè®¡æŒ‡æ ‡
        median_chg = daily_df['æ¶¨è·Œå¹…'].median()
        total_turnover = daily_df['æˆäº¤é¢'].sum() / 1e8 # äº¿å…ƒ
        top_gainer = daily_df.loc[daily_df['æ¶¨è·Œå¹…'].idxmax()]
        top_loser = daily_df.loc[daily_df['æ¶¨è·Œå¹…'].idxmin()]
        
        # æ˜¾ç¤ºæŒ‡æ ‡è¡Œ
        col1, col2, col3, col4 = st.columns(4)
        col1.metric("å½“å‰å›æ”¾æ—¥æœŸ", selected_date.strftime("%Y-%m-%d"))
        col2.metric("æˆåˆ†è‚¡ä¸­ä½æ•°æ¶¨è·Œ", f"{median_chg:.2f}%", 
                    delta=f"{median_chg:.2f}%", delta_color="normal") # Aè‚¡ä¹ æƒ¯éœ€ç»“åˆ Streamlit theme, ç”¨ normal éœ€è‡ªè¡Œè„‘è¡¥çº¢ç»¿
        col3.metric("æˆåˆ†è‚¡æ€»æˆäº¤", f"{total_turnover:.1f} äº¿")
        col4.metric("é¢†æ¶¨é¾™å¤´", f"{top_gainer['åç§°']} ({top_gainer['æ¶¨è·Œå¹…']:.2f}%)")
    # --- æ–°å¢åŠŸèƒ½ï¼šåˆ†æ—¶èµ°åŠ¿å åŠ  ---
    st.markdown("---")
    st.subheader("ğŸ“ˆ æ ¸å¿ƒèµ„äº§åˆ†æ—¶èµ°åŠ¿å åŠ ")
    
    # æ¨¡å¼é€‰æ‹©
    chart_mode = st.radio("é€‰è‚¡æ¨¡å¼", ["æˆäº¤é¢ Top 10 (æ´»è·ƒåº¦)", "æŒ‡æ•°è´¡çŒ® Top 20 (å½±å“å¤§ç›˜)"], horizontal=True)
    st.caption("æ³¨ï¼šæŒ‡æ•°è´¡çŒ® = æ¶¨è·Œå¹… Ã— æƒé‡(è¿‘ä¼¼ä¸ºæˆäº¤é¢/å¸‚å€¼å æ¯”)ã€‚æ­¤æ¨¡å¼èƒ½çœ‹åˆ°æ˜¯è°åœ¨æ‹‰åŠ¨æˆ–ç ¸ç›˜ã€‚")

    show_intraday = st.checkbox("åŠ è½½åˆ†æ—¶èµ°åŠ¿ (éœ€ä»ç½‘ç»œå®æ—¶æ‹‰å–)", value=False)
    
    if show_intraday:
        with st.spinner(f"æ­£åœ¨æ‹‰å– {len(target_dates)} å¤©çš„åˆ†é’Ÿçº¿æ•°æ® (èŒƒå›´: {target_dates[0]} ~ {target_dates[-1]})..."):
            
            if "æˆäº¤é¢" in chart_mode:
                # åŸé€»è¾‘ï¼šæˆäº¤é¢æœ€é«˜
                top_stocks_df = daily_df.sort_values('æˆäº¤é¢', ascending=False).head(10)
            else:
                # æ–°é€»è¾‘ï¼šæŒ‡æ•°è´¡çŒ®åº¦ (ä¸Šæµ· Top 20 + æ·±åœ³ Top 20)
                # Impact = abs(æ¶¨è·Œå¹… * æˆäº¤é¢) 
                daily_df['abs_impact'] = (daily_df['æ¶¨è·Œå¹…'] * daily_df['æˆäº¤é¢']).abs()
                
                # åˆ†åˆ«ç­›é€‰æ²ªå¸‚å’Œæ·±å¸‚
                sh_pool = daily_df[daily_df['ä»£ç '].astype(str).str.startswith('6')].copy()
                sz_pool = daily_df[~daily_df['ä»£ç '].astype(str).str.startswith('6')].copy()
                
                sh_top = sh_pool.sort_values('abs_impact', ascending=False).head(20)
                sz_top = sz_pool.sort_values('abs_impact', ascending=False).head(20)
                
                top_stocks_df = pd.concat([sh_top, sz_top], ignore_index=True)

            # å‡†å¤‡å‚æ•°åˆ—è¡¨
            target_stocks_list = []
            for _, row in top_stocks_df.iterrows():
                target_stocks_list.append((row['ä»£ç '], row['åç§°'], 0)) # Turnover temporarily 0, unused in fetch
            
            # å¾ªç¯è·å–æ‰€æœ‰ç›®æ ‡æ—¥æœŸçš„æ•°æ®å¹¶åˆå¹¶
            all_intraday_data = [] # List of results
            
            # è‡ªåŠ¨è°ƒæ•´æ•°æ®ç²¾åº¦ç­–ç•¥
            # 1åˆ†é’Ÿçº¿é€šå¸¸åªèƒ½è·å–æœ€è¿‘5å¤©
            # 5åˆ†é’Ÿçº¿é€šå¸¸èƒ½è·å–æœ€è¿‘1-2ä¸ªæœˆ
            period_to_use = '1'
            if len(target_dates) > 5 and playback_mode == "å¤šæ—¥èµ°åŠ¿æ‹¼æ¥":
                period_to_use = '5'
                st.info(f"â„¹ï¸ æ‚¨é€‰æ‹©äº† {len(target_dates)} å¤©ï¼šç³»ç»Ÿè‡ªåŠ¨åˆ‡æ¢è‡³ã€5åˆ†é’Ÿçº§ã€‘æ•°æ®ï¼Œä»¥æ”¯æŒæŸ¥çœ‹æ›´ä¹…è¿œçš„å†å²èµ°åŠ¿ã€‚")
            elif len(target_dates) > 10:
                 st.toast(f"âš ï¸ æ‚¨é€‰æ‹©äº† {len(target_dates)} å¤©çš„æ•°æ®ï¼ŒåŠ è½½å¯èƒ½è¾ƒæ…¢ï¼Œè¯·è€å¿ƒç­‰å¾…...")
            
            target_dates_to_fetch = target_dates

            # è¿›åº¦æ¡
            fetch_progress = st.progress(0)
            
            for i, d_date in enumerate(target_dates_to_fetch):
                fetch_progress.progress((i + 1) / len(target_dates_to_fetch))
                d_str = d_date.strftime("%Y-%m-%d")
                
                # è·å–è¯¥æ—¥æ‰€æœ‰æ•°æ®
                # æ³¨æ„ï¼šturnover éœ€è¦ä¼ å…¥è¯¥æ—¥å®é™…çš„ turnoverï¼Œè¿™é‡Œæˆ‘ä»¬åšä¸€ä¸ªç®€åŒ–ï¼š
                # ä¾ç„¶ç”¨ fetch_intraday_data_v2ï¼Œä½†å®ƒè¿”å›çš„ turnover æ˜¯è¾“å…¥å‚æ•°ã€‚
                # å®é™…ä¸Šç”»å›¾æ—¶æˆ‘ä»¬å¸Œæœ›çº¿å®½éšã€å½“æ—¥ã€‘æˆäº¤é¢å˜åŒ–ï¼Ÿæˆ–è€…ä¿æŒä¸€è‡´ï¼Ÿ
                # å¦‚æœæ˜¯å¤šæ—¥æ‹¼æ¥ï¼Œå»ºè®®çº¿å®½å›ºå®šæˆ–å–å¹³å‡ã€‚ç®€å•èµ·è§ï¼Œçº¿å®½ä½¿ç”¨æœ€åä¸€å¤©çš„æˆäº¤é¢å®šçº§ã€‚
                
                day_results = fetch_intraday_data_v2(target_stocks_list, d_str, period=period_to_use)
                
                # ä¸ºæ•°æ®æ·»åŠ  'date_str' æ ‡è¯†
                for res in day_results:
                     res['data']['date_col'] = d_str
                     res['real_date'] = d_date
                
                all_intraday_data.extend(day_results)
            
            fetch_progress.empty()
            
            if not all_intraday_data:
                st.warning("æœªèƒ½è·å–åˆ°åˆ†æ—¶æ•°æ®")
            else:
                # --- æ•°æ®é‡ç»„ ---
                # å°†åˆ†æ•£çš„æ•°æ®åˆå¹¶ä¸ºï¼š { 'code': { 'name':..., 'is_index':..., 'full_data': DataFrame } }
                combined_series = {}
                
                for item in all_intraday_data:
                    code = item['code']
                    if code not in combined_series:
                        # æŸ¥æ‰¾è¯¥è‚¡ç¥¨åœ¨é€‰å®šæ—¥(æœ€åä¸€æ—¥)çš„æˆäº¤é¢ï¼Œç”¨äºå®šçº¿å®½
                        to_val = 0
                        if not item.get('is_index'):
                            matches = daily_df[daily_df['ä»£ç '] == code]
                            if not matches.empty:
                                to_val = matches.iloc[0]['æˆäº¤é¢']
                        
                        combined_series[code] = {
                            'name': item['name'],
                            'code': code,
                            'is_index': item.get('is_index', False),
                            'turnover': to_val, # ä½¿ç”¨æœ€åä¸€å¤©çš„æˆäº¤é¢
                            'dfs': []
                        }
                    combined_series[code]['dfs'].append(item['data'])
                
                # åˆå¹¶ DataFrame å¹¶æ„å»ºè¿ç»­æ—¶é—´è½´
                # ç­–ç•¥ï¼šä¸ä½¿ç”¨çœŸå®æ—¶é—´è½´ï¼Œè€Œæ˜¯ä½¿ç”¨ "äº¤æ˜“åˆ†é’Ÿåºåˆ—" (Trading Minute Index)
                # æ¯å¤© 240 åˆ†é’Ÿã€‚Day 1: 0-239, Day 2: 240-479...
                # éœ€è¦ç”Ÿæˆä¸€ä¸ª Xè½´ Label Map
                
                final_plot_data = [] # List of {idx, pct_chg, ...}
                x_tick_vals = []
                x_tick_text = []
                
                # ä¸ºæ¯ä¸€å¤©ç”Ÿæˆæ ‡å‡†æ—¶é—´åºåˆ— (é¿å…ç¼ºå¤±åˆ†é’Ÿå¯¼è‡´çš„é”™ä½)
                # 09:30 - 11:30 (121 points usually inclusive? Aè‚¡åˆ†é’Ÿçº¿é€šå¸¸åŒ…å« 11:30 å’Œ 15:00)
                # åˆ†é’Ÿçº¿é€šå¸¸æ˜¯ 09:31 - 11:30 (120 mins) 13:01 - 15:00 (120 mins). 
                # AkShare è¿”å›çš„æ•°æ®æ—¶é—´å¦‚æœæ˜¯ 09:30 é€šå¸¸ä»£è¡¨å¼€ç›˜?
                # è®©æˆ‘ä»¬é€šè¿‡è§‚å¯Ÿç¬¬ä¸€æ¡æ•°æ®æ¥å†³å®šé€»è¾‘ï¼Œé€šå¸¸ç›´æ¥ concat å³å¯
                # ä¸ºäº†è§£å†³ GAPï¼Œæˆ‘ä»¬åœ¨ UI ç»˜å›¾å±‚å¼ºåˆ¶æŠŠ x æ˜ å°„ä¸º 0, 1, 2...
                
                # æå–çœŸæ­£è·å–åˆ°æ•°æ®çš„æ—¥æœŸåˆ—è¡¨ (å»é™¤èŠ‚å‡æ—¥/æ— æ•°æ®æ—¥)
                valid_dates = set()
                for item in all_intraday_data:
                     if 'real_date' in item:
                         valid_dates.add(item['real_date'].strftime("%Y-%m-%d"))
                
                days_list = sorted(list(valid_dates))
                
                if not days_list:
                    # å¦‚æœè¿‡æ»¤åå±…ç„¶æ²¡äº†ï¼ˆç†è®ºä¸Šå¤–å±‚ checked not emptyï¼‰ï¼Œå›é€€å…œåº•
                     days_list = sorted(list(set([x.strftime("%Y-%m-%d") for x in target_dates_to_fetch])))
                
                # æ„å»ºåŸºå‡†æ—¶é—´ç½‘æ ¼ (Template) - æ¯å¤© 240/241 ä¸ªç‚¹
                # 09:30 - 11:30, 13:00 - 15:00
                dummy_date = "2000-01-01"
                morning_range = pd.date_range(f"{dummy_date} 09:30", f"{dummy_date} 11:30", freq="1min")
                afternoon_range = pd.date_range(f"{dummy_date} 13:00", f"{dummy_date} 15:00", freq="1min")
                daily_time_template = morning_range.union(afternoon_range) # Size approx 242
                
                # è®¡ç®—æ€»åç§»é‡
                points_per_day = len(daily_time_template)
                
                for code, info in combined_series.items():
                    # åˆå¹¶ã€æ’åº
                    full_df = pd.concat(info['dfs']).sort_values(['date_col', 'time'])
                    
                    # é‡æ–°æ„é€  X è½´ (Int)
                    # ç®—æ³•ï¼šå¯¹äºæ¯ä¸€è¡Œï¼Œæ‰¾åˆ°å®ƒæ˜¯ ç¬¬å‡ å¤© çš„ ç¬¬å‡ åˆ†é’Ÿ
                    # x_int = day_index * points_per_day + minute_index_in_day
                    
                    full_df['time_str'] = full_df['time'].dt.strftime("%H:%M:%S")
                    
                    x_values = []
                    
                    for idx, row in full_df.iterrows():
                        d_str = row['date_col']
                        t_str = row['time_str'] # å®Œæ•´æ—¶é—´å¯¹è±¡
                        
                        # ç¡®å®šæ˜¯ç¬¬å‡ å¤©
                        day_idx = days_list.index(d_str) if d_str in days_list else 0
                        
                        # ç¡®å®šæ˜¯å½“å¤©çš„ç¬¬å‡ åˆ†é’Ÿ
                        # ç®€å•è½¬æ¢ï¼šå°æ—¶*60 + åˆ†é’Ÿ
                        t_obj = row['time'] # timestamp
                        mins_from_midnight = t_obj.hour * 60 + t_obj.minute
                        
                        # æŠŠä¸­åˆä¼‘å¸‚çš„æ—¶é—´å‹æ‰
                        # 9:30 (570) -> 11:30 (690). Length 120.
                        # 13:00 (780) -> 15:00 (900). 
                        
                        if mins_from_midnight <= 690: # Morning
                            offset = mins_from_midnight - 570 # 09:30 is 0
                        else: # Afternoon
                            offset = 120 + (mins_from_midnight - 780) # 13:00 starts at 120+
                            
                        final_x = day_idx * (240 + 20) + offset # åŠ 20ä¸ªå•ä½çš„é—´éš”è®©å¤©ä¸å¤©ä¹‹é—´æœ‰ç‚¹ç©ºéš™
                        x_values.append(final_x)
                        
                    full_df['x_int'] = x_values
                    
                    # è®¡ç®—ç´¯è®¡æ¶¨è·Œå¹… (å¦‚æœæ˜¯å¤šæ—¥ï¼Œéœ€è¦é“¾å¼è®¡ç®—ï¼Ÿ)
                    # ç®€å•æ–¹æ¡ˆï¼šæ¯å¤©é‡æ–°å½’é›¶ï¼Ÿè¿˜æ˜¯å¤šæ—¥è¿è´¯ï¼Ÿ
                    # ç”¨æˆ·è¯´ "æ‹¼èµ·æ¥å½¢æˆå®Œæ•´çš„å›¾"ï¼Œé€šå¸¸æ„å‘³ç€è¿è´¯è¶‹åŠ¿ã€‚
                    # ä»¥ç¬¬ä¸€å¤©å¼€ç›˜ä»·ä¸ºåŸºå‡†
                    base_price = full_df['close'].iloc[0]
                    # å¦‚æœä¸­é—´æœ‰æ–­ç‚¹ï¼Œç®€å•çš„ (close - base) / base å¯èƒ½å¤±çœŸï¼ˆå› ä¸ºæ˜¨æ”¶...ï¼‰
                    # å‡†ç¡®åšæ³•ï¼šç´¯ä¹˜æ¯å¤©çš„æ¶¨è·Œå¹…ã€‚
                    # ä½†è¿™é‡Œåªè¦å¤§æ¦‚è¶‹åŠ¿ã€‚å¦‚æœç”¨ (Px - P0) / P0ï¼Œé‚£è·¨æ—¥ç¼ºå£ä¼šä½“ç°ä¸ºç›´çº¿çš„è·³è·ƒã€‚
                    # è¿™ç¬¦åˆ "çœŸå®ä»·æ ¼èµ°åŠ¿"ã€‚
                    
                    full_df['cumulative_pct'] = (full_df['close'] - base_price) / base_price * 100
                    
                    info['plot_data'] = full_df
                
                # ç”Ÿæˆ X è½´æ ‡ç­¾ (åªæ˜¾ç¤ºæ¯å¤©çš„ 9:30, 10:30, 11:30/13:00, 14:00, 15:00)
                # æˆ–è€…åªæ˜¾ç¤ºæ—¥æœŸ + å…³é”®ç‚¹
                for i, d_str in enumerate(days_list):
                    base_x = i * (240 + 20)
                    day_label = d_str[5:] # MM-DD
                    # 09:30
                    x_tick_vals.append(base_x)
                    x_tick_text.append(f"{day_label}\n09:30")
                    # 11:30
                    x_tick_vals.append(base_x + 120)
                    x_tick_text.append("11:30/13:00")
                    # 15:00
                    x_tick_vals.append(base_x + 240)
                    x_tick_text.append("15:00")

                # åˆ†ç±»
                idx_data = [v for k,v in combined_series.items() if v['is_index']]
                stk_data = [v for k,v in combined_series.items() if not v['is_index']]
                
                sh_stocks = [v for v in stk_data if v['code'].startswith('6')]
                sz_stocks = [v for v in stk_data if not v['code'].startswith('6')]
                
                sh_index = [v for v in idx_data if v['code'] in ['000001', '000300']]
                sz_index = [v for v in idx_data if v['code'] in ['399001', '000300']]

                # ç»˜å›¾å‡½æ•° v3
                def plot_intraday_v3(stocks, indices, title_suffix):
                    fig = go.Figure()
                    
                    # ä¸ªè‚¡
                    if stocks:
                        max_to = max([s['turnover'] for s in stocks]) if stocks else 1
                        min_to = min([s['turnover'] for s in stocks]) if stocks else 0
                        
                        # ç”Ÿæˆ distinct colors
                        color_palette = px.colors.qualitative.Alphabet + px.colors.qualitative.Dark24
                        
                        for i, s in enumerate(stocks):
                            if max_to == min_to: width=2
                            else: width = 1 + 3*(s['turnover'] - min_to)/(max_to - min_to)
                            
                            df_p = s['plot_data']
                            last_val = df_p['cumulative_pct'].iloc[-1]
                            
                            # ä¹‹å‰çš„çº¢ç»¿é€»è¾‘: color = 'rgba(214, 39, 40, 0.4)' if last_val > 0 else 'rgba(44, 160, 44, 0.4)'
                            # ç°åœ¨æ”¹ä¸ºåŒºåˆ†é¢œè‰²
                            color = color_palette[i % len(color_palette)]
                            
                            fig.add_trace(go.Scatter(
                                x=df_p['x_int'],
                                y=df_p['cumulative_pct'],
                                mode='lines',
                                name=s['name'],
                                # line=dict(width=width, color=color),
                                # ä¸ªè‚¡çº¿å®½ä¸éœ€è¦å¤ªç²—ï¼Œé¢œè‰²è¦æ¸…æ™°
                                line=dict(width=max(1.5, width), color=color),
                                hovertemplate=f"<b>{s['name']}</b><br>æ¶¨è·Œ: %{{y:.2f}}%<br>æ—¶é—´: %{{customdata}}",
                                customdata=df_p['date_col'] + ' ' + df_p['time_str']
                            ))
                            
                    # æŒ‡æ•°
                    idx_colors = {'000300': 'black', '000001': '#d62728', '399001': '#1f77b4'}
                    for idx in indices:
                        df_p = idx['plot_data']
                        c_code = idx.get('code', '000300')
                        
                        fig.add_trace(go.Scatter(
                            x=df_p['x_int'],
                            y=df_p['cumulative_pct'],
                            mode='lines',
                            name=idx['name'],
                            line=dict(width=3, color=idx_colors.get(c_code, 'black')),
                            hovertemplate=f"<b>{idx['name']}</b><br>æ¶¨è·Œ: %{{y:.2f}}%"
                        ))

                    # 3. åˆ†å‰²çº¿ (å¦‚æœæ˜¯å¤šæ—¥)
                    if len(days_list) > 1:
                        for i in range(1, len(days_list)):
                            # åœ¨æ¯ä¸€å¤©å¼€å§‹å‰ç”»ç«–çº¿
                            boundary = i * (240 + 20) - 10
                            fig.add_vline(x=boundary, line_width=1, line_dash="dash", line_color="gray")

                    fig.update_layout(
                        title=f"åˆ†æ—¶èµ°åŠ¿å åŠ  ({'å¤šæ—¥æ‹¼æ¥' if len(days_list)>1 else days_list[0]}) - {title_suffix}",
                        xaxis=dict(
                            tickmode='array',
                            tickvals=x_tick_vals,
                            ticktext=x_tick_text,
                            showgrid=True,
                            showspikes=True, # æ˜¾ç¤ºå‚ç›´è¾…åŠ©çº¿
                            spikemode='across',
                            spikesnap='cursor',
                            showline=True, 
                            linewidth=1, 
                            linecolor='black',
                            mirror=True
                        ),
                        yaxis=dict(
                            showspikes=True # Yè½´ä¹Ÿæ˜¾ç¤ºè¾…åŠ©çº¿ï¼Œæ–¹ä¾¿çœ‹ç‚¹ä½
                        ),
                        yaxis_title="ç´¯è®¡æ¶¨è·Œå¹… (%)",
                        hovermode="x unified", # å¼€å¯ç»Ÿä¸€ Hoverï¼Œæ˜¾ç¤ºè¯¥æ—¶é—´ç‚¹æ‰€æœ‰æ•°æ®
                        height=700, # ç¨å¾®è°ƒé«˜é«˜åº¦ä»¥å®¹çº³æ›´å¤š Hover ä¿¡æ¯
                        legend=dict(orientation="h", y=1.02, x=1, xanchor='right') # ä¿æŒå›¾ä¾‹çš„å¸ƒå±€
                    )
                    
                    # âš ï¸ å…³é”®ä¿®æ­£ï¼šç¡®ä¿ Hover Tooltip çš„æ’åºæŒ‰ç…§ Y è½´æ•°å€¼ (ä»é«˜åˆ°ä½)
                    # "closest" æ¨¡å¼é…åˆ "compare" å¯èƒ½æ— æ³•ç”Ÿæ•ˆï¼Œä½†åœ¨ "x unified" æ¨¡å¼ä¸‹ï¼Œ
                    # é»˜è®¤æ˜¯æŒ‰ç…§ Trace æ·»åŠ é¡ºåºæ’åºçš„ã€‚
                    # Plotly (JSå±‚) åœ¨ x unified ä¸‹æœ‰é»˜è®¤æ’åºé€»è¾‘ (é€šå¸¸æ˜¯ value descending)ï¼Œä½†åœ¨æŸäº›ç‰ˆæœ¬å¯èƒ½ä¸ç¨³å®šã€‚
                    # ä¸ºäº†å¢å¼ºæ’åºä½“éªŒï¼Œæˆ‘ä»¬å¯ä»¥å°è¯•è®¾ç½® layout.hoverlabel.namelength = -1
                    
                    fig.update_layout(hoverlabel=dict(namelength=-1))
                    
                    return fig

                tab1, tab2 = st.tabs(["æ²ªå¸‚ (SH)", "æ·±å¸‚ (SZ)"])
                
                with tab1:
                    st.plotly_chart(plot_intraday_v3(sh_stocks, sh_index, "æ²ªå¸‚æƒé‡è‚¡"), use_container_width=True)
                with tab2:
                    st.plotly_chart(plot_intraday_v3(sz_stocks, sz_index, "æ·±å¸‚æƒé‡è‚¡"), use_container_width=True)


        st.divider()
        
        # --- å¯è§†åŒ– ---
        st.subheader(f"ğŸ“Š {selected_date.strftime('%Yå¹´%mæœˆ%dæ—¥')} å¸‚åœºå…¨æ™¯çƒ­åŠ›å›¾")
        
        # Aè‚¡ä¸“ç”¨è‰²è°±
        max_limit = 7
        min_limit = -7
        
        fig = px.treemap(
            daily_df,
            path=['åç§°'],
            values='æˆäº¤é¢', # ç”¨æˆäº¤é¢ä»£è¡¨çƒ­åº¦/æƒé‡ (å› ä¸ºå†å²å¸‚å€¼éš¾è·å–)
            color='æ¶¨è·Œå¹…',
            color_continuous_scale=['#00a65a', '#ffffff', '#dd4b39'], # ç»¿ -> ç™½ -> çº¢
            range_color=[min_limit, max_limit],
            hover_data={
                'åç§°': True,
                'ä»£ç ': True,
                'æ”¶ç›˜': True,
                'æ¶¨è·Œå¹…': ':.2f',
                'æˆäº¤é¢': True
            },
            height=650
        )
        
        # ä¼˜åŒ–æ˜¾ç¤º
        fig.update_traces(
            textinfo="label+value+percent entry",
            hovertemplate="<b>%{label}</b><br>æ”¶ç›˜ä»·: %{customdata[2]}<br>æ¶¨è·Œå¹…: %{color:.2f}%<br>æˆäº¤é¢: %{value:.2s}"
        )
        fig.update_layout(
            margin=dict(t=10, l=10, r=10, b=10),
            coloraxis_colorbar=dict(title="æ¶¨è·Œå¹…(%)")
        )
        
        st.plotly_chart(fig, use_container_width=True)
        
        # å¯é€‰ï¼šæ˜¾ç¤ºè¯¦ç»†æ•°æ®è¡¨
        with st.expander("æŸ¥çœ‹å½“æ—¥è¯¦ç»†æ•°æ®"):
            st.dataframe(
                daily_df[['ä»£ç ', 'åç§°', 'æ”¶ç›˜', 'æ¶¨è·Œå¹…', 'æˆäº¤é¢']].style.format({
                    'æ”¶ç›˜': '{:.2f}',
                    'æ¶¨è·Œå¹…': '{:.2f}%',
                    'æˆäº¤é¢': '{:,.0f}'
                })
            )

else:
    st.error("æ•°æ®åŠ è½½å¤±è´¥ï¼Œè¯·åˆ·æ–°é‡è¯•ã€‚")
