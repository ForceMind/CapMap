import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
import akshare as ak
from datetime import datetime, timedelta
import time
import random
import os
import concurrent.futures
import threading
import sys
import io
import zipfile
import shutil
import json
import logging
import html

# å°è¯•å¯¼å…¥ Streamlit ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œç”¨äºè§£å†³å¤šçº¿ç¨‹ "missing ScriptRunContext" è­¦å‘Š
try:
    from streamlit.runtime.scriptrunner import add_script_run_ctx, get_script_run_ctx
except ImportError:
    # å…¼å®¹æ—§ç‰ˆæœ¬ Streamlit
    from streamlit.scriptrunner import add_script_run_ctx, get_script_run_ctx

# é…ç½®é¡µé¢ä¿¡æ¯
st.set_page_config(
    page_title="Aè‚¡å†å²ç›˜é¢å›æ”¾ç³»ç»Ÿ",
    page_icon="âª",
    layout="wide"
)

# -----------------------------------------------------------------------------
from core.data_access import *

# 2. UI å¸ƒå±€
# -----------------------------------------------------------------------------

st.title("Aè‚¡å†å²ç›˜é¢å›æ”¾ç³»ç»Ÿ (æ²ªæ·±300 Market Replay)")

st.markdown("""
> ğŸ•¹ï¸ **æ“ä½œæŒ‡å—**ï¼š
> 1. ç­‰å¾…æ•°æ®åˆå§‹åŒ–å®Œæˆï¼ˆåˆæ¬¡è¿è¡Œå¯èƒ½éœ€è¦ 2-3 åˆ†é’Ÿï¼‰ã€‚
> 2. æ‹–åŠ¨ä¸‹æ–¹æ»‘å—é€‰æ‹©å†å²æ—¥æœŸã€‚
> 3. è§‚å¯Ÿå½“æ—¥ç›˜é¢çš„èµ„é‡‘æµå‘ä¸çƒ­åº¦ã€‚
""")

# ä¾§è¾¹æ 
with st.sidebar:
    st.header("âš™ï¸ æ•°æ®ç®¡ç†")
    
    with st.expander("æ•°æ®åˆ·æ–°ä¸ç»´æŠ¤", expanded=True):
        st.write("å¦‚æœæ•°æ®æ˜¾ç¤ºä¸æ­£ç¡®ï¼Œè¯·å°è¯•ä»¥ä¸‹æ“ä½œï¼š")
        
        # 1. ????
        if st.button("ğŸŸ¢ åˆ·æ–°ä»Šæ—¥è¡Œæƒ… (ç›˜ä¸­)"):
            log_action("åˆ·æ–°ä»Šæ—¥è¡Œæƒ…(ç›˜ä¸­)")
            try:
                if os.path.exists(CACHE_FILE):
                    _df = pd.read_parquet(CACHE_FILE)
                    _today = datetime.now().date()
                    _df_new = _df[_df["æ—¥æœŸ"].dt.date < _today]
                    _df_new.to_parquet(CACHE_FILE)
                    st.toast("å·²æ¸…é™¤ä»Šæ—¥ç¼“å­˜ï¼Œæ­£åœ¨é‡æ–°æ‹‰å–å®æ—¶æ•°æ®...")
                st.cache_data.clear()
                st.rerun()
            except Exception as e:
                st.error(f"æ“ä½œå¤±è´¥: {e}")

        # 2. ??????????
        if st.button("ğŸ§¹ æ¸…ç©ºåˆ†æ—¶å›¾å†…å­˜ç¼“å­˜"):
            log_action("æ¸…ç©ºåˆ†æ—¶å›¾å†…å­˜ç¼“å­˜")
            st.cache_data.clear()
            st.toast("âœ… å†…å­˜ç¼“å­˜å·²æ¸…ç©ºï¼Œç£ç›˜ç¼“å­˜ä¿ç•™ã€‚")

        # 3. ????????
        if st.button("ğŸ”„ æ‰‹åŠ¨æ›´æ–°è‚¡ç¥¨åç§°"):
            codes_hint = st.session_state.get("last_top_codes", [])
            log_action("æ‰‹åŠ¨æ›´æ–°è‚¡ç¥¨åç§°", codes=len(codes_hint))
            name_map = _refresh_name_map_for_codes(codes_hint, force=True)
            if name_map:
                st.toast(f"âœ… å·²æ›´æ–°åç§°æ˜ å°„ï¼š{len(name_map)} æ¡")
            else:
                st.warning("æœªè·å–åˆ°æœ€æ–°åç§°æ˜ å°„ã€‚")

        # 4. ????????
        if st.button("ğŸ—‘ï¸ åˆ é™¤æœ¬åœ°åˆ†æ—¶ç¼“å­˜"):
            log_action("åˆ é™¤æœ¬åœ°åˆ†æ—¶ç¼“å­˜")
            clear_min_cache()
            st.cache_data.clear()
            st.toast("âœ… æœ¬åœ°åˆ†æ—¶ç¼“å­˜å·²åˆ é™¤ã€‚")

        # 5. ????
        if st.button("ğŸš¨ å½»åº•é‡ç½® (åˆ é™¤æ‰€æœ‰)"):
            log_action("å½»åº•é‡ç½®")
            if os.path.exists(CACHE_FILE):
                os.remove(CACHE_FILE)
                st.toast("å·²åˆ é™¤å†å²æ—¥çº¿ç¼“å­˜ã€‚")
            clear_min_cache()
            st.cache_data.clear()
            st.rerun()

    with st.expander("ğŸ’¾ æ•°æ®å¤‡ä»½ä¸æ¢å¤", expanded=False):
        st.caption("å¤‡ä»½ data ç›®å½•ï¼ˆå†å²æ—¥çº¿ + åˆ†æ—¶ç¼“å­˜ï¼‰")
        if st.button("ğŸ“¦ ç”Ÿæˆå¤‡ä»½", key="backup_build"):
            log_action("ç”Ÿæˆå¤‡ä»½")
            data_bytes = build_data_backup_zip()
            if data_bytes:
                st.session_state["backup_zip"] = data_bytes
                st.session_state["backup_name"] = f"capmap_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.zip"
                st.toast("âœ… å¤‡ä»½å·²ç”Ÿæˆ")
            else:
                st.warning("æ²¡æœ‰å¯å¤‡ä»½çš„æ•°æ®ã€‚")
        if "backup_zip" in st.session_state:
            download_clicked = st.download_button(
                "â¬‡ï¸ ä¸‹è½½å¤‡ä»½",
                data=st.session_state["backup_zip"],
                file_name=st.session_state.get("backup_name", "capmap_data_backup.zip"),
                mime="application/zip",
                key="backup_download",
            )
            if download_clicked:
                log_action("ä¸‹è½½å¤‡ä»½")
        uploaded = st.file_uploader("æ¢å¤å¤‡ä»½ï¼ˆ.zipï¼‰", type=["zip"], key="backup_upload")
        if uploaded is not None and st.button("â™»ï¸ æ¢å¤å¤‡ä»½", key="backup_restore"):
            log_action("æ¢å¤å¤‡ä»½", file=getattr(uploaded, "name", ""))
            try:
                restored = restore_data_backup(uploaded)
                st.cache_data.clear()
                log_action("æ¢å¤å¤‡ä»½å®Œæˆ", files=restored)
                st.toast(f"âœ… å·²æ¢å¤ {restored} ä¸ªæ–‡ä»¶")
                st.rerun()
            except Exception as e:
                st.error(f"æ¢å¤å¤±è´¥: {e}")
    st.info("æ•°æ®æºï¼šæ²ªæ·±300æˆåˆ†è‚¡ (AkShare)")
    st.caption("æ³¨ï¼šæ–¹å—å¤§å°ä½¿ç”¨'æˆäº¤é¢'ä»£æ›¿'å¸‚å€¼'ï¼Œ\nåæ˜ å½“æ—¥äº¤æ˜“çƒ­åº¦ã€‚")

    st.markdown("---")
    st.markdown("### ğŸ› ï¸ æ¿å—è¿‡æ»¤")
    filter_cyb = st.checkbox("å±è”½åˆ›ä¸šæ¿ (300å¼€å¤´)", value=False)
    filter_kcb = st.checkbox("å±è”½ç§‘åˆ›æ¿ (688å¼€å¤´)", value=False)
    filter_state = (filter_cyb, filter_kcb)
    if st.session_state.get("filter_state") != filter_state:
        st.session_state["filter_state"] = filter_state
        log_action("ç­›é€‰æ¡ä»¶å˜æ›´", cyb=filter_cyb, kcb=filter_kcb)
    
# åŠ è½½æ•°æ®
with st.spinner("æ­£åœ¨åˆå§‹åŒ–å†å²æ•°æ®ä»“åº“..."):
    origin_df = fetch_history_data()
    _start_auto_prefetch_if_needed(origin_df)

# --- åå°ä»»åŠ¡æ£€æµ‹ä¸æ§åˆ¶ ---
# æ£€æŸ¥æ˜¯å¦æœ‰åä¸º "PrefetchWorker" çš„åå°çº¿ç¨‹
bg_thread = None
for t in threading.enumerate():
    if t.name == "PrefetchWorker":
        bg_thread = t
        break

# æ›´æ–° Sidebar UI
with st.sidebar:
    st.markdown("---")
    
    # å¯¼èˆªæ 
    nav_option = st.radio("ğŸ“¡ åŠŸèƒ½å¯¼èˆª", ["âª å†å²ç›˜é¢å›æ”¾", "ğŸŒŠ èµ„é‡‘åç¦»åˆ†æ", "ğŸ—‚ï¸ æ•°æ®ç®¡ç†"], index=0)
    prev_nav = st.session_state.get("nav_option_prev")
    if prev_nav != nav_option:
        st.session_state["nav_option_prev"] = nav_option
        log_action("åŠŸèƒ½å¯¼èˆªåˆ‡æ¢", nav=nav_option)
    
    with st.expander("ğŸ“¥ åå°æ•°æ®é¢„å–", expanded=False):
        st.caption("åå°é™é»˜ä¸‹è½½æœ€è¿‘ N å¤©åˆ†æ—¶æ•°æ®")
        prefetch_days = st.number_input("é¢„å–å¤©æ•°", min_value=5, max_value=200, value=30, step=10)
        
        if bg_thread and bg_thread.is_alive():
            st.info(f"ğŸŸ¢ åå°ä»»åŠ¡è¿è¡Œä¸­...\nè¯·å…³æ³¨æ§åˆ¶å°(Console)æ—¥å¿—")
            # æ— æ³•é€šè¿‡ Button åœæ­¢çº¿ç¨‹ï¼Œé™¤éä½¿ç”¨ Eventã€‚æš‚ä¸å®ç°åœæ­¢ã€‚
        else:
            if st.button("ğŸš€ å¯åŠ¨åå°ä¸‹è½½"):
                log_action("å¯åŠ¨åå°é¢„å–", days=prefetch_days)
                if not origin_df.empty:
                    # è·å–æ—¥æœŸåˆ—è¡¨
                    all_dates = sorted(origin_df['æ—¥æœŸ'].dt.date.unique())
                    target_prefetch_dates = all_dates[-prefetch_days:]
                    
                    # å¯åŠ¨çº¿ç¨‹
                    t = threading.Thread(
                        target=background_prefetch_task,
                        args=(target_prefetch_dates, origin_df),
                        name="PrefetchWorker",
                        daemon=True
                    )
                    add_script_run_ctx(t)
                    t.start()
                    st.rerun()
                else:
                    st.error("å†å²æ•°æ®å°šæœªå°±ç»ª")

if not origin_df.empty:
    # --- å…¨å±€è¿‡æ»¤é€»è¾‘ ---
    df = origin_df.copy()
    if filter_cyb:
        df = df[~df['ä»£ç '].astype(str).str.startswith('300')]
    if filter_kcb:
        df = df[~df['ä»£ç '].astype(str).str.startswith('688')]

    if df.empty:
        st.warning("è¿‡æ»¤åæ²¡æœ‰å‰©ä½™è‚¡ç¥¨æ•°æ®ï¼Œè¯·å–æ¶ˆå‹¾é€‰è¿‡æ»¤é€‰é¡¹ã€‚")
        st.stop()

    # --- æ—¶é—´é€‰æ‹©å™¨é€»è¾‘ (Session State ç®¡ç†) ---
    available_dates = sorted(df['æ—¥æœŸ'].dt.date.unique())
    
    if nav_option == "âª å†å²ç›˜é¢å›æ”¾":
        if 'selected_date_idx' not in st.session_state:
            st.session_state.selected_date_idx = len(available_dates) - 1

        #ç¡®ä¿ç´¢å¼•ä¸è¶Šç•Œ
        if st.session_state.selected_date_idx >= len(available_dates):
            st.session_state.selected_date_idx = len(available_dates) - 1
        if st.session_state.selected_date_idx < 0:
            st.session_state.selected_date_idx = 0

        # å¸ƒå±€ï¼šå‰ä¸€å¤© | æ»‘å— | åä¸€å¤©
        st.markdown("### ğŸ“… é€‰æ‹©å›æ”¾æ—¥æœŸ")
        
        # æ¨¡å¼é€‰æ‹©
        mode_col1, mode_col2 = st.columns([1, 3])
        with mode_col1:
            playback_mode = st.radio("å›æ”¾æ¨¡å¼", ["å•æ—¥å¤ç›˜", "å¤šæ—¥èµ°åŠ¿æ‹¼æ¥"], horizontal=True)

        if playback_mode == "å•æ—¥å¤ç›˜":
            col_prev, col_slider, col_next = st.columns([1, 6, 1])
            
            with col_prev:
                st.write("") 
                st.write("")
                if st.button("â¬…ï¸ å‰ä¸€å¤©"):
                    if st.session_state.selected_date_idx > 0:
                        st.session_state.selected_date_idx -= 1
                        st.rerun()

            with col_next:
                st.write("")
                st.write("")
                if st.button("åä¸€å¤© â¡ï¸"):
                    if st.session_state.selected_date_idx < len(available_dates) - 1:
                        st.session_state.selected_date_idx += 1
                        st.rerun()

            with col_slider:
                # åŸ select_slider æ›¿æ¢ä¸º date_input ä»¥æ”¯æŒå¿«é€Ÿå¹´ä»½é€‰æ‹©
                current_date_val = available_dates[st.session_state.selected_date_idx]
                
                picked_date = st.date_input(
                    "æ—¥æœŸ",
                    value=current_date_val,
                    min_value=available_dates[0],
                    max_value=available_dates[-1],
                    label_visibility="collapsed"
                )
                
                # å¦‚æœæ—¥æœŸå‘ç”Ÿå˜åŒ–
                if picked_date != current_date_val:
                    if picked_date in available_dates:
                        st.session_state.selected_date_idx = available_dates.index(picked_date)
                    else:
                        # å¦‚æœé€‰ä¸­çš„æ˜¯éäº¤æ˜“æ—¥ï¼Œå¯»æ‰¾æœ€è¿‘çš„äº¤æ˜“æ—¥
                        closest_date = min(available_dates, key=lambda d: abs(d - picked_date))
                        st.session_state.selected_date_idx = available_dates.index(closest_date)
                        st.toast(f"ğŸ“… ä¼‘å¸‚æ—¥ï¼Œå·²è‡ªåŠ¨å®šä½åˆ°æœ€è¿‘äº¤æ˜“æ—¥: {closest_date}")
                    st.rerun()
            
            target_dates = [available_dates[st.session_state.selected_date_idx]] # ä½¿ç”¨ state ä¸­çš„æ—¥æœŸ
            selected_date = target_dates[0]
            display_date_str = selected_date.strftime("%Y-%m-%d")
            
        else: # å¤šæ—¥èµ°åŠ¿æ‹¼æ¥
            with mode_col2:
                date_range = st.date_input(
                    "é€‰æ‹©æ—¶é—´èŒƒå›´ (å»ºè®®ä¸è¶…è¿‡5å¤©ï¼Œå¦åˆ™åŠ è½½è¾ƒæ…¢)",
                    value=[available_dates[-5] if len(available_dates)>5 else available_dates[0], available_dates[-1]],
                    min_value=available_dates[0],
                    max_value=available_dates[-1]
                )
            
            if len(date_range) == 2:
                start_d, end_d = date_range
                # ç­›é€‰å‡ºèŒƒå›´å†…çš„äº¤æ˜“æ—¥
                target_dates = [d for d in available_dates if start_d <= d <= end_d]
                if not target_dates: # å¦‚æœé€‰å®šçš„èŒƒå›´å†…æ²¡æœ‰äº¤æ˜“æ—¥ (ä¾‹å¦‚å…¨é€‰äº†å‡æœŸ)
                    st.warning("âš ï¸ é€‰å®šèŒƒå›´å†…æ— äº¤æ˜“æ•°æ®ï¼Œå·²è‡ªåŠ¨é‡ç½®ä¸ºæœ€è¿‘äº¤æ˜“æ—¥")
                    target_dates = [available_dates[-1]]
                
                st.info(f"å·²é€‰æ‹© {len(target_dates)} ä¸ªäº¤æ˜“æ—¥è¿›è¡Œæ‹¼æ¥å±•ç¤º")
                selected_date = target_dates[-1] # ç”¨äºä¸‹æ–¹æ˜¾ç¤ºç»Ÿè®¡é¢æ¿çš„åŸºå‡† (å…¼å®¹æ—§ä»£ç å˜é‡å)
                display_date_str = f"{target_dates[0].strftime('%Y%m%d')} ~ {target_dates[-1].strftime('%Y%m%d')}"
            else:
                st.warning("è¯·é€‰æ‹©å®Œæ•´çš„å¼€å§‹å’Œç»“æŸæ—¥æœŸ")
                target_dates = [available_dates[-1]]
                selected_date = available_dates[-1]
                display_date_str = selected_date.strftime("%Y-%m-%d")

        # --- æ•°æ®åˆ‡ç‰‡ä¸ç»Ÿè®¡ (å…¼å®¹å•æ—¥ä¸å¤šæ—¥) ---
        if len(target_dates) == 1:
            # å•æ—¥é€»è¾‘
            daily_df = df[df['æ—¥æœŸ'].dt.date == selected_date].copy()
            if daily_df.empty:
                st.warning(f"{selected_date} å½“æ—¥æ— äº¤æ˜“æ•°æ®ã€‚")
                st.stop()
                
            median_chg = daily_df['æ¶¨è·Œå¹…'].median()
            total_turnover = daily_df['æˆäº¤é¢'].sum() / 1e8 
            top_gainer = daily_df.loc[daily_df['æ¶¨è·Œå¹…'].idxmax()]
            
            metric_label_date = "å½“å‰å›æ”¾æ—¥æœŸ"
            metric_label_chg = "æˆåˆ†è‚¡ä¸­ä½æ•°æ¶¨è·Œ"
            metric_label_to = "æˆåˆ†è‚¡æ€»æˆäº¤"
            
        else:
            # å¤šæ—¥é€»è¾‘ (è®¡ç®—ç´¯è®¡)
            # 1. ç­›é€‰å‡ºèŒƒå›´å†…æ‰€æœ‰æ•°æ®
            start_date_ts = pd.Timestamp(target_dates[0])
            end_date_ts = pd.Timestamp(target_dates[-1])
            
            period_df = df[(df['æ—¥æœŸ'] >= start_date_ts) & (df['æ—¥æœŸ'] <= end_date_ts)].copy()
            
            if period_df.empty:
                st.stop()
                
            # 2. è®¡ç®—åŒºé—´ç´¯è®¡æ¶¨è·Œå¹…
            # æ–¹æ³•: å¯¹æ¯ä¸ªä»£ç ï¼Œæ‰¾åˆ°é¦–å°¾ä»·æ ¼
            # æ³¨æ„: å¦‚æœåªç”¨ period_dfï¼Œé¦–æ—¥çš„æ•°æ®é‡Œçš„ 'æ”¶ç›˜' æ˜¯é¦–æ—¥çš„æ”¶ç›˜ä»·ã€‚
            # åŒºé—´æ¶¨å¹… = (End_Close - Start_Close) / Start_Close ? 
            # æˆ–è€…æ›´ç²¾ç¡®ï¼šStart_Close åº”è¯¥æ˜¯ Start_Date çš„ å‰ä¸€æ—¥æ”¶ç›˜ä»· (å³ Start_Open / (1+Start_Chg))?
            # ç®€åŒ–èµ·è§ï¼Œæˆ‘ä»¬ç”¨ (End_Date Close - Start_Date Open) / Start_Date Open
            # è¿™æ ·èƒ½åŒ…å« Start_Date å½“å¤©çš„æ¶¨è·Œ
            
            agg_stats = []
            
            # ä½¿ç”¨ groupby åŠ é€Ÿ
            grouped = period_df.groupby('ä»£ç ')
            
            for code, group in grouped:
                group = group.sort_values('æ—¥æœŸ')
                if group.empty: continue
                
                first_row = group.iloc[0]
                last_row = group.iloc[-1]
                
                # æ¨ç®—é¦–æ—¥å¼€ç›˜ä»· = æ”¶ç›˜ / (1 + chg/100)
                # è¿™ç§åæ¨å¦‚æœæ˜¯æ¶¨åœæ¿å¤æƒå¯èƒ½å¾®å°è¯¯å·®ï¼Œä½†å¤Ÿç”¨ã€‚
                # ä¹Ÿå¯ä»¥ç›´æ¥ç”¨ akshare ä¸‹è½½çš„ Openï¼Œä½†è¿™é‡Œåªæœ‰ Close/Chg
                # å‡è®¾ Chg æ˜¯ç²¾ç¡®çš„
                try:
                    start_open = first_row['æ”¶ç›˜'] / (1 + first_row['æ¶¨è·Œå¹…']/100)
                    end_close = last_row['æ”¶ç›˜']
                    
                    period_chg = (end_close - start_open) / start_open * 100
                    period_turnover = group['æˆäº¤é¢'].sum()
                    
                    agg_stats.append({
                        'ä»£ç ': code,
                        'åç§°': first_row['åç§°'], # å‡è®¾æ²¡æ”¹å
                        'åŒºé—´æ¶¨è·Œå¹…': period_chg,
                        'åŒºé—´æ€»æˆäº¤': period_turnover
                    })
                except:
                    pass
            
            agg_df = pd.DataFrame(agg_stats)
            
            if agg_df.empty:
                st.warning("åŒºé—´æ•°æ®è®¡ç®—å¼‚å¸¸")
                st.stop()
                
            median_chg = agg_df['åŒºé—´æ¶¨è·Œå¹…'].median()
            total_turnover = period_df['æˆäº¤é¢'].sum() / 1e8
            top_gainer = agg_df.loc[agg_df['åŒºé—´æ¶¨è·Œå¹…'].idxmax()]
            
            # ä¸ºäº†å…¼å®¹åç»­ daily_df çš„ä½¿ç”¨ (Treemap å’Œ é€‰è‚¡)
            # æˆ‘ä»¬éœ€è¦æ„é€ ä¸€ä¸ª "Proxy Daily DF"
            # è®©åé¢çš„é€‰è‚¡é€»è¾‘åŸºäº "åŒºé—´è¡¨ç°"
            daily_df = agg_df.rename(columns={'åŒºé—´æ¶¨è·Œå¹…': 'æ¶¨è·Œå¹…', 'åŒºé—´æ€»æˆäº¤': 'æˆäº¤é¢'}).copy()
            # è¡¥é½å…¶ä»–å­—æ®µ
            # æ”¶ç›˜ä»·ç”¨æœ€åä¸€å¤©çš„
            # daily_df è¿˜éœ€è¦ 'æ”¶ç›˜' ç”¨äºå±•ç¤º? Treemap hover éœ€è¦
            # æˆ‘ä»¬å¯ä»¥ join å›å»ï¼Œä½† Treemap hover ä¹Ÿå¯ä»¥åªå±•ç¤ºæ¶¨è·Œ
            daily_df['æ”¶ç›˜'] = 0 # Placeholder
            
            metric_label_date = "å½“å‰å›æ”¾åŒºé—´"
            metric_label_chg = "åŒºé—´æ¶¨è·Œå¹…ä¸­ä½æ•°"
            metric_label_to = "åŒºé—´æ€»æˆäº¤"

        # æ˜¾ç¤ºæŒ‡æ ‡è¡Œ
        col1, col2, col3, col4 = st.columns(4)
        col1.metric(metric_label_date, display_date_str)
        col2.metric(metric_label_chg, f"{median_chg:.2f}%", 
                    delta=f"{median_chg:.2f}%", delta_color="normal")
        col3.metric(metric_label_to, f"{total_turnover:.1f} äº¿")
        col4.metric("é¢†æ¶¨é¾™å¤´", f"{top_gainer['åç§°']} ({'æ¶¨è·Œå¹…' in top_gainer and top_gainer['æ¶¨è·Œå¹…'] or top_gainer.get('åŒºé—´æ¶¨è·Œå¹…'):.2f}%)")

        # --- æ–°å¢åŠŸèƒ½ï¼šåˆ†æ—¶èµ°åŠ¿å åŠ  ---
        st.markdown("---")
        st.subheader("ğŸ“ˆ æ ¸å¿ƒèµ„äº§åˆ†æ—¶èµ°åŠ¿å åŠ ")
        
        # æ¨¡å¼é€‰æ‹©
        col_mode, col_num = st.columns([3, 1])
        with col_mode:
            chart_mode = st.radio("é€‰è‚¡æ¨¡å¼", ["æˆäº¤é¢ Top (æ´»è·ƒåº¦)", "æŒ‡æ•°è´¡çŒ® Top (å½±å“å¤§ç›˜)"], horizontal=True)
        with col_num:
            # æ·»åŠ  key é¿å… Bad setIn index é”™è¯¯ï¼Œå¹¶å¼ºåˆ¶é‡ç½®çŠ¶æ€
            top_n = st.number_input("æ ‡çš„æ•°é‡", min_value=5, max_value=50, value=20, step=5, 
                                   help="æ²ª/æ·±å„å– N ä¸ªæ ‡çš„ï¼ˆå³æ€»æ•°ä¸º 2Nï¼‰", key="top_n_stocks_input")

        st.caption(f"æ³¨ï¼šè¿™é‡Œçš„æ’åæ˜¯åŸºäº **{selected_date}** å½“æ—¥çš„æ•°æ®è®¡ç®—çš„ã€‚å¦‚æœæ˜¯å¤šæ—¥æ¨¡å¼ï¼Œåˆ™å±•ç¤ºè¿™äº›è‚¡ç¥¨åœ¨è¿‡å»å‡ å¤©çš„èµ°åŠ¿ã€‚")
        st.caption("æ³¨ï¼šæŒ‡æ•°è´¡çŒ® = æ¶¨è·Œå¹… Ã— æƒé‡(è¿‘ä¼¼ä¸ºæˆäº¤é¢/å¸‚å€¼å æ¯”)ã€‚æ­¤æ¨¡å¼èƒ½çœ‹åˆ°æ˜¯è°åœ¨æ‹‰åŠ¨æˆ–ç ¸ç›˜ã€‚")

        dates_sig = ("", "", 0)
        if target_dates:
            dates_sig = (
                target_dates[0].strftime("%Y-%m-%d"),
                target_dates[-1].strftime("%Y-%m-%d"),
                len(target_dates),
            )
        intraday_sig = (playback_mode, chart_mode, int(top_n), dates_sig)
        if st.session_state.get("intraday_sig") != intraday_sig:
            st.session_state["intraday_sig"] = intraday_sig
            log_action("\u5206\u65f6\u9009\u9879\u53d8\u66f4", playback=playback_mode, chart=chart_mode, top_n=top_n, dates=dates_sig)
            st.session_state["show_intraday"] = False

        show_intraday = st.checkbox("åŠ è½½åˆ†æ—¶èµ°åŠ¿ (æœ¬åœ°ä¼˜å…ˆï¼Œæ— åˆ™ç½‘ç»œæ‹‰å–)", key="show_intraday")
        prev_show = st.session_state.get("show_intraday_prev", False)
        if show_intraday and not prev_show:
            log_action("\u52fe\u9009\u5206\u65f6\u52a0\u8f7d")
        st.session_state["show_intraday_prev"] = show_intraday
        
        if show_intraday:
            # ä½¿ç”¨ placeholder æ”¾ç½®è¿›åº¦æ¡ï¼Œé¿å…ç»„ä»¶é”€æ¯å¯¼è‡´çš„ç´¢å¼•é”™ä¹±
            progress_area = st.empty()
            
            # ç»Ÿä¸€é€‰è‚¡é€»è¾‘ï¼šæ— è®ºæ˜¯æˆäº¤é¢è¿˜æ˜¯æŒ‡æ•°è´¡çŒ®ï¼Œéƒ½æŒ‰æ²ªæ·±åˆ†åˆ«å– Top N
            if "æˆäº¤é¢" in chart_mode:
                sort_col = 'æˆäº¤é¢'
            else:
                daily_df['abs_impact'] = (daily_df['æ¶¨è·Œå¹…'] * daily_df['æˆäº¤é¢']).abs()
                sort_col = 'abs_impact'
                
            sh_pool = daily_df[daily_df['ä»£ç '].astype(str).str.startswith('6')].copy()
            sz_pool = daily_df[~daily_df['ä»£ç '].astype(str).str.startswith('6')].copy()
            
            sh_top = sh_pool.sort_values(sort_col, ascending=False).head(top_n)
            sz_top = sz_pool.sort_values(sort_col, ascending=False).head(top_n)
            
            top_stocks_df = pd.concat([sh_top, sz_top], ignore_index=True)
            top_codes = top_stocks_df['ä»£ç '].astype(str).tolist()
            st.session_state["last_top_codes"] = top_codes
            name_map = _refresh_name_map_for_codes(top_codes, force=False)
            if name_map:
                top_stocks_df['åç§°'] = top_stocks_df['ä»£ç '].astype(str).map(name_map).fillna(top_stocks_df['åç§°'])

            target_stocks_list = []
            for _, row in top_stocks_df.iterrows():
                target_stocks_list.append((row['ä»£ç '], row['åç§°'], row['æˆäº¤é¢'])) 
            
            all_intraday_data = [] 
            
            period_to_use = '5'
            
            if len(target_dates) > 5 and playback_mode == "å¤šæ—¥èµ°åŠ¿æ‹¼æ¥":
                if len(target_dates) > 30:
                    period_to_use = '15' # è¶…è¿‡30å¤©ä½¿ç”¨15åˆ†é’Ÿçº¿
                    st.info(f"â„¹ï¸ æ‚¨é€‰æ‹©äº† {len(target_dates)} å¤©ï¼šç³»ç»Ÿè‡ªåŠ¨åˆ‡æ¢è‡³ã€15åˆ†é’Ÿçº§ã€‘æ•°æ®ã€‚")
                else:
                    period_to_use = '5'
                    st.info(f"â„¹ï¸ æ‚¨é€‰æ‹©äº† {len(target_dates)} å¤©ï¼šç³»ç»Ÿè‡ªåŠ¨åˆ‡æ¢è‡³ã€5åˆ†é’Ÿçº§ã€‘æ•°æ®ã€‚")
            elif len(target_dates) > 10:
                 st.toast(f"âš ï¸ æ‚¨é€‰æ‹©äº† {len(target_dates)} å¤©çš„æ•°æ®ï¼ŒåŠ è½½å¯èƒ½è¾ƒæ…¢ï¼Œè¯·è€å¿ƒç­‰å¾…...")
            
            target_dates_to_fetch = target_dates
            total_steps = len(target_dates_to_fetch)
            logger.info("åˆ†æ—¶åŠ è½½å¼€å§‹: æ¨¡å¼=%s æ—¥æœŸæ•°=%s æ ‡çš„æ•°=%s å‘¨æœŸ=%s", chart_mode, len(target_dates_to_fetch), len(target_stocks_list), period_to_use)

            # æ”¹å›æ‰å¹³åŒ–ç»“æ„ï¼Œä¸å†ä½¿ç”¨ containerï¼Œå‡å°‘ DOM æ“ä½œå±‚çº§
            # å¹¶å‘çº¿ç¨‹ä¸­ç¼“å­˜çš„ show_spinner=False å·²ç»è®¾ç½®ï¼Œè¿™é‡Œåº”è¯¥å®‰å…¨äº†
            status_text = st.empty()
            fetch_progress = st.progress(0)
            for i, d_date in enumerate(target_dates_to_fetch):
                status_text.text(f"ğŸ”„ \u6b63\u5728\u83b7\u53d6: {d_date.strftime('%Y-%m-%d')} | \u5468\u671f={period_to_use}\u5206\u949f | \u76ee\u6807={len(target_stocks_list)}+\u6307\u65703 ({i+1}/{total_steps})")
                fetch_progress.progress((i + 1) / total_steps)
                
                d_str = d_date.strftime("%Y-%m-%d")
                day_results, day_failures, day_stats = fetch_intraday_data_v2(target_stocks_list, d_str, period=period_to_use)

                success = day_stats.get('success', 0)
                failed = day_stats.get('failed', 0)
                total_req = day_stats.get('total', 0)
                cache_hits = day_stats.get('cache', 0)
                network_calls = day_stats.get('network', 0)

                logger.info("\u5206\u65f6\u65e5\u6c47\u603b: date=%s total=%s success=%s failed=%s cache=%s network=%s", d_str, total_req, success, failed, cache_hits, network_calls)
                if day_failures:
                    for err in day_failures[:3]:
                        logger.warning("\u5206\u65f6\u5931\u8d25: code=%s name=%s api=%s reason=%s", err.get('code'), err.get('name'), err.get('api'), err.get('reason'))

                for res in day_results:
                    res["data"]["date_col"] = d_str
                    res["real_date"] = d_date
                
                all_intraday_data.extend(day_results)
            
            logger.info("åˆ†æ—¶åŠ è½½å®Œæˆ: ç»“æœæ•°=%s", len(all_intraday_data))
            # æ•°æ®æ‹‰å–å®Œæ¯•åï¼Œæ¸…é™¤è¿›åº¦ç»„ä»¶
            status_text.empty()
            fetch_progress.empty()
            # ç§»é™¤å¤–å±‚å ä½ç¬¦çš„æ¸…ç†ï¼Œå› ä¸ºå·²ç»ä¸å†ä½¿ç”¨
            progress_area.empty()
                
            if not all_intraday_data:
                st.warning("æœªèƒ½è·å–åˆ°åˆ†æ—¶æ•°æ®")
            else:
                    # --- æ•°æ®é‡ç»„ ---
                    # å°†åˆ†æ•£çš„æ•°æ®åˆå¹¶ä¸ºï¼š { 'code': { 'name':..., 'is_index':..., 'full_data': DataFrame } }
                    combined_series = {}
                    
                    for item in all_intraday_data:
                        code = item['code']
                        if code not in combined_series:
                            # æŸ¥æ‰¾è¯¥è‚¡ç¥¨åœ¨é€‰å®šæ—¥(æœ€åä¸€æ—¥)çš„æˆäº¤é¢ï¼Œç”¨äºå®šçº¿å®½
                            to_val = 0
                            if not item.get('is_index'):
                                matches = daily_df[daily_df['ä»£ç '] == code]
                                if not matches.empty:
                                    to_val = matches.iloc[0]['æˆäº¤é¢']
                            
                            combined_series[code] = {
                                'name': item['name'],
                                'code': code,
                                'is_index': item.get('is_index', False),
                                'turnover': to_val, # ä½¿ç”¨æœ€åä¸€å¤©çš„æˆäº¤é¢
                                'dfs': []
                            }
                        combined_series[code]['dfs'].append(item['data'])
                    
                    # åˆå¹¶ DataFrame å¹¶æ„å»ºè¿ç»­æ—¶é—´è½´
                    # ç­–ç•¥ï¼šä¸ä½¿ç”¨çœŸå®æ—¶é—´è½´ï¼Œè€Œæ˜¯ä½¿ç”¨ "äº¤æ˜“åˆ†é’Ÿåºåˆ—" (Trading Minute Index)
                    # æ¯å¤© 240 åˆ†é’Ÿã€‚Day 1: 0-239, Day 2: 240-479...
                    # éœ€è¦ç”Ÿæˆä¸€ä¸ª Xè½´ Label Map
                    
                    final_plot_data = [] # List of {idx, pct_chg, ...}
                    x_tick_vals = []
                    x_tick_text = []
                    
                    # ä¸ºæ¯ä¸€å¤©ç”Ÿæˆæ ‡å‡†æ—¶é—´åºåˆ— (é¿å…ç¼ºå¤±åˆ†é’Ÿå¯¼è‡´çš„é”™ä½)
                    # 09:30 - 11:30 (121 points usually inclusive? Aè‚¡åˆ†é’Ÿçº¿é€šå¸¸åŒ…å« 11:30 å’Œ 15:00)
                    # åˆ†é’Ÿçº¿é€šå¸¸æ˜¯ 09:31 - 11:30 (120 mins) 13:01 - 15:00 (120 mins). 
                    # AkShare è¿”å›çš„æ•°æ®æ—¶é—´å¦‚æœæ˜¯ 09:30 é€šå¸¸ä»£è¡¨å¼€ç›˜?
                    # è®©æˆ‘ä»¬é€šè¿‡è§‚å¯Ÿç¬¬ä¸€æ¡æ•°æ®æ¥å†³å®šé€»è¾‘ï¼Œé€šå¸¸ç›´æ¥ concat å³å¯
                    # ä¸ºäº†è§£å†³ GAPï¼Œæˆ‘ä»¬åœ¨ UI ç»˜å›¾å±‚å¼ºåˆ¶æŠŠ x æ˜ å°„ä¸º 0, 1, 2...
                    
                    # æå–çœŸæ­£è·å–åˆ°æ•°æ®çš„æ—¥æœŸåˆ—è¡¨ (å»é™¤èŠ‚å‡æ—¥/æ— æ•°æ®æ—¥)
                    valid_dates = set()
                    for item in all_intraday_data:
                         if 'real_date' in item:
                             valid_dates.add(item['real_date'].strftime("%Y-%m-%d"))
                    
                    days_list = sorted(list(valid_dates))
                    
                    if not days_list:
                        # å¦‚æœè¿‡æ»¤åå±…ç„¶æ²¡äº†ï¼ˆç†è®ºä¸Šå¤–å±‚ checked not emptyï¼‰ï¼Œå›é€€å…œåº•
                         days_list = sorted(list(set([x.strftime("%Y-%m-%d") for x in target_dates_to_fetch])))
                    
                    # æ„å»ºåŸºå‡†æ—¶é—´ç½‘æ ¼ (Template) - æ¯å¤© 240/241 ä¸ªç‚¹
                    # 09:30 - 11:30, 13:00 - 15:00
                    dummy_date = "2000-01-01"
                    morning_range = pd.date_range(f"{dummy_date} 09:30", f"{dummy_date} 11:30", freq="1min")
                    afternoon_range = pd.date_range(f"{dummy_date} 13:00", f"{dummy_date} 15:00", freq="1min")
                    daily_time_template = morning_range.union(afternoon_range) # Size approx 242
                    
                    # è®¡ç®—æ€»åç§»é‡
                    points_per_day = len(daily_time_template)
                    
                    for code, info in combined_series.items():
                        # åˆå¹¶ã€æ’åº
                        full_df = pd.concat(info['dfs']).sort_values(['date_col', 'time'])
                        
                        # é‡æ–°æ„é€  X è½´ (Int)
                        # ç®—æ³•ï¼šå¯¹äºæ¯ä¸€è¡Œï¼Œæ‰¾åˆ°å®ƒæ˜¯ ç¬¬å‡ å¤© çš„ ç¬¬å‡ åˆ†é’Ÿ
                        # x_int = day_index * points_per_day + minute_index_in_day
                        
                        full_df['time_str'] = full_df['time'].dt.strftime("%H:%M:%S")
                        
                        x_values = []
                        
                        for idx, row in full_df.iterrows():
                            d_str = row['date_col']
                            t_str = row['time_str'] # å®Œæ•´æ—¶é—´å¯¹è±¡
                            
                            # ç¡®å®šæ˜¯ç¬¬å‡ å¤©
                            day_idx = days_list.index(d_str) if d_str in days_list else 0
                            
                            # ç¡®å®šæ˜¯å½“å¤©çš„ç¬¬å‡ åˆ†é’Ÿ
                            # ç®€å•è½¬æ¢ï¼šå°æ—¶*60 + åˆ†é’Ÿ
                            t_obj = row['time'] # timestamp
                            mins_from_midnight = t_obj.hour * 60 + t_obj.minute
                            
                            # æŠŠä¸­åˆä¼‘å¸‚çš„æ—¶é—´å‹æ‰
                            # 9:30 (570) -> 11:30 (690). Length 120.
                            # 13:00 (780) -> 15:00 (900). 
                            
                            if mins_from_midnight <= 690: # Morning
                                offset = mins_from_midnight - 570 # 09:30 is 0
                            else: # Afternoon
                                offset = 120 + (mins_from_midnight - 780) # 13:00 starts at 120+
                                
                            final_x = day_idx * (240 + 20) + offset # åŠ 20ä¸ªå•ä½çš„é—´éš”è®©å¤©ä¸å¤©ä¹‹é—´æœ‰ç‚¹ç©ºéš™
                            x_values.append(final_x)
                            
                        full_df['x_int'] = x_values
                        
                        # è®¡ç®—ç´¯è®¡æ¶¨è·Œå¹… (å¦‚æœæ˜¯å¤šæ—¥ï¼Œéœ€è¦é“¾å¼è®¡ç®—ï¼Ÿ)
                        # ç®€å•æ–¹æ¡ˆï¼šæ¯å¤©é‡æ–°å½’é›¶ï¼Ÿè¿˜æ˜¯å¤šæ—¥è¿è´¯ï¼Ÿ
                        # ç”¨æˆ·è¯´ "æ‹¼èµ·æ¥å½¢æˆå®Œæ•´çš„å›¾"ï¼Œé€šå¸¸æ„å‘³ç€è¿è´¯è¶‹åŠ¿ã€‚
                        # ä»¥ç¬¬ä¸€å¤©å¼€ç›˜ä»·ä¸ºåŸºå‡†
                        base_price = full_df['close'].iloc[0]
                        # å¦‚æœä¸­é—´æœ‰æ–­ç‚¹ï¼Œç®€å•çš„ (close - base) / base å¯èƒ½å¤±çœŸï¼ˆå› ä¸ºæ˜¨æ”¶...ï¼‰
                        # å‡†ç¡®åšæ³•ï¼šç´¯ä¹˜æ¯å¤©çš„æ¶¨è·Œå¹…ã€‚
                        # ä½†è¿™é‡Œåªè¦å¤§æ¦‚è¶‹åŠ¿ã€‚å¦‚æœç”¨ (Px - P0) / P0ï¼Œé‚£è·¨æ—¥ç¼ºå£ä¼šä½“ç°ä¸ºç›´çº¿çš„è·³è·ƒã€‚
                        # è¿™ç¬¦åˆ "çœŸå®ä»·æ ¼èµ°åŠ¿"ã€‚
                        
                        full_df['cumulative_pct'] = (full_df['close'] - base_price) / base_price * 100
                        
                        info['plot_data'] = full_df
                    
                    # ç”Ÿæˆ X è½´æ ‡ç­¾
                    for i, d_str in enumerate(days_list):
                        base_x = i * (240 + 20)
                        day_label = d_str[5:] # MM-DD
                        
                        if len(days_list) > 1:
                            # å¤šæ—¥æ¨¡å¼ï¼šåªæ˜¾ç¤ºæ—¥æœŸåœ¨ä¸­é—´ æˆ–è€… å¼€å¤´
                            # ä¸ºäº†ç®€æ´ï¼Œåªåœ¨æ¯å¤©çš„ä¸­é—´æ˜¾ç¤ºä¸€ä¸ªæ—¥æœŸ
                            x_tick_vals.append(base_x + 120) 
                            x_tick_text.append(day_label)
                        else:
                            # å•æ—¥æ¨¡å¼ï¼šæ˜¾ç¤ºè¯¦ç»†æ—¶é—´ç‚¹
                            # 09:30
                            x_tick_vals.append(base_x)
                            x_tick_text.append(f"{day_label}\n09:30")
                            # 11:30
                            x_tick_vals.append(base_x + 120)
                            x_tick_text.append("11:30/13:00")
                            # 15:00
                            x_tick_vals.append(base_x + 240)
                            x_tick_text.append("15:00")

                    # åˆ†ç±»
                    idx_data = [v for k,v in combined_series.items() if v['is_index']]
                    stk_data = [v for k,v in combined_series.items() if not v['is_index']]
                    
                    sh_stocks = [v for v in stk_data if v['code'].startswith('6')]
                    sz_stocks = [v for v in stk_data if not v['code'].startswith('6')]
                    
                    sh_index = [v for v in idx_data if v['code'] in ['000001', '000300']]
                    sz_index = [v for v in idx_data if v['code'] in ['399001', '000300']]

                    # ç»˜å›¾å‡½æ•° v3
                    def plot_intraday_v3(stocks, indices, title_suffix):
                        fig = go.Figure()
                        
                        # ä¸ªè‚¡
                        if stocks:
                            max_to = max([s['turnover'] for s in stocks]) if stocks else 1
                            min_to = min([s['turnover'] for s in stocks]) if stocks else 0
                            
                            # ç”Ÿæˆ distinct colors
                            color_palette = px.colors.qualitative.Alphabet + px.colors.qualitative.Dark24
                            
                            for i, s in enumerate(stocks):
                                if max_to == min_to: width=2
                                else: width = 1 + 3*(s['turnover'] - min_to)/(max_to - min_to)
                                
                                df_p = s['plot_data']
                                last_val = df_p['cumulative_pct'].iloc[-1]
                                
                                # ä¹‹å‰çš„çº¢ç»¿é€»è¾‘: color = 'rgba(214, 39, 40, 0.4)' if last_val > 0 else 'rgba(44, 160, 44, 0.4)'
                                # ç°åœ¨æ”¹ä¸ºåŒºåˆ†é¢œè‰²
                                color = color_palette[i % len(color_palette)]
                                
                                fig.add_trace(go.Scatter(
                                    x=df_p['x_int'],
                                    y=df_p['cumulative_pct'],
                                    mode='lines',
                                    name=s['name'],
                                    # line=dict(width=width, color=color),
                                    # ä¸ªè‚¡çº¿å®½ä¸éœ€è¦å¤ªç²—ï¼Œé¢œè‰²è¦æ¸…æ™°
                                    line=dict(width=max(1.5, width), color=color),
                                    hovertemplate=f"<b>{s['name']}</b><br>æ¶¨è·Œ: %{{y:.2f}}%<br>æ—¶é—´: %{{customdata}}",
                                    customdata=df_p['date_col'] + ' ' + df_p['time_str']
                                ))
                                
                        # æŒ‡æ•°
                        idx_colors = {'000300': 'black', '000001': '#d62728', '399001': '#1f77b4'}
                        for idx in indices:
                            df_p = idx['plot_data']
                            c_code = idx.get('code', '000300')
                            
                            fig.add_trace(go.Scatter(
                                x=df_p['x_int'],
                                y=df_p['cumulative_pct'],
                                mode='lines',
                                name=idx['name'],
                                line=dict(width=3, color=idx_colors.get(c_code, 'black')),
                                hovertemplate=f"<b>{idx['name']}</b><br>æ¶¨è·Œ: %{{y:.2f}}%"
                            ))

                        # 3. åˆ†å‰²çº¿ (å¦‚æœæ˜¯å¤šæ—¥)
                        if len(days_list) > 1:
                            for i in range(1, len(days_list)):
                                # åœ¨æ¯ä¸€å¤©å¼€å§‹å‰ç”»ç«–çº¿
                                boundary = i * (240 + 20) - 10
                                fig.add_vline(x=boundary, line_width=1, line_dash="dash", line_color="gray")

                        fig.update_layout(
                            title=f"åˆ†æ—¶èµ°åŠ¿å åŠ  ({'å¤šæ—¥æ‹¼æ¥' if len(days_list)>1 else days_list[0]}) - {title_suffix}",
                            xaxis=dict(
                                tickmode='array',
                                tickvals=x_tick_vals,
                                ticktext=x_tick_text,
                                showgrid=True,
                                showspikes=True, # æ˜¾ç¤ºå‚ç›´è¾…åŠ©çº¿
                                spikemode='across',
                                spikesnap='cursor',
                                showline=True, 
                                linewidth=1, 
                                linecolor='black',
                                mirror=True
                            ),
                            yaxis=dict(
                                showspikes=True # Yè½´ä¹Ÿæ˜¾ç¤ºè¾…åŠ©çº¿ï¼Œæ–¹ä¾¿çœ‹ç‚¹ä½
                            ),
                            yaxis_title="ç´¯è®¡æ¶¨è·Œå¹… (%)",
                            hovermode="x unified", # å¼€å¯ç»Ÿä¸€ Hoverï¼Œæ˜¾ç¤ºè¯¥æ—¶é—´ç‚¹æ‰€æœ‰æ•°æ®
                            height=700, # ç¨å¾®è°ƒé«˜é«˜åº¦ä»¥å®¹çº³æ›´å¤š Hover ä¿¡æ¯
                            legend=dict(orientation="h", y=1.02, x=1, xanchor='right') # ä¿æŒå›¾ä¾‹çš„å¸ƒå±€
                        )
                        
                        # âš ï¸ å…³é”®ä¿®æ­£ï¼šç¡®ä¿ Hover Tooltip çš„æ’åºæŒ‰ç…§ Y è½´æ•°å€¼ (ä»é«˜åˆ°ä½)
                        # "closest" æ¨¡å¼é…åˆ "compare" å¯èƒ½æ— æ³•ç”Ÿæ•ˆï¼Œä½†åœ¨ "x unified" æ¨¡å¼ä¸‹ï¼Œ
                        # é»˜è®¤æ˜¯æŒ‰ç…§ Trace æ·»åŠ é¡ºåºæ’åºçš„ã€‚
                        # Plotly (JSå±‚) åœ¨ x unified ä¸‹æœ‰é»˜è®¤æ’åºé€»è¾‘ (é€šå¸¸æ˜¯ value descending)ï¼Œä½†åœ¨æŸäº›ç‰ˆæœ¬å¯èƒ½ä¸ç¨³å®šã€‚
                        # ä¸ºäº†å¢å¼ºæ’åºä½“éªŒï¼Œæˆ‘ä»¬å¯ä»¥å°è¯•è®¾ç½® layout.hoverlabel.namelength = -1
                        
                        fig.update_layout(hoverlabel=dict(namelength=-1))
                        
                        return fig

                    tab1, tab2 = st.tabs(["æ²ªå¸‚ (SH)", "æ·±å¸‚ (SZ)"])
                    
                    with tab1:
                        st.plotly_chart(plot_intraday_v3(sh_stocks, sh_index, f"æ²ªå¸‚ - {chart_mode}"), width="stretch")
                    with tab2:
                        st.plotly_chart(plot_intraday_v3(sz_stocks, sz_index, f"æ·±å¸‚ - {chart_mode}"), width="stretch")
        
        # --- å¯è§†åŒ– ---
        st.subheader(f"ğŸ“Š {selected_date.strftime('%Yå¹´%mæœˆ%dæ—¥')} å¸‚åœºå…¨æ™¯çƒ­åŠ›å›¾")
        
        # Aè‚¡ä¸“ç”¨è‰²è°±
        max_limit = 7
        min_limit = -7
        
        fig = px.treemap(
            daily_df,
            path=['åç§°'],
            values='æˆäº¤é¢', # ç”¨æˆäº¤é¢ä»£è¡¨çƒ­åº¦/æƒé‡ (å› ä¸ºå†å²å¸‚å€¼éš¾è·å–)
            color='æ¶¨è·Œå¹…',
            color_continuous_scale=['#00a65a', '#ffffff', '#dd4b39'], # ç»¿ -> ç™½ -> çº¢
            range_color=[min_limit, max_limit],
            hover_data={
                'åç§°': True,
                'ä»£ç ': True,
                'æ”¶ç›˜': True,
                'æ¶¨è·Œå¹…': ':.2f',
                'æˆäº¤é¢': True
            },
            height=650
        )
        
        # ä¼˜åŒ–æ˜¾ç¤º
        fig.update_traces(
            textinfo="label+value+percent entry",
            hovertemplate="<b>%{label}</b><br>æ”¶ç›˜ä»·: %{customdata[2]}<br>æ¶¨è·Œå¹…: %{color:.2f}%<br>æˆäº¤é¢: %{value:.2s}"
        )
        fig.update_layout(
            margin=dict(t=10, l=10, r=10, b=10),
            coloraxis_colorbar=dict(title="æ¶¨è·Œå¹…(%)")
        )
        
        st.plotly_chart(fig, width="stretch")
        
        # å¯é€‰ï¼šæ˜¾ç¤ºè¯¦ç»†æ•°æ®è¡¨
        with st.expander("æŸ¥çœ‹å½“æ—¥è¯¦ç»†æ•°æ®"):
            st.dataframe(
                daily_df[['ä»£ç ', 'åç§°', 'æ”¶ç›˜', 'æ¶¨è·Œå¹…', 'æˆäº¤é¢']].style.format({
                    'æ”¶ç›˜': '{:.2f}',
                    'æ¶¨è·Œå¹…': '{:.2f}%',
                    'æˆäº¤é¢': '{:,.0f}'
                }),
                hide_index=True
            )

    elif nav_option == "ğŸ—‚ï¸ æ•°æ®ç®¡ç†":
        st.subheader("ğŸ“¦ æœ¬åœ°åˆ†æ—¶æ•°æ®ç®¡ç†")
        st.caption("è¯´æ˜ï¼šåªæ˜¾ç¤º 1åˆ†é’Ÿåˆ†æ—¶ç¼“å­˜ï¼Œå¯æ‰‹åŠ¨è¡¥é½ç¼ºå¤±ã€‚")
        if origin_df is None or origin_df.empty:
            st.warning("æš‚æ— å†å²æ•°æ®ï¼Œè¯·å…ˆåˆå§‹åŒ–ã€‚")
        else:
            trading_dates = sorted(origin_df['æ—¥æœŸ'].dt.date.unique())
            date_strs = [d.strftime("%Y-%m-%d") for d in trading_dates]
            selected_date_str = st.selectbox("é€‰æ‹©äº¤æ˜“æ—¥æœŸ", date_strs, index=len(date_strs) - 1)
            selected_date = datetime.strptime(selected_date_str, "%Y-%m-%d").date()
            date_key = selected_date_str.replace("-", "")
            codes, name_map = _get_daily_codes(origin_df, selected_date)
            st.write(f"å½“æ—¥æˆåˆ†è‚¡æ•°é‡: {len(codes)}")

            cached_codes = _get_cached_codes_for_date(date_key, codes, period=DEFAULT_MIN_PERIOD, is_index=False)
            missing_codes = [c for c in codes if c not in cached_codes]
            st.write(f"è‚¡ç¥¨åˆ†æ—¶ç¼“å­˜: {len(cached_codes)}/{len(codes)}")
            if missing_codes:
                missing_lines = [f"{c} {name_map.get(c, c)}" for c in missing_codes]
                st.text_area("ç¼ºå¤±è‚¡ç¥¨", "\n".join(missing_lines), height=160)
            else:
                st.success("è‚¡ç¥¨åˆ†æ—¶å·²å®Œæ•´ã€‚")

            index_codes = ["000300", "000001", "399001"]
            cached_idx = _get_cached_codes_for_date(date_key, index_codes, period=DEFAULT_MIN_PERIOD, is_index=True)
            missing_idx = [c for c in index_codes if c not in cached_idx]
            st.write(f"æŒ‡æ•°åˆ†æ—¶ç¼“å­˜: {len(cached_idx)}/{len(index_codes)}")
            if missing_idx:
                st.warning("ç¼ºå¤±æŒ‡æ•°: " + ", ".join(missing_idx))
            else:
                st.success("æŒ‡æ•°åˆ†æ—¶å·²å®Œæ•´ã€‚")

            st.markdown("### æ‰‹åŠ¨è¡¥é½")
            include_indices = st.checkbox("åŒ…å«æŒ‡æ•°", value=True)
            default_codes = missing_codes[:20]
            select_codes = st.multiselect("é€‰æ‹©è¦è¡¥é½çš„è‚¡ç¥¨(ç¼ºå¤±)", options=missing_codes, default=default_codes)
            if st.button("å¯åŠ¨åå°è¡¥é½"):
                started = _start_manual_prefetch(selected_date_str, select_codes, name_map, include_indices=include_indices)
                if started:
                    st.info("å·²å¯åŠ¨åå°è¡¥é½ï¼Œè¯·æŸ¥çœ‹ logs/app.logã€‚")
                else:
                    st.warning("æ²¡æœ‰å¯è¡¥é½çš„ä»»åŠ¡ã€‚")

            with st.expander("æœ¬åœ°åˆ†æ—¶ç¼“å­˜æ—¥æœŸ"):
                cached_dates = _scan_cached_dates(period=DEFAULT_MIN_PERIOD, is_index=False)
                if cached_dates:
                    readable = [d[:4] + '-' + d[4:6] + '-' + d[6:] for d in cached_dates]
                    st.text_area("ç¼“å­˜æ—¥æœŸ", "\n".join(readable), height=120)
                else:
                    st.write("æš‚æ— æœ¬åœ°åˆ†æ—¶ç¼“å­˜ã€‚")

    elif nav_option == "ğŸŒŠ èµ„é‡‘åç¦»åˆ†æ":
        st.subheader("ğŸŒŠ èµ„é‡‘åç¦»åº¦åˆ†æ (Alpha Divergence)")
        st.info("ğŸ’¡ **é€»è¾‘è¯´æ˜**ï¼šè®¡ç®—é€‰å®šå‘¨æœŸå†…æ¯åªè‚¡ç¥¨ç›¸å¯¹äºã€å¸‚åœºä¸­ä½æ•°ã€‘çš„è¶…é¢æ¶¨è·Œå¹…ï¼ˆåç¦»åº¦ï¼‰ã€‚\n\nå¦‚æœæŸåªè‚¡ç¥¨ **æˆäº¤é¢å·¨å¤§** ä¸” **å‘ä¸‹åç¦»æå¤§**ï¼Œé€šå¸¸æ„å‘³ç€ä¸»åŠ›èµ„é‡‘åœ¨å¤§ä¸¾å‡ºè´§ï¼›åä¹‹åˆ™æ˜¯ä¸»åŠ›æŠ¢ç­¹ã€‚")

        # 1. å‘¨æœŸé€‰æ‹© (Reuse simplified logic)
        available_dates = sorted(df['æ—¥æœŸ'].dt.date.unique())
        col_d1, col_d2 = st.columns(2)
        with col_d1:
            date_range_div = st.date_input(
                "åˆ†æå‘¨æœŸ",
                value=[available_dates[-5] if len(available_dates)>5 else available_dates[0], available_dates[-1]],
                min_value=available_dates[0],
                max_value=available_dates[-1],
                key="divergence_date_input"
            )
        
        target_dates_div = []
        if len(date_range_div) == 2:
            s_d, e_d = date_range_div
            target_dates_div = [d for d in available_dates if s_d <= d <= e_d]
        
        if not target_dates_div:
            st.warning("è¯·é€‰æ‹©æœ‰æ•ˆçš„æ—¶é—´èŒƒå›´")
            st.stop()
            
        st.caption(f"å·²é€‰å– {target_dates_div[0]} è‡³ {target_dates_div[-1]}ï¼Œå…± {len(target_dates_div)} ä¸ªäº¤æ˜“æ—¥ã€‚")
        
        # 2. è®¡ç®—åŒºé—´æ•°æ®
        start_date_ts = pd.Timestamp(target_dates_div[0])
        end_date_ts = pd.Timestamp(target_dates_div[-1])
        
        div_period_df = df[(df['æ—¥æœŸ'] >= start_date_ts) & (df['æ—¥æœŸ'] <= end_date_ts)].copy()
        
        # èšåˆ
        div_stats = []
        grouped = div_period_df.groupby('ä»£ç ')
        
        for code, group in grouped:
            group = group.sort_values('æ—¥æœŸ')
            if group.empty: continue
            
            first_row = group.iloc[0]
            last_row = group.iloc[-1]
            
            try:
                # ä¼°ç®—åŒºé—´æ¶¨å¹…
                s_open = first_row['æ”¶ç›˜'] / (1 + first_row['æ¶¨è·Œå¹…']/100)
                e_close = last_row['æ”¶ç›˜']
                cum_pct = (e_close - s_open) / s_open * 100
                total_to = group['æˆäº¤é¢'].sum()
                
                div_stats.append({
                    'ä»£ç ': code,
                    'åç§°': first_row['åç§°'],
                    'åŒºé—´æ¶¨è·Œå¹…': cum_pct,
                    'åŒºé—´æ€»æˆäº¤': total_to
                })
            except:
                pass
                
        div_df = pd.DataFrame(div_stats)
        if div_df.empty:
            st.stop()
            
        # 3. è®¡ç®—åç¦»åº¦ (Deviation)
        market_median_chg = div_df['åŒºé—´æ¶¨è·Œå¹…'].median()
        div_df['åç¦»åº¦'] = div_df['åŒºé—´æ¶¨è·Œå¹…'] - market_median_chg
        
        # è¾…åŠ©åˆ—
        div_df['æˆäº¤é¢(äº¿)'] = div_df['åŒºé—´æ€»æˆäº¤'] / 1e8
        
        col_m1, col_m2 = st.columns(2)
        col_m1.metric("åŸºå‡†(ä¸­ä½æ•°)æ¶¨è·Œå¹…", f"{market_median_chg:.2f}%")
        col_m2.metric("åˆ†ææ ·æœ¬æ•°", f"{len(div_df)} åª")
        
        st.divider()

        # 4. å¯è§†åŒ– - æ•£ç‚¹å›¾
        # X: æˆäº¤é¢(Log), Y: åç¦»åº¦, Color: åç¦»åº¦
        fig_scatter = px.scatter(
            div_df,
            x='æˆäº¤é¢(äº¿)',
            y='åç¦»åº¦',
            color='åç¦»åº¦',
            text='åç§°', # æ˜¾ç¤ºåå­—
            color_continuous_scale=['#00a65a', '#ffffff', '#dd4b39'],
            range_color=[-20, 20], # é™åˆ¶é¢œè‰²èŒƒå›´é¿å…æå€¼
            log_x=True,
            hover_data=['ä»£ç ', 'åŒºé—´æ¶¨è·Œå¹…'],
            title=f"èµ„é‡‘åç¦»åº¦åˆ†å¸ƒå›¾ (Xè½´ä¸ºæˆäº¤é¢å¯¹æ•°)"
        )
        fig_scatter.update_traces(textposition='top center')
        fig_scatter.update_layout(height=600)
        st.plotly_chart(fig_scatter, width="stretch")
        
        # 5. æ¦œå•
        col_list1, col_list2 = st.columns(2)
        
        with col_list1:
            st.subheader("ğŸ”¥ èµ„é‡‘æŠ±å›¢ (æ”¾é‡å‘ä¸Šåç¦»)")
            # é€»è¾‘ï¼šæˆäº¤é¢å¤§ & åç¦»åº¦ > 0
            # æ’åºï¼šç»¼åˆåˆ† = æˆäº¤é¢ * åç¦»åº¦ (ä»…å‚è€ƒ) æˆ–è€…æŒ‰æˆäº¤é¢é™åºçœ‹è°åœ¨æ¶¨
            # ç”¨æˆ·éœ€æ±‚ï¼šæ‰¾å‡ºå‘ä¸Šåç¦»çš„ã€‚é€šå¸¸æƒ³çœ‹â€œå¤§èµ„é‡‘ä¹°è°â€ã€‚æ‰€ä»¥æŒ‰æˆäº¤é¢é™åºï¼Œä¸”åç¦»åº¦>0
            
            buy_df = div_df[div_df['åç¦»åº¦'] > 0].sort_values('åŒºé—´æ€»æˆäº¤', ascending=False).head(20)
            st.dataframe(
                buy_df[['ä»£ç ', 'åç§°', 'åç¦»åº¦', 'æˆäº¤é¢(äº¿)', 'åŒºé—´æ¶¨è·Œå¹…']].style.format({
                    'åç¦»åº¦': '+{:.2f}%',
                    'æˆäº¤é¢(äº¿)': '{:.1f}',
                    'åŒºé—´æ¶¨è·Œå¹…': '{:.2f}%'
                }),
                hide_index=True
            )
            
        with col_list2:
            st.subheader("ğŸ“‰ èµ„é‡‘å‡ºé€ƒ (æ”¾é‡å‘ä¸‹åç¦»)")
            # é€»è¾‘ï¼šæˆäº¤é¢å¤§ & åç¦»åº¦ < 0
            sell_df = div_df[div_df['åç¦»åº¦'] < 0].sort_values('åŒºé—´æ€»æˆäº¤', ascending=False).head(20)
            
            st.dataframe(
                sell_df[['ä»£ç ', 'åç§°', 'åç¦»åº¦', 'æˆäº¤é¢(äº¿)', 'åŒºé—´æ¶¨è·Œå¹…']].style.format({
                    'åç¦»åº¦': '{:.2f}%',
                    'æˆäº¤é¢(äº¿)': '{:.1f}',
                    'åŒºé—´æ¶¨è·Œå¹…': '{:.2f}%'
                }),
                hide_index=True
            )

else:
    st.error("æ•°æ®åŠ è½½å¤±è´¥ï¼Œè¯·åˆ·æ–°é‡è¯•ã€‚")
