import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
import akshare as ak
from datetime import datetime, timedelta
import time
import random
import os
import concurrent.futures
import threading
import sys
import io
import zipfile
import shutil
import json
import logging
import html

# å°è¯•å¯¼å…¥ Streamlit ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œç”¨äºè§£å†³å¤šçº¿ç¨‹ "missing ScriptRunContext" è­¦å‘Š
try:
    from streamlit.runtime.scriptrunner import add_script_run_ctx, get_script_run_ctx
except ImportError:
    # å…¼å®¹æ—§ç‰ˆæœ¬ Streamlit
    from streamlit.scriptrunner import add_script_run_ctx, get_script_run_ctx

# é…ç½®é¡µé¢ä¿¡æ¯
st.set_page_config(
    page_title="Aè‚¡å†å²ç›˜é¢å›æ”¾ç³»ç»Ÿ",
    page_icon="âª",
    layout="wide"
)

# -----------------------------------------------------------------------------
from core.data_access import *
from core.data_access import (
    _get_cached_codes_for_date,
    _get_daily_codes,
    _refresh_name_map_for_codes,
    _scan_cached_dates,
    _start_auto_prefetch_if_needed,
    _start_manual_prefetch,
)
from ui.history_view import render_history_view
from ui.data_manager_view import render_data_manager
from ui.divergence_view import render_divergence_view
from ui.stock_analysis_view import render_stock_analysis_view
from ui.market_overview import render_market_overview  # æ–°å¢

NAV_OVERVIEW = "ğŸ” å¸‚åœºæ¦‚è§ˆ (Market Overview)"
NAV_HISTORY = "âª å†å²ç›˜é¢å›æ”¾"
NAV_STOCK = "ğŸ“ˆ ä¸ªè‚¡å¤šæ—¥åˆ†æ"
NAV_DIVERGENCE = "ğŸŒŠ èµ„é‡‘åç¦»åˆ†æ"
NAV_MANAGER = "ğŸ“‚ æ•°æ®ç®¡ç†"

# 2. UI å¸ƒå±€
# -----------------------------------------------------------------------------

st.title("Aè‚¡å†å²ç›˜é¢å›æ”¾ç³»ç»Ÿ (å¤šæŒ‡æ•°åˆ†å±‚ç‰ˆ)")

st.markdown("""
> ğŸ•¹ï¸ **æ“ä½œæŒ‡å—**ï¼š
> 1. ç­‰å¾…æ•°æ®åˆå§‹åŒ–å®Œæˆï¼ˆåˆæ¬¡è¿è¡Œå¯èƒ½éœ€è¦ 2-3 åˆ†é’Ÿï¼‰ã€‚
> 2. æ‹–åŠ¨ä¸‹æ–¹æ»‘å—é€‰æ‹©å†å²æ—¥æœŸã€‚
> 3. è§‚å¯Ÿå½“æ—¥ç›˜é¢çš„èµ„é‡‘æµå‘ä¸çƒ­åº¦ã€‚
""")

# ä¾§è¾¹æ 
with st.sidebar:
    st.header("âš™ï¸ æ•°æ®ç®¡ç†")

    # æŒ‡æ•°æ± é€‰æ‹© (ä¼˜å…ˆæ˜¾ç¤ºï¼Œä»¥ä¾¿åŠ è½½æ•°æ®)
    target_pool = st.selectbox(
        "ğŸ¯ åˆ†æå¯¹è±¡æ± ", 
        ["000300 | æ²ªæ·±300 (å¤§ç›˜)", "000905 | ä¸­è¯500 (ä¸­ç›˜, å‰”é™¤300)", "000852 | ä¸­è¯1000 (å°ç›˜, å‰”é™¤300+500)"],
        index=0,
        help="åˆ‡æ¢ä¸åŒçš„è‚¡ç¥¨æ± è¿›è¡Œåˆ†å±‚åˆ†æã€‚æ³¨æ„ï¼šåˆ‡æ¢åéœ€è¦é‡æ–°æ‹‰å–æ•°æ®ï¼Œå¯èƒ½éœ€è¦ä¸€ç‚¹æ—¶é—´ã€‚"
    )
    # Extract code
    pool_code = target_pool.split(" | ")[0]
    st.session_state["index_pool_code"] = pool_code
    
    with st.expander("æ•°æ®åˆ·æ–°ä¸ç»´æŠ¤", expanded=True):
        st.write("å¦‚æœæ•°æ®æ˜¾ç¤ºä¸æ­£ç¡®ï¼Œè¯·å°è¯•ä»¥ä¸‹æ“ä½œï¼š")
        
        # 1. ????
        if st.button("ğŸŸ¢ åˆ·æ–°ä»Šæ—¥è¡Œæƒ… (ç›˜ä¸­)"):
            log_action("åˆ·æ–°ä»Šæ—¥è¡Œæƒ…(ç›˜ä¸­)")
            try:
                if os.path.exists(CACHE_FILE):
                    _df = pd.read_parquet(CACHE_FILE)
                    _today = datetime.now().date()
                    _df_new = _df[_df["æ—¥æœŸ"].dt.date < _today]
                    _df_new.to_parquet(CACHE_FILE)
                    st.toast("å·²æ¸…é™¤ä»Šæ—¥ç¼“å­˜ï¼Œæ­£åœ¨é‡æ–°æ‹‰å–å®æ—¶æ•°æ®...")
                st.cache_data.clear()
                st.rerun()
            except Exception as e:
                st.error(f"æ“ä½œå¤±è´¥: {e}")

        # 2. ??????????
        if st.button("ğŸ§¹ æ¸…ç©ºåˆ†æ—¶å›¾å†…å­˜ç¼“å­˜"):
            log_action("æ¸…ç©ºåˆ†æ—¶å›¾å†…å­˜ç¼“å­˜")
            st.cache_data.clear()
            st.toast("âœ… å†…å­˜ç¼“å­˜å·²æ¸…ç©ºï¼Œç£ç›˜ç¼“å­˜ä¿ç•™ã€‚")

        # 3. ????????
        if st.button("ğŸ”„ æ‰‹åŠ¨æ›´æ–°è‚¡ç¥¨åç§°"):
            codes_hint = st.session_state.get("last_top_codes", [])
            log_action("æ‰‹åŠ¨æ›´æ–°è‚¡ç¥¨åç§°", codes=len(codes_hint))
            name_map = _refresh_name_map_for_codes(codes_hint, force=True)
            if name_map:
                st.toast(f"âœ… å·²æ›´æ–°åç§°æ˜ å°„ï¼š{len(name_map)} æ¡")
            else:
                st.warning("æœªè·å–åˆ°æœ€æ–°åç§°æ˜ å°„ã€‚")

        # 4. ????????
        if st.button("ğŸ—‘ï¸ åˆ é™¤æœ¬åœ°åˆ†æ—¶ç¼“å­˜"):
            log_action("åˆ é™¤æœ¬åœ°åˆ†æ—¶ç¼“å­˜")
            clear_min_cache()
            st.cache_data.clear()
            st.toast("âœ… æœ¬åœ°åˆ†æ—¶ç¼“å­˜å·²åˆ é™¤ã€‚")

        # 5. ????
        if st.button("ğŸš¨ å½»åº•é‡ç½® (åˆ é™¤æ‰€æœ‰)"):
            log_action("å½»åº•é‡ç½®")
            if os.path.exists(CACHE_FILE):
                os.remove(CACHE_FILE)
                st.toast("å·²åˆ é™¤å†å²æ—¥çº¿ç¼“å­˜ã€‚")
            clear_min_cache()
            st.cache_data.clear()
            st.rerun()

    with st.expander("ğŸ’¾ æ•°æ®å¤‡ä»½ä¸æ¢å¤", expanded=False):
        st.caption("å¤‡ä»½ data ç›®å½•ï¼ˆå†å²æ—¥çº¿ + åˆ†æ—¶ç¼“å­˜ï¼‰")
        if st.button("ğŸ“¦ ç”Ÿæˆå¤‡ä»½", key="backup_build"):
            log_action("ç”Ÿæˆå¤‡ä»½")
            data_bytes = build_data_backup_zip()
            if data_bytes:
                st.session_state["backup_zip"] = data_bytes
                st.session_state["backup_name"] = f"capmap_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.zip"
                st.toast("âœ… å¤‡ä»½å·²ç”Ÿæˆ")
            else:
                st.warning("æ²¡æœ‰å¯å¤‡ä»½çš„æ•°æ®ã€‚")
        if "backup_zip" in st.session_state:
            download_clicked = st.download_button(
                "â¬‡ï¸ ä¸‹è½½å¤‡ä»½",
                data=st.session_state["backup_zip"],
                file_name=st.session_state.get("backup_name", "capmap_data_backup.zip"),
                mime="application/zip",
                key="backup_download",
            )
            if download_clicked:
                log_action("ä¸‹è½½å¤‡ä»½")
        uploaded = st.file_uploader("æ¢å¤å¤‡ä»½ï¼ˆ.zipï¼‰", type=["zip"], key="backup_upload")
        if uploaded is not None and st.button("â™»ï¸ æ¢å¤å¤‡ä»½", key="backup_restore"):
            log_action("æ¢å¤å¤‡ä»½", file=getattr(uploaded, "name", ""))
            try:
                restored = restore_data_backup(uploaded)
                st.cache_data.clear()
                log_action("æ¢å¤å¤‡ä»½å®Œæˆ", files=restored)
                st.toast(f"âœ… å·²æ¢å¤ {restored} ä¸ªæ–‡ä»¶")
                st.rerun()
            except Exception as e:
                st.error(f"æ¢å¤å¤±è´¥: {e}")
    st.info("æ•°æ®æºï¼šæ²ªæ·±300æˆåˆ†è‚¡ (Biying)")
    st.caption("æ³¨ï¼šæ–¹å—å¤§å°ä½¿ç”¨'æˆäº¤é¢'ä»£æ›¿'å¸‚å€¼'ï¼Œ\nåæ˜ å½“æ—¥äº¤æ˜“çƒ­åº¦ã€‚")

    st.markdown("---")
    st.markdown("### ğŸ› ï¸ æ¿å—è¿‡æ»¤")
    filter_cyb = st.checkbox("å±è”½åˆ›ä¸šæ¿ (300å¼€å¤´)", value=False)
    # é»˜è®¤å‹¾é€‰å±è”½ç§‘åˆ›æ¿ï¼Œå› ä¸ºå…¶æ³¢åŠ¨è¾ƒå¤§ï¼Œå¯èƒ½å½±å“æ•´ä½“çƒ­åŠ›å›¾è§‚æ„Ÿ
    filter_kcb = st.checkbox("å±è”½ç§‘åˆ›æ¿ (688å¼€å¤´)", value=True)
    filter_state = (filter_cyb, filter_kcb)
    if st.session_state.get("filter_state") != filter_state:
        st.session_state["filter_state"] = filter_state
        log_action("ç­›é€‰æ¡ä»¶å˜æ›´", cyb=filter_cyb, kcb=filter_kcb)
    
# åŠ è½½æ•°æ®
with st.spinner("æ­£åœ¨åˆå§‹åŒ–å†å²æ•°æ®ä»“åº“..."):
    pool_code = st.session_state.get("index_pool_code", "000300")
    origin_df = fetch_history_data(pool_code)
    _start_auto_prefetch_if_needed(origin_df)

# --- åå°ä»»åŠ¡æ£€æµ‹ä¸æ§åˆ¶ ---
# æ£€æŸ¥æ˜¯å¦æœ‰åä¸º "PrefetchWorker" çš„åå°çº¿ç¨‹
bg_thread = None
for t in threading.enumerate():
    if t.name == "PrefetchWorker":
        bg_thread = t
        break

# æ›´æ–° Sidebar UI
with st.sidebar:
    st.markdown("---")
    
    # å¯¼èˆªæ 
    nav_option = st.radio("ğŸ“¡ åŠŸèƒ½å¯¼èˆª", [NAV_OVERVIEW, NAV_HISTORY, NAV_STOCK, NAV_DIVERGENCE, NAV_MANAGER], index=0)
    prev_nav = st.session_state.get("nav_option_prev")
    if prev_nav != nav_option:
        st.session_state["nav_option_prev"] = nav_option
        log_action("åŠŸèƒ½å¯¼èˆªåˆ‡æ¢", nav=nav_option)
    
    # (æŒ‡æ•°æ± é€‰æ‹©å·²ç§»è‡³é¡¶éƒ¨)

    with st.expander("ğŸ“¥ åå°æ•°æ®é¢„å–", expanded=False):
        st.caption("åå°é™é»˜ä¸‹è½½è¿‡å»åˆ†æ—¶æ•°æ® (5åˆ†é’ŸKçº¿)")
        st.info("ğŸ’¡ ä½¿ç”¨å•†ç”¨æ¥å£æ—¶ï¼Œå¯ä»¥è®¾ç½®è¾ƒå¤§çš„é¢„å–å¤©æ•°ä»¥è¦†ç›–æ‰€æœ‰å†å²ã€‚æ¨èè®¾ç½®ä¸º 3650 (10å¹´) ä»¥åˆå§‹åŒ–å…¨é‡åˆ†æ—¶æ•°æ®ã€‚")
        prefetch_days = st.number_input("é¢„å–å¤©æ•°", min_value=5, max_value=5000, value=365, step=10)
        
        if bg_thread and bg_thread.is_alive():
            st.info(f"ğŸŸ¢ åå°ä»»åŠ¡è¿è¡Œä¸­...\nè¯·å…³æ³¨æ§åˆ¶å°(Console)æ—¥å¿—")
            # æ— æ³•é€šè¿‡ Button åœæ­¢çº¿ç¨‹ï¼Œé™¤éä½¿ç”¨ Eventã€‚æš‚ä¸å®ç°åœæ­¢ã€‚
        else:
            if st.button("ğŸš€ å¯åŠ¨åå°ä¸‹è½½"):
                log_action("å¯åŠ¨åå°é¢„å–", days=prefetch_days)
                if not origin_df.empty:
                    # è·å–æ—¥æœŸåˆ—è¡¨
                    all_dates = sorted(origin_df['æ—¥æœŸ'].dt.date.unique())
                    target_prefetch_dates = all_dates[-prefetch_days:]
                    
                    # å¯åŠ¨çº¿ç¨‹
                    t = threading.Thread(
                        target=background_prefetch_task,
                        args=(target_prefetch_dates, origin_df),
                        name="PrefetchWorker",
                        daemon=True
                    )
                    add_script_run_ctx(t)
                    t.start()
                    st.rerun()
                else:
                    st.error("å†å²æ•°æ®å°šæœªå°±ç»ª")

if not origin_df.empty:
    # --- å…¨å±€è¿‡æ»¤é€»è¾‘ ---
    df = origin_df.copy()
    if filter_cyb:
        df = df[~df['ä»£ç '].astype(str).str.startswith('300')]
    if filter_kcb:
        df = df[~df['ä»£ç '].astype(str).str.startswith('688')]

    if df.empty:
        st.warning("è¿‡æ»¤åæ²¡æœ‰å‰©ä½™è‚¡ç¥¨æ•°æ®ï¼Œè¯·å–æ¶ˆå‹¾é€‰è¿‡æ»¤é€‰é¡¹ã€‚")
        st.stop()

    # --- æ—¶é—´é€‰æ‹©å™¨é€»è¾‘ (Session State ç®¡ç†) ---
    available_dates = sorted(df['æ—¥æœŸ'].dt.date.unique())
    
    if nav_option == NAV_OVERVIEW:
        render_market_overview(origin_df)

    elif nav_option == NAV_HISTORY:
        render_history_view(df, available_dates)
    
    elif nav_option == NAV_STOCK:
        render_stock_analysis_view(origin_df)

    elif nav_option == NAV_MANAGER:
        render_data_manager(origin_df)

    elif nav_option == NAV_DIVERGENCE:
        render_divergence_view(df, available_dates)

else:
    st.warning("âš ï¸ æœªè·å–åˆ°ä»»ä½•å†å²æ•°æ®ã€‚å¯èƒ½åŸå› ï¼š")
    st.markdown("""
    1. **ç½‘ç»œè¿æ¥é—®é¢˜**ï¼šæ— æ³•è¿æ¥åˆ° AkShare æˆ– å¿…ç›ˆ APIã€‚
    2. **æ¥å£é™åˆ¶**ï¼šæ•°æ®æºæ¥å£å¯èƒ½æš‚æ—¶ä¸å¯ç”¨ã€‚
    3. **åˆå§‹åŒ–ä¸­æ–­**ï¼šå¦‚æœæ˜¯é¦–æ¬¡è¿è¡Œï¼Œè¯·æ£€æŸ¥æ§åˆ¶å°æ—¥å¿—æ˜¯å¦æœ‰æŠ¥é”™ã€‚
    """)
    if st.button("ğŸ”„ é‡è¯•å…¨é‡åˆå§‹åŒ–"):
        st.cache_data.clear()
        if os.path.exists("data/csi300_history_cache.parquet"):
            os.remove("data/csi300_history_cache.parquet")
        st.rerun()
