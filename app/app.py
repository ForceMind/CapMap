import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
import akshare as ak
from datetime import datetime, timedelta
import time
import random
import os
import concurrent.futures
import threading
import sys

# å°è¯•å¯¼å…¥ Streamlit ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œç”¨äºè§£å†³å¤šçº¿ç¨‹ "missing ScriptRunContext" è­¦å‘Š
try:
    from streamlit.runtime.scriptrunner import add_script_run_ctx, get_script_run_ctx
except ImportError:
    # å…¼å®¹æ—§ç‰ˆæœ¬ Streamlit
    from streamlit.scriptrunner import add_script_run_ctx, get_script_run_ctx

# é…ç½®é¡µé¢ä¿¡æ¯
st.set_page_config(
    page_title="Aè‚¡å†å²ç›˜é¢å›æ”¾ç³»ç»Ÿ",
    page_icon="âª",
    layout="wide"
)

# -----------------------------------------------------------------------------
# 1. æ ¸å¿ƒæ•°æ®é€»è¾‘
# -----------------------------------------------------------------------------

CACHE_FILE = "data/csi300_history_cache.parquet"

def get_start_date(years_back=2):
    """è®¡ç®— N å¹´å‰çš„æ—¥æœŸï¼Œè¿”å› YYYYMMDD å­—ç¬¦ä¸²"""
    target = datetime.now() - timedelta(days=365 * years_back)
    return target.strftime("%Y%m%d")

def fetch_history_data():
    """
    è·å–æ²ªæ·±300æˆåˆ†è‚¡è¿‡å»2å¹´çš„æ—¥çº¿æ•°æ®ã€‚
    å¢é‡æ›´æ–°é€»è¾‘ï¼š
    1. å°è¯•è¯»å–æœ¬åœ°ç¼“å­˜ã€‚
    2. å¦‚æœæœ‰ç¼“å­˜ï¼Œæ£€æŸ¥ç¼“å­˜ä¸­æœ€æ–°çš„æ—¥æœŸã€‚
    3. å¦‚æœ æœ€æ–°æ—¥æœŸ < æ˜¨å¤© (æˆ–ä»Šå¤©æ”¶ç›˜å)ï¼Œåˆ™åªä¸‹è½½å¢é‡æ•°æ®ï¼ˆä¸ºäº†ç®€å•å¯é ï¼ŒAkShareæ—¥çº¿æ¥å£é€šå¸¸æ˜¯æŒ‰æ®µä¸‹è½½ï¼Œæˆ–è€…å…¨é‡ä¸‹è½½ï¼‰ã€‚
       * ä¿®æ­£ç­–ç•¥ï¼šç”±äº ak.stock_zh_a_hist æ¥å£å‚æ•°æ˜¯ start_date å’Œ end_dateï¼Œ
         æˆ‘ä»¬å¯ä»¥åªä¸‹è½½ [ç¼“å­˜æœ€æ–°æ—¥æœŸ+1, ä»Šå¤©] çš„æ•°æ®ï¼Œç„¶å append åˆ°ç¼“å­˜ä¸­ã€‚
    """
    cached_df = pd.DataFrame()
    last_cached_date = None

    # 1. å°è¯•åŠ è½½æœ¬åœ°ç¼“å­˜
    if os.path.exists(CACHE_FILE):
        try:
            cached_df = pd.read_parquet(CACHE_FILE)
            if not cached_df.empty:
                last_cached_date = cached_df['æ—¥æœŸ'].max().date()
                st.toast(f"âœ… å·²åŠ è½½æœ¬åœ°ç¼“å­˜ï¼Œæœ€æ–°æ—¥æœŸ: {last_cached_date}")
        except Exception as e:
            st.error(f"è¯»å–ç¼“å­˜æ–‡ä»¶å¤±è´¥: {e}")

    # 2. è®¡ç®—éœ€è¦ä¸‹è½½çš„æ—¶é—´èŒƒå›´
    today = datetime.now().date()
    
    # å¦‚æœç¼“å­˜é‡Œçš„æ—¥æœŸå·²ç»æ˜¯ä»Šå¤©ï¼Œä¸”ç°åœ¨æ˜¯ç›˜ä¸­ï¼Œå¯èƒ½ç”¨æˆ·æƒ³åˆ·æ–°
    # ä½†ç®€å•èµ·è§ï¼Œæˆ‘ä»¬è®¾å®šï¼šå¦‚æœç¼“å­˜æœ€æ–°æ—¥æœŸ < ä»Šå¤©ï¼Œè‚¯å®šè¦å°è¯•ä¸‹è½½ã€‚
    # å¦‚æœç¼“å­˜æœ€æ–°æ—¥æœŸ == ä»Šå¤©ï¼Œåªæœ‰å½“å¼ºåˆ¶åˆ·æ–°æ—¶æ‰é€šè¿‡(å¤–éƒ¨æ§åˆ¶)ï¼Œè¿™é‡Œå‡½æ•°å†…éƒ¨å…ˆå‡è®¾"å·²æ˜¯æœ€æ–°"
    # ä½†ä¸ºäº†æ”¯æŒç›˜ä¸­åˆ·æ–°ï¼Œå¦‚æœ last_cached_date == todayï¼Œæˆ‘ä»¬å…¶å®å¯ä»¥é‡æ‹‰ä»Šå¤©çš„ã€‚
    # è¿™é‡Œæˆ‘ä»¬åªå¤„ç† last_cached_date < today çš„è‡ªåŠ¨å¢é‡, æˆ–è€… force refresh (caller clears cache)
    
    if last_cached_date:
        if last_cached_date >= today:
             # å¦‚æœå·²ç»æœ‰ä»Šå¤©çš„æ•°æ®ï¼Œæš‚æ—¶ç›´æ¥è¿”å› (ç”¨æˆ·éœ€ç‚¹å‡»å¼ºåˆ¶åˆ·æ–°æ¥æ›´æ–°ä»Šæ—¥ç›˜ä¸­æ•°æ®)
             # ä½†ä¸ºäº†èƒ½å¤Ÿ"è‡ªåŠ¨"æ‹‰å–ç›˜ä¸­ï¼Œå¦‚æœ last_cached_date == todayï¼Œæˆ‘ä»¬åšä¸ªåˆ¤æ–­ï¼Ÿ
             # ç°åœ¨çš„é€»è¾‘æ˜¯ï¼šå¦‚æœç¼“å­˜æ–‡ä»¶å­˜åœ¨ä¸”æ—¥æœŸ>=ä»Šå¤©ï¼Œå°±ä¸åŠ¨äº†ã€‚
             # è¿™å¯¼è‡´å¦‚æœæ—©ä¸Š9ç‚¹è·‘äº†ä¸€æ¬¡ï¼ˆæœ‰æ•°æ®ï¼‰ï¼Œä¸‹åˆ3ç‚¹å†è·‘ï¼Œè¿˜æ˜¯æ—§çš„ã€‚
             # æ”¹è¿›ï¼šå¦‚æœæ˜¯ä»Šå¤©ï¼Œä¸”ç°åœ¨è¿˜æ²¡æ”¶ç›˜ï¼Œæˆ–è€…åˆšæ”¶ç›˜ï¼Œå…è®¸è¦†ç›–ï¼Ÿ
             # æš‚ä¿ç•™åŸé€»è¾‘é˜²æ­¢é¢‘ç¹è¯·æ±‚ï¼Œä¾é  "å¼ºåˆ¶åˆ·æ–°" æŒ‰é’®æ¥æ¸…ç©ºç¼“å­˜ã€‚
             return cached_df
        
        start_date_str = (last_cached_date + timedelta(days=1)).strftime("%Y%m%d")
    else:
        start_date_str = get_start_date(2)
        
    end_date_str = today.strftime("%Y%m%d")

    # å¦‚æœä¸éœ€è¦æ›´æ–°
    if start_date_str > end_date_str:
        return cached_df

    # çŠ¶æ€å®¹å™¨
    status_text = st.empty()
    progress_bar = st.progress(0)
    
    try:
        # å¦‚æœæ˜¯å¢é‡æ›´æ–°
        is_incremental = not cached_df.empty
        if not is_incremental:
            status_text.text("æ­£åœ¨åˆå§‹åŒ–å…¨é‡å†å²æ•°æ®...")
        else:
            status_text.text(f"æ­£åœ¨æ£€æŸ¥å¢é‡æ•°æ® ({start_date_str} - {end_date_str})...")

        # è·å–æˆåˆ†è‚¡åˆ—è¡¨
        try:
            cons_df = ak.index_stock_cons(symbol="000300")
        except:
             if not cached_df.empty:
                 st.warning("æˆåˆ†è‚¡åˆ—è¡¨è·å–å¤±è´¥ï¼Œä½¿ç”¨ç¼“å­˜æ•°æ®")
                 return cached_df
             return pd.DataFrame()
        
        if cons_df is None or cons_df.empty:
             return cached_df if not cached_df.empty else pd.DataFrame()

        if 'variety' in cons_df.columns:
            code_col, name_col = 'variety', 'name'
        elif 'å“ç§ä»£ç ' in cons_df.columns:
            code_col, name_col = 'å“ç§ä»£ç ', 'å“ç§åç§°'
        else:
            code_col = cons_df.columns[0]
            name_col = cons_df.columns[1]
            
        stock_list = cons_df[code_col].tolist()
        stock_names = dict(zip(cons_df[code_col], cons_df[name_col]))
        
        # --- å°è¯•è·å–ä»Šæ—¥å®æ—¶æ•°æ® (Spot) ä¿®æ”¹è‚¡ç¥¨åç§° ---
        try:
             spot_df = ak.stock_zh_a_spot_em()
             if spot_df is not None and not spot_df.empty:
                 # æ›´æ–°åç§°æ˜ å°„
                 spot_df['ä»£ç '] = spot_df['ä»£ç '].astype(str)
                 new_names = dict(zip(spot_df['ä»£ç '], spot_df['åç§°']))
                 stock_names.update(new_names)
        except Exception as e:
             print(f"Update stock names failed: {e}")

        new_data_list = []
        total_stocks = len(stock_list)
        
        # --- å°è¯•è·å–ä»Šæ—¥å®æ—¶æ•°æ® (Spot) ä½œä¸ºè¡¥å…… ---
        # å¾ˆå¤šæ—¶å€™ stock_zh_a_hist åœ¨ç›˜ä¸­ä¸è¿”å›å½“æ—¥æ•°æ®ï¼Œæˆ–è€…æœ‰äº›æºä¸è¿”å›ã€‚
        # æˆ‘ä»¬å¯ä»¥æ‹‰å– ak.stock_zh_a_spot_em() è·å–æ‰€æœ‰Aè‚¡å®æ—¶è¡Œæƒ…ï¼Œç„¶åè¿‡æ»¤å‡º CSI300
        # ä»…å½“æˆ‘ä»¬éœ€è¦ "ä»Šå¤©" çš„æ•°æ®æ—¶ (start_date_str <= today_str)
        today_spot_map = {}
        has_today_hist = False # æ ‡è®°æ˜¯å¦é€šè¿‡ hist æ¥å£æ‹¿åˆ°äº†ä»Šå¤©æ•°æ®
        
        if end_date_str >= start_date_str:
             try:
                 spot_df = ak.stock_zh_a_spot_em()
                 if spot_df is not None and not spot_df.empty:
                     # spot_df columns: ä»£ç , åç§°, æœ€æ–°ä»·, æ¶¨è·Œå¹…, æˆäº¤é¢ ...
                     # å»ºç«‹æ˜ å°„: code -> row
                     spot_df['ä»£ç '] = spot_df['ä»£ç '].astype(str)
                     today_spot_map = spot_df.set_index('ä»£ç ').to_dict('index')
             except Exception as e:
                 print(f"å®æ—¶æ•°æ®æ‹‰å–å¤±è´¥: {e}")

        # å¾ªç¯è·å–å†å²
        # ä½¿ç”¨ ThreadPoolExecutor åŠ é€Ÿå¢é‡å†å²ä¸‹è½½ (å¦‚æœéœ€è¦ä¸‹è½½å¾ˆå¤šå¤©)
        # ä½† akshare æ¥å£é¢‘ç¹è°ƒç”¨å¯èƒ½å—é™ï¼Œé€‚åº¦å¹¶å‘
        
        def fetch_one_stock(code, name):
            try:
                # è·å–æ—¥çº¿
                df_hist = ak.stock_zh_a_hist(symbol=code, start_date=start_date_str, end_date=end_date_str, adjust="qfq")
                
                # æ£€æŸ¥æ˜¯å¦åŒ…å«ä»Šå¤©
                # å¦‚æœ df_hist ä¸åŒ…å«ä»Šå¤©ï¼Œä½†æˆ‘ä»¬æœ‰ today_spot_mapï¼Œåˆ™äººå·¥è¡¥ä¸€è¡Œ
                fetched_today = False
                if df_hist is not None and not df_hist.empty:
                    df_hist['æ—¥æœŸ'] = pd.to_datetime(df_hist['æ—¥æœŸ'])
                    if end_date_str in df_hist['æ—¥æœŸ'].dt.strftime("%Y%m%d").values:
                        fetched_today = True
                else:
                    df_hist = pd.DataFrame()

                # å¦‚æœæ²¡æœ‰æ‹‰åˆ°ä»Šå¤©çš„æ•°æ®ï¼Œä¸”æˆ‘ä»¬éœ€è¦ä»Šå¤© (end_date_str == today)ï¼Œè¡¥å…¨
                if (not fetched_today) and (end_date_str == datetime.now().strftime("%Y%m%d")):
                    if code in today_spot_map:
                        row = today_spot_map[code]
                        # æ„é€ ä¸€è¡Œ
                        # å¿…é¡»å­—æ®µ: æ—¥æœŸ, æ”¶ç›˜, æ¶¨è·Œå¹…, æˆäº¤é¢, ä»£ç , åç§°
                        # spot row keys: 'æœ€æ–°ä»·', 'æ¶¨è·Œå¹…', 'æˆäº¤é¢'
                        try:
                             new_row = pd.DataFrame([{
                                 'æ—¥æœŸ': pd.to_datetime(end_date_str),
                                 'æ”¶ç›˜': row['æœ€æ–°ä»·'],
                                 'æ¶¨è·Œå¹…': row['æ¶¨è·Œå¹…'],
                                 'æˆäº¤é¢': row['æˆäº¤é¢'],
                                 'ä»£ç ': code,
                                 'åç§°': name
                             }])
                             df_hist = pd.concat([df_hist, new_row], ignore_index=True)
                        except:
                            pass
                
                if df_hist is not None and not df_hist.empty:
                    # ç¡®ä¿åˆ—å­˜åœ¨
                    if 'æ—¥æœŸ' not in df_hist.columns: return None
                    cols_needed = ['æ—¥æœŸ', 'æ”¶ç›˜', 'æ¶¨è·Œå¹…', 'æˆäº¤é¢']
                    for c in cols_needed:
                        if c not in df_hist.columns: return None
                    
                    df_hist = df_hist[cols_needed].copy()
                    df_hist['ä»£ç '] = code
                    df_hist['åç§°'] = name
                    return df_hist
            except Exception:
                pass
            return None

        # å¦‚æœæ˜¯å¢é‡åªå·®1å¤©ï¼Œå…¶å®å•çº¿ç¨‹ä¹Ÿå¿«ã€‚å¦‚æœæ˜¯åˆå§‹åŒ–ï¼Œå¹¶å‘ã€‚
        # Use concurrency
        ctx = get_script_run_ctx()
        def fetch_one_stock_wrapper(code, name):
            if ctx:
                add_script_run_ctx(threading.current_thread(), ctx)
            return fetch_one_stock(code, name)

        with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
             future_map = {executor.submit(fetch_one_stock_wrapper, c, stock_names.get(c, c)): c for c in stock_list}
             
             for i, future in enumerate(concurrent.futures.as_completed(future_map)):
                 # Update progress
                 if i % 10 == 0:
                     progress_bar.progress((i + 1) / total_stocks)
                     status_text.text(f"æ­£åœ¨åŒæ­¥æ•°æ®: {i+1}/{total_stocks}")
                 
                 res = future.result()
                 if res is not None:
                     new_data_list.append(res)
                
        status_text.empty()
        progress_bar.empty()
        
        # åˆå¹¶é€»è¾‘
        if new_data_list:
            new_df = pd.concat(new_data_list, ignore_index=True)
            # ç±»å‹è½¬æ¢
            new_df['æ—¥æœŸ'] = pd.to_datetime(new_df['æ—¥æœŸ'])
            new_df['æ¶¨è·Œå¹…'] = pd.to_numeric(new_df['æ¶¨è·Œå¹…'], errors='coerce')
            new_df['æˆäº¤é¢'] = pd.to_numeric(new_df['æˆäº¤é¢'], errors='coerce')
            new_df['æ”¶ç›˜'] = pd.to_numeric(new_df['æ”¶ç›˜'], errors='coerce')
            
            if cached_df.empty:
                final_df = new_df
            else:
                # åˆå¹¶æ—§æ•°æ®å’Œæ–°æ•°æ®ï¼Œå¹¶å»é‡
                st.toast(f"ğŸ“¥ æˆåŠŸè·å– {len(new_df)} æ¡æ–°è®°å½•")
                final_df = pd.concat([cached_df, new_df], ignore_index=True)
                # æŒ‰ 'æ—¥æœŸ' + 'ä»£ç ' å»é‡ï¼Œä¿ç•™æ–°çš„ï¼ˆå¦‚æœé‡å ï¼‰
                final_df.drop_duplicates(subset=['æ—¥æœŸ', 'ä»£ç '], keep='last', inplace=True)
        else:
            # æ²¡ä¸‹è½½åˆ°æ–°æ•°æ®ï¼ˆå¯èƒ½æ˜¯å‡æœŸï¼‰
            final_df = cached_df
            
        if final_df.empty:
            return pd.DataFrame()

        final_df = final_df.sort_values('æ—¥æœŸ')
        
        # ä½¿ç”¨æœ€æ–°çš„ stock_names æ›´æ–° DataFrame ä¸­çš„åç§°åˆ—
        if final_df is not None and not final_df.empty:
            # åªæ›´æ–°å­˜åœ¨çš„ä»£ç 
            final_df['åç§°'] = final_df['ä»£ç '].map(stock_names).fillna(final_df['åç§°'])
        
        # åªæœ‰å½“æœ‰æ–°æ•°æ® æˆ–è€… æ˜¯é¦–æ¬¡ä¸‹è½½æ—¶ï¼Œæ‰ä¿å­˜
        if new_data_list or cached_df.empty:
            try:
                if not os.path.exists("data"):
                    os.makedirs("data")
                final_df.to_parquet(CACHE_FILE)
                if not cached_df.empty:
                    st.toast("ğŸ’¾ å¢é‡æ•°æ®å·²åˆå¹¶å¹¶ä¿å­˜")
                else:
                    st.success("ğŸ’¾ å…¨é‡æ•°æ®å·²åˆå§‹åŒ–")
            except Exception as e:
                st.warning(f"æ— æ³•ä¿å­˜ç¼“å­˜: {e}")

        return final_df

    except Exception as e:
        st.error(f"å…¨å±€æ•°æ®é”™è¯¯: {e}")
        status_text.empty()
        progress_bar.empty()
        return pd.DataFrame()

# -----------------------------------------------------------------------------
@st.cache_data(ttl=3600*24, show_spinner=False)
def fetch_cached_min_data(symbol, date_str, is_index=False, period='1'):
    """
    åŸå­åŒ–è·å–å•ä¸ªæ ‡çš„çš„åˆ†æ—¶æ•°æ®ï¼Œç‹¬ç«‹ç¼“å­˜ã€‚
    é¿å…å› è‚¡ç¥¨åˆ—è¡¨ç»„åˆå˜åŒ–å¯¼è‡´æ•´ä¸ªç¼“å­˜å¤±æ•ˆã€‚
    params:
    period: '1', '5', '15', '30', '60'
    """
    start_time = f"{date_str} 09:30:00"
    end_time = f"{date_str} 15:00:00"
    
    # æŒ‡æ•°é€€é¿ç­–ç•¥å…¨å±€å˜é‡ (ç®€å•æ¨¡æ‹Ÿï¼Œå®é™…ç¯å¢ƒåº”ç”¨ç±»å°è£…)
    # ä½¿ç”¨å‡½æ•°å±æ€§æš‚å­˜çŠ¶æ€
    if not hasattr(fetch_cached_min_data, "current_backoff"):
        fetch_cached_min_data.current_backoff = 0
            
    # ç®€å•çš„é‡è¯•æœºåˆ¶
    max_retries = 3
    
    # å¦‚æœå¤„äº"å†·å´æœŸ"å†…? è¿™é‡Œç®€åŒ–ä¸ºï¼šæ¯æ¬¡å¤±è´¥åå¢åŠ ç­‰å¾…æ—¶é—´ï¼ŒæˆåŠŸåˆ™é‡ç½®
    
    for attempt in range(max_retries):
        try:
            if is_index:
                # æŒ‡æ•°æ¥å£
                df = ak.index_zh_a_hist_min_em(symbol=symbol, period=period, start_date=start_time, end_date=end_time)
            else:
                # ä¸ªè‚¡æ¥å£
                df = ak.stock_zh_a_hist_min_em(symbol=symbol, start_date=start_time, end_date=end_time, period=period, adjust='qfq')
            
            if df is not None and not df.empty:
                # æˆåŠŸ - é‡ç½®é€€é¿
                if fetch_cached_min_data.current_backoff > 0:
                     print(f"[{datetime.now().time()}] API æ¢å¤ã€‚é‡ç½®é€€é¿æ—¶é—´ã€‚")
                     fetch_cached_min_data.current_backoff = 0

                # ç»Ÿä¸€åˆ—å
                if 'æ—¶é—´' in df.columns:
                    df.rename(columns={'æ—¶é—´': 'time', 'å¼€ç›˜': 'open', 'æ”¶ç›˜': 'close'}, inplace=True)
                
                # ç®€å•æ¸…æ´—
                df['time'] = pd.to_datetime(df['time'])
                
                # è®¡ç®—æ¶¨è·Œå¹… (ç›¸å¯¹äºå½“æ—¥å¼€ç›˜)
                base_price = df['open'].iloc[0]
                df['pct_chg'] = (df['close'] - base_price) / base_price * 100
                
                return df[['time', 'pct_chg', 'close']]
                
        except Exception as e:
            # å¤±è´¥å¤„ç†é€»è¾‘
            # å¦‚æœæ˜¯ç‰¹å®šçš„ API é™åˆ¶é”™è¯¯ (éœ€åˆ†æ eï¼Œè¿™é‡Œç®€å•å‡è®¾æ‰€æœ‰å¼‚å¸¸éƒ½å¯èƒ½ç”±é¢‘ç‡å¯¼è‡´)
            # å¢åŠ é€€é¿æ—¶é—´
            if fetch_cached_min_data.current_backoff == 0:
                fetch_cached_min_data.current_backoff = 60 # åˆå§‹ 1 åˆ†é’Ÿ
            else:
                fetch_cached_min_data.current_backoff *= 2 # ç¿»å€
            
            wait_time = fetch_cached_min_data.current_backoff
            
            # åªæœ‰å½“è¿™æ˜¯åå°é¢„å–ä»»åŠ¡æ—¶æ‰è¿›è¡Œé•¿æ—¶é—´ç­‰å¾…? 
            # å‰å°å®æ—¶æ‹‰å–ä¸å®œç­‰å¾…å¤ªä¹…ã€‚è¿™é‡Œæˆ‘ä»¬æ·»åŠ ä¸€ä¸ªä¸Šä¸‹æ–‡åˆ¤æ–­æ˜¯ä¸ç°å®çš„ã€‚
            # ä½†æ—¢ç„¶ç”¨æˆ·æåˆ°äº†"ç¿»å€ç­‰å¾…"ï¼Œè¿™é€šå¸¸æ˜¯é’ˆå¯¹åå°çˆ¬è™«ã€‚
            # å¯¹äºå‰å°äº¤äº’ï¼Œç­‰å¾…1åˆ†é’Ÿç”¨æˆ·æ—©è·‘äº†ã€‚
            # ä¸ºäº†å…¼å®¹ï¼Œæˆ‘ä»¬åªåœ¨ "é¢„å–/çˆ¬è™«" æ¨¡å¼ä¸‹å¯ç”¨æ­¤é€»è¾‘ï¼Ÿ 
            # ä½† fetch_cached_min_data æ˜¯é€šç”¨å‡½æ•°ã€‚
            # å¦¥åï¼šå¦‚æœç­‰å¾…æ—¶é—´å¾ˆé•¿ (>5s)ï¼Œåˆ™å¯ä»¥è®¤ä¸ºè¿™æ˜¯ä¸€ä¸ªéœ€è¦é•¿æ—¶é—´æ¢å¤çš„é”™è¯¯ï¼Œ
            # åœ¨å‰å°ç›´æ¥å¤±è´¥æ¯”è¾ƒå¥½ã€‚åœ¨åå°åˆ™ sleepã€‚
            # ä½†è¿™é‡Œæ— æ³•åŒºåˆ†ã€‚æˆ‘ä»¬å‡è®¾æ­¤ä¸¥æ ¼çš„é€€é¿ç­–ç•¥åªåœ¨å¤–éƒ¨æ§åˆ¶å¾ªç¯ä¸­ç”Ÿæ•ˆæ¯”è¾ƒå¥½ã€‚
            # ä¿®æ”¹ï¼šå°†ä¸¥æ ¼çš„é€€é¿é€»è¾‘ç§»åˆ°è°ƒç”¨æ–¹çš„ loop ä¸­ (Task Worker)ï¼Œ
            # è¿™é‡Œçš„ fetch_cached_min_data åªè´Ÿè´£å•æ¬¡å°è¯•ã€‚
            pass

    return None

# --- æ–°å¢ï¼šåå°é¢„å–çº¿ç¨‹é€»è¾‘ ---
def background_prefetch_task(date_list, origin_df):
    """
    åå°çº¿ç¨‹ï¼šæ‰§è¡Œæ•°æ®é¢„å–ã€‚
    """
    total_dates = len(date_list)
    print(f"\n[åå°ä»»åŠ¡] å¼€å§‹é¢„å– {total_dates} å¤©çš„æ•°æ®ã€‚")
    
    current_backoff = 0 # ç§’
    
    indices_codes = ["000300", "000001", "399001"]
    
    for i, d in enumerate(date_list):
        d_str = d.strftime("%Y-%m-%d")
        print(f"[åå°ä»»åŠ¡] æ­£åœ¨å¤„ç†: {d_str} ({i+1}/{total_dates})")
        
        # ç­›é€‰
        daily = origin_df[origin_df['æ—¥æœŸ'].dt.date == d]
        if daily.empty: continue
        
        # Top 25
        top_stocks = daily.sort_values('æˆäº¤é¢', ascending=False).head(25)['ä»£ç '].tolist()
        
        # ä»»åŠ¡åˆ—è¡¨
        tasks = []
        for code in indices_codes: tasks.append((code, d_str, True))
        for code in top_stocks: tasks.append((code, d_str, False))
        
        # å†…å±‚é€ä¸ªæ‰§è¡Œ (ä¸ºäº†æ–¹ä¾¿æ§åˆ¶é€€é¿ï¼Œä¸”åå°ä»»åŠ¡ä¸æ€¥äºä¸€æ—¶çš„å¹¶å‘ï¼Œç¨³å®šç¬¬ä¸€)
        # å¦‚æœè¦å¹¶å‘ï¼Œä¹Ÿå¿…é¡»åœ¨å¹¶å‘å‘ç”Ÿå¼‚å¸¸æ—¶æ•è·å¹¶è§¦å‘é€€é¿ã€‚
        # ç®€å•èµ·è§ï¼Œè¿™é‡ŒæŒ‰é¡ºåºæˆ–å°æ‰¹æ¬¡æ‰§è¡Œã€‚
        
        for t_code, t_date, t_is_index in tasks:
            
            # Indefinite retry loop with backoff
            while True:
                try:
                    # æ£€æŸ¥é€€é¿
                    if current_backoff > 0:
                        print(f"[åå°ä»»åŠ¡] å¤„äºå†·å´çŠ¶æ€ã€‚ç­‰å¾… {current_backoff} ç§’...")
                        time.sleep(current_backoff)
                        
                    # å°è¯•æ‹‰å– (fetch_cached_min_data å†…éƒ¨æœ‰ç¼“å­˜ï¼Œå¦‚æœå·²å­˜åœ¨ä¼šç›´æ¥è¿”å›)
                    # ä¸ºäº†æµ‹è¯• API è¿æ¥ï¼Œå¦‚æœç¼“å­˜å·²å­˜åœ¨ï¼Œå…¶å®ä¸ä¼šè§¦å‘ç½‘ç»œè¯·æ±‚ã€‚
                    # æˆ‘ä»¬éœ€è¦å‡è®¾ fetch_cached_min_data ä¼šå¤„ç†ç½‘ç»œã€‚
                    # æ³¨æ„ï¼šfetch_cached_min_data è¢« @st.cache_data è£…é¥°ã€‚
                    # åœ¨åå°çº¿ç¨‹è°ƒç”¨ st.cache_data è£…é¥°çš„å‡½æ•°é€šå¸¸æ˜¯æ²¡é—®é¢˜çš„ã€‚
                    
                    fetch_cached_min_data(t_code, t_date, is_index=t_is_index, period='1')
                    # åªæœ‰å½“æˆ‘ä»¬éœ€è¦æ›´å¤šæ•°æ®æ—¶æ‰æ‹‰5åˆ†é’Ÿ
                    # fetch_cached_min_data(t_code, t_date, is_index=t_is_index, period='5') 
                    
                    # Success
                    if current_backoff > 0:
                        print(f"[åå°ä»»åŠ¡] å·²æ¢å¤ã€‚é‡ç½®é€€é¿æ—¶é—´ã€‚")
                        current_backoff = 0
                    
                    # æ‹‰å–æˆåŠŸåç¨å¾® sleep ä¸€ä¸‹é¿å…è¿‡äºé¢‘ç¹ (0.1s)
                    time.sleep(0.1)
                    break # è·³å‡º whileï¼Œå¤„ç†ä¸‹ä¸€ä¸ª task

                except Exception as e:
                    print(f"[åå°ä»»åŠ¡] è·å– {t_code} ({t_date}) å¤±è´¥: {e}")
                    # è§¦å‘é€€é¿æœºåˆ¶
                    if current_backoff == 0:
                        current_backoff = 60
                    else:
                        current_backoff *= 2
                    
                    print(f"[åå°ä»»åŠ¡] é€€é¿æ—¶é—´å¢åŠ åˆ° {current_backoff}ç§’ã€‚æ­£åœ¨é‡è¯•åŒä¸€ä»»åŠ¡...")
                    # Loop continues, will sleep at start of next iteration
    
    print("[åå°ä»»åŠ¡] æ‰€æœ‰ä»»åŠ¡å·²å®Œæˆã€‚")


def fetch_intraday_data_v2(stock_codes, target_date_str, period='1'):
    """
    è·å–æŒ‡å®šè‚¡ç¥¨åˆ—è¡¨ + ä¸‰å¤§æŒ‡æ•° çš„åˆ†é’Ÿçº§æ•°æ® (å¹¶å‘ç‰ˆ)ã€‚
    v2: å¢åŠ ä¸Šè¯ã€æ·±è¯æŒ‡æ•°ï¼Œä¼˜åŒ–ç¼“å­˜ï¼ŒåŸå­åŒ–è°ƒç”¨ã€‚
    v3: å¼•å…¥å¤šçº¿ç¨‹å¹¶å‘åŠ é€Ÿ
    """
    results = [] 
    
    # å®šä¹‰éœ€è¦è·å–çš„æŒ‡æ•°
    indices_map = {
        "000300": "ğŸ“Š æ²ªæ·±300",
        "000001": "ğŸ“ˆ ä¸Šè¯æŒ‡æ•°",
        "399001": "ğŸ“‰ æ·±è¯æˆæŒ‡"
    }

    # ä»»åŠ¡åˆ—è¡¨
    tasks = []

    # 1. æäº¤æŒ‡æ•°ä»»åŠ¡
    for idx_code, idx_name in indices_map.items():
        tasks.append({
            'type': 'index',
            'code': idx_code,
            'name': idx_name,
            'to_val': 99999999999
        })

    # 2. æäº¤ä¸ªè‚¡ä»»åŠ¡
    for code, name, to_val in stock_codes:
        tasks.append({
            'type': 'stock',
            'code': code,
            'name': name,
            'to_val': to_val
        })
        
    def _worker(task):
        try:
            is_index = (task['type'] == 'index')
            data = fetch_cached_min_data(task['code'], target_date_str, is_index=is_index, period=period)
            if data is not None:
                return {
                    'code': task['code'],
                    'name': task['name'],
                    'data': data,
                    'turnover': task['to_val'],
                    'is_index': is_index
                }
        except Exception:
            pass
        return None

    # å¹¶å‘æ‰§è¡Œ
    # çº¿ç¨‹æ•°ä¸å®œè¿‡å¤šï¼Œä»¥å…è§¦å‘åçˆ¬é™åˆ¶ï¼Œ10-20å·¦å³è¾ƒä¸ºå®‰å…¨
    ctx = get_script_run_ctx()
    def _worker_wrapper(t):
        if ctx:
            add_script_run_ctx(threading.current_thread(), ctx)
        return _worker(t)

    with concurrent.futures.ThreadPoolExecutor(max_workers=16) as executor:
        future_to_task = {executor.submit(_worker_wrapper, t): t for t in tasks}
        
        for future in concurrent.futures.as_completed(future_to_task):
            res = future.result()
            if res:
                results.append(res)
            
    return results

# 2. UI å¸ƒå±€
# -----------------------------------------------------------------------------

st.title("Aè‚¡å†å²ç›˜é¢å›æ”¾ç³»ç»Ÿ (æ²ªæ·±300 Market Replay)")

st.markdown("""
> ğŸ•¹ï¸ **æ“ä½œæŒ‡å—**ï¼š
> 1. ç­‰å¾…æ•°æ®åˆå§‹åŒ–å®Œæˆï¼ˆåˆæ¬¡è¿è¡Œå¯èƒ½éœ€è¦ 2-3 åˆ†é’Ÿï¼‰ã€‚
> 2. æ‹–åŠ¨ä¸‹æ–¹æ»‘å—é€‰æ‹©å†å²æ—¥æœŸã€‚
> 3. è§‚å¯Ÿå½“æ—¥ç›˜é¢çš„èµ„é‡‘æµå‘ä¸çƒ­åº¦ã€‚
""")

# ä¾§è¾¹æ 
with st.sidebar:
    st.header("âš™ï¸ æ•°æ®ç®¡ç†")
    
    with st.expander("æ•°æ®åˆ·æ–°ä¸ç»´æŠ¤", expanded=True):
        st.write("å¦‚æœæ•°æ®æ˜¾ç¤ºä¸æ­£ç¡®ï¼Œè¯·å°è¯•ä»¥ä¸‹æ“ä½œï¼š")
        
        # 1. åˆ·æ–°ç›˜ä¸­
        if st.button("ğŸŸ¢ åˆ·æ–°ä»Šæ—¥è¡Œæƒ… (ç›˜ä¸­)"):
            try:
                if os.path.exists(CACHE_FILE):
                    # è¯»å–å¹¶åˆ é™¤ä»Šå¤©çš„è®°å½•ï¼Œå¼ºåˆ¶ä¸‹æ¬¡åŠ è½½æ—¶è§¦å‘å¢é‡æ›´æ–°
                    _df = pd.read_parquet(CACHE_FILE)
                    _today = datetime.now().date()
                    # è¿‡æ»¤æ‰ >= ä»Šå¤©çš„æ•°æ®
                    _df_new = _df[_df['æ—¥æœŸ'].dt.date < _today]
                    _df_new.to_parquet(CACHE_FILE)
                    st.toast("å·²æ¸…é™¤ä»Šæ—¥ç¼“å­˜ï¼Œæ­£åœ¨é‡æ–°æ‹‰å–å®æ—¶æ•°æ®...")
                st.cache_data.clear() # å³ä½¿æ˜¯åˆ†æ—¶æ•°æ®æœ€å¥½ä¹Ÿæ¸…ä¸€ä¸‹ï¼Œä»¥é˜²ä¸‡ä¸€
                st.rerun()
            except Exception as e:
                st.error(f"æ“ä½œå¤±è´¥: {e}")

        # 2. æ¸…ç†åˆ†æ—¶ç¼“å­˜
        if st.button("ğŸ§¹ æ¸…ç©ºåˆ†æ—¶å›¾ç¼“å­˜"):
            st.cache_data.clear()
            st.toast("âœ… æ‰€æœ‰å†…å­˜ç¼“å­˜å·²æ¸…ç©ºï¼Œä¸‹æ¬¡æŸ¥çœ‹åˆ†æ—¶å›¾å°†é‡æ–°ä¸‹è½½ã€‚")

        # 3. ç¡¬é‡ç½®
        if st.button("ğŸš¨ å½»åº•é‡ç½® (åˆ é™¤æ‰€æœ‰)"):
            if os.path.exists(CACHE_FILE):
                os.remove(CACHE_FILE)
                st.toast("å·²åˆ é™¤æœ¬åœ°æ‰€æœ‰å†å²æ•°æ®ã€‚")
            st.cache_data.clear()
            st.rerun()

    st.info("æ•°æ®æºï¼šæ²ªæ·±300æˆåˆ†è‚¡ (AkShare)")
    st.caption("æ³¨ï¼šæ–¹å—å¤§å°ä½¿ç”¨'æˆäº¤é¢'ä»£æ›¿'å¸‚å€¼'ï¼Œ\nåæ˜ å½“æ—¥äº¤æ˜“çƒ­åº¦ã€‚")

    st.markdown("---")
    st.markdown("### ğŸ› ï¸ æ¿å—è¿‡æ»¤")
    filter_cyb = st.checkbox("å±è”½åˆ›ä¸šæ¿ (300å¼€å¤´)", value=False)
    filter_kcb = st.checkbox("å±è”½ç§‘åˆ›æ¿ (688å¼€å¤´)", value=False)
    
# åŠ è½½æ•°æ®
with st.spinner("æ­£åœ¨åˆå§‹åŒ–å†å²æ•°æ®ä»“åº“..."):
    origin_df = fetch_history_data()

# --- åå°ä»»åŠ¡æ£€æµ‹ä¸æ§åˆ¶ ---
# æ£€æŸ¥æ˜¯å¦æœ‰åä¸º "PrefetchWorker" çš„åå°çº¿ç¨‹
bg_thread = None
for t in threading.enumerate():
    if t.name == "PrefetchWorker":
        bg_thread = t
        break

# æ›´æ–° Sidebar UI
with st.sidebar:
    st.markdown("---")
    
    # å¯¼èˆªæ 
    nav_option = st.radio("ğŸ“¡ åŠŸèƒ½å¯¼èˆª", ["âª å†å²ç›˜é¢å›æ”¾", "ğŸŒŠ èµ„é‡‘åç¦»åˆ†æ"], index=0)
    
    with st.expander("ğŸ“¥ åå°æ•°æ®é¢„å–", expanded=False):
        st.caption("åå°é™é»˜ä¸‹è½½æœ€è¿‘ N å¤©åˆ†æ—¶æ•°æ®")
        prefetch_days = st.number_input("é¢„å–å¤©æ•°", min_value=5, max_value=200, value=30, step=10)
        
        if bg_thread and bg_thread.is_alive():
            st.info(f"ğŸŸ¢ åå°ä»»åŠ¡è¿è¡Œä¸­...\nè¯·å…³æ³¨æ§åˆ¶å°(Console)æ—¥å¿—")
            # æ— æ³•é€šè¿‡ Button åœæ­¢çº¿ç¨‹ï¼Œé™¤éä½¿ç”¨ Eventã€‚æš‚ä¸å®ç°åœæ­¢ã€‚
        else:
            if st.button("ğŸš€ å¯åŠ¨åå°ä¸‹è½½"):
                if not origin_df.empty:
                    # è·å–æ—¥æœŸåˆ—è¡¨
                    all_dates = sorted(origin_df['æ—¥æœŸ'].dt.date.unique())
                    target_prefetch_dates = all_dates[-prefetch_days:]
                    
                    # å¯åŠ¨çº¿ç¨‹
                    t = threading.Thread(
                        target=background_prefetch_task,
                        args=(target_prefetch_dates, origin_df),
                        name="PrefetchWorker",
                        daemon=True
                    )
                    add_script_run_ctx(t)
                    t.start()
                    st.rerun()
                else:
                    st.error("å†å²æ•°æ®å°šæœªå°±ç»ª")

if not origin_df.empty:
    # --- å…¨å±€è¿‡æ»¤é€»è¾‘ ---
    df = origin_df.copy()
    if filter_cyb:
        df = df[~df['ä»£ç '].astype(str).str.startswith('300')]
    if filter_kcb:
        df = df[~df['ä»£ç '].astype(str).str.startswith('688')]

    if df.empty:
        st.warning("è¿‡æ»¤åæ²¡æœ‰å‰©ä½™è‚¡ç¥¨æ•°æ®ï¼Œè¯·å–æ¶ˆå‹¾é€‰è¿‡æ»¤é€‰é¡¹ã€‚")
        st.stop()

    # --- æ—¶é—´é€‰æ‹©å™¨é€»è¾‘ (Session State ç®¡ç†) ---
    available_dates = sorted(df['æ—¥æœŸ'].dt.date.unique())
    
    if nav_option == "âª å†å²ç›˜é¢å›æ”¾":
        if 'selected_date_idx' not in st.session_state:
            st.session_state.selected_date_idx = len(available_dates) - 1

        #ç¡®ä¿ç´¢å¼•ä¸è¶Šç•Œ
        if st.session_state.selected_date_idx >= len(available_dates):
            st.session_state.selected_date_idx = len(available_dates) - 1
        if st.session_state.selected_date_idx < 0:
            st.session_state.selected_date_idx = 0

        # å¸ƒå±€ï¼šå‰ä¸€å¤© | æ»‘å— | åä¸€å¤©
        st.markdown("### ğŸ“… é€‰æ‹©å›æ”¾æ—¥æœŸ")
        
        # æ¨¡å¼é€‰æ‹©
        mode_col1, mode_col2 = st.columns([1, 3])
        with mode_col1:
            playback_mode = st.radio("å›æ”¾æ¨¡å¼", ["å•æ—¥å¤ç›˜", "å¤šæ—¥èµ°åŠ¿æ‹¼æ¥"], horizontal=True)

        if playback_mode == "å•æ—¥å¤ç›˜":
            col_prev, col_slider, col_next = st.columns([1, 6, 1])
            
            with col_prev:
                st.write("") 
                st.write("")
                if st.button("â¬…ï¸ å‰ä¸€å¤©"):
                    if st.session_state.selected_date_idx > 0:
                        st.session_state.selected_date_idx -= 1
                        st.rerun()

            with col_next:
                st.write("")
                st.write("")
                if st.button("åä¸€å¤© â¡ï¸"):
                    if st.session_state.selected_date_idx < len(available_dates) - 1:
                        st.session_state.selected_date_idx += 1
                        st.rerun()

            with col_slider:
                # åŸ select_slider æ›¿æ¢ä¸º date_input ä»¥æ”¯æŒå¿«é€Ÿå¹´ä»½é€‰æ‹©
                current_date_val = available_dates[st.session_state.selected_date_idx]
                
                picked_date = st.date_input(
                    "æ—¥æœŸ",
                    value=current_date_val,
                    min_value=available_dates[0],
                    max_value=available_dates[-1],
                    label_visibility="collapsed"
                )
                
                # å¦‚æœæ—¥æœŸå‘ç”Ÿå˜åŒ–
                if picked_date != current_date_val:
                    if picked_date in available_dates:
                        st.session_state.selected_date_idx = available_dates.index(picked_date)
                    else:
                        # å¦‚æœé€‰ä¸­çš„æ˜¯éäº¤æ˜“æ—¥ï¼Œå¯»æ‰¾æœ€è¿‘çš„äº¤æ˜“æ—¥
                        closest_date = min(available_dates, key=lambda d: abs(d - picked_date))
                        st.session_state.selected_date_idx = available_dates.index(closest_date)
                        st.toast(f"ğŸ“… ä¼‘å¸‚æ—¥ï¼Œå·²è‡ªåŠ¨å®šä½åˆ°æœ€è¿‘äº¤æ˜“æ—¥: {closest_date}")
                    st.rerun()
            
            target_dates = [available_dates[st.session_state.selected_date_idx]] # ä½¿ç”¨ state ä¸­çš„æ—¥æœŸ
            selected_date = target_dates[0]
            display_date_str = selected_date.strftime("%Y-%m-%d")
            
        else: # å¤šæ—¥èµ°åŠ¿æ‹¼æ¥
            with mode_col2:
                date_range = st.date_input(
                    "é€‰æ‹©æ—¶é—´èŒƒå›´ (å»ºè®®ä¸è¶…è¿‡5å¤©ï¼Œå¦åˆ™åŠ è½½è¾ƒæ…¢)",
                    value=[available_dates[-5] if len(available_dates)>5 else available_dates[0], available_dates[-1]],
                    min_value=available_dates[0],
                    max_value=available_dates[-1]
                )
            
            if len(date_range) == 2:
                start_d, end_d = date_range
                # ç­›é€‰å‡ºèŒƒå›´å†…çš„äº¤æ˜“æ—¥
                target_dates = [d for d in available_dates if start_d <= d <= end_d]
                if not target_dates: # å¦‚æœé€‰å®šçš„èŒƒå›´å†…æ²¡æœ‰äº¤æ˜“æ—¥ (ä¾‹å¦‚å…¨é€‰äº†å‡æœŸ)
                    st.warning("âš ï¸ é€‰å®šèŒƒå›´å†…æ— äº¤æ˜“æ•°æ®ï¼Œå·²è‡ªåŠ¨é‡ç½®ä¸ºæœ€è¿‘äº¤æ˜“æ—¥")
                    target_dates = [available_dates[-1]]
                
                st.info(f"å·²é€‰æ‹© {len(target_dates)} ä¸ªäº¤æ˜“æ—¥è¿›è¡Œæ‹¼æ¥å±•ç¤º")
                selected_date = target_dates[-1] # ç”¨äºä¸‹æ–¹æ˜¾ç¤ºç»Ÿè®¡é¢æ¿çš„åŸºå‡† (å…¼å®¹æ—§ä»£ç å˜é‡å)
                display_date_str = f"{target_dates[0].strftime('%Y%m%d')} ~ {target_dates[-1].strftime('%Y%m%d')}"
            else:
                st.warning("è¯·é€‰æ‹©å®Œæ•´çš„å¼€å§‹å’Œç»“æŸæ—¥æœŸ")
                target_dates = [available_dates[-1]]
                selected_date = available_dates[-1]
                display_date_str = selected_date.strftime("%Y-%m-%d")

        # --- æ•°æ®åˆ‡ç‰‡ä¸ç»Ÿè®¡ (å…¼å®¹å•æ—¥ä¸å¤šæ—¥) ---
        if len(target_dates) == 1:
            # å•æ—¥é€»è¾‘
            daily_df = df[df['æ—¥æœŸ'].dt.date == selected_date].copy()
            if daily_df.empty:
                st.warning(f"{selected_date} å½“æ—¥æ— äº¤æ˜“æ•°æ®ã€‚")
                st.stop()
                
            median_chg = daily_df['æ¶¨è·Œå¹…'].median()
            total_turnover = daily_df['æˆäº¤é¢'].sum() / 1e8 
            top_gainer = daily_df.loc[daily_df['æ¶¨è·Œå¹…'].idxmax()]
            
            metric_label_date = "å½“å‰å›æ”¾æ—¥æœŸ"
            metric_label_chg = "æˆåˆ†è‚¡ä¸­ä½æ•°æ¶¨è·Œ"
            metric_label_to = "æˆåˆ†è‚¡æ€»æˆäº¤"
            
        else:
            # å¤šæ—¥é€»è¾‘ (è®¡ç®—ç´¯è®¡)
            # 1. ç­›é€‰å‡ºèŒƒå›´å†…æ‰€æœ‰æ•°æ®
            start_date_ts = pd.Timestamp(target_dates[0])
            end_date_ts = pd.Timestamp(target_dates[-1])
            
            period_df = df[(df['æ—¥æœŸ'] >= start_date_ts) & (df['æ—¥æœŸ'] <= end_date_ts)].copy()
            
            if period_df.empty:
                st.stop()
                
            # 2. è®¡ç®—åŒºé—´ç´¯è®¡æ¶¨è·Œå¹…
            # æ–¹æ³•: å¯¹æ¯ä¸ªä»£ç ï¼Œæ‰¾åˆ°é¦–å°¾ä»·æ ¼
            # æ³¨æ„: å¦‚æœåªç”¨ period_dfï¼Œé¦–æ—¥çš„æ•°æ®é‡Œçš„ 'æ”¶ç›˜' æ˜¯é¦–æ—¥çš„æ”¶ç›˜ä»·ã€‚
            # åŒºé—´æ¶¨å¹… = (End_Close - Start_Close) / Start_Close ? 
            # æˆ–è€…æ›´ç²¾ç¡®ï¼šStart_Close åº”è¯¥æ˜¯ Start_Date çš„ å‰ä¸€æ—¥æ”¶ç›˜ä»· (å³ Start_Open / (1+Start_Chg))?
            # ç®€åŒ–èµ·è§ï¼Œæˆ‘ä»¬ç”¨ (End_Date Close - Start_Date Open) / Start_Date Open
            # è¿™æ ·èƒ½åŒ…å« Start_Date å½“å¤©çš„æ¶¨è·Œ
            
            agg_stats = []
            
            # ä½¿ç”¨ groupby åŠ é€Ÿ
            grouped = period_df.groupby('ä»£ç ')
            
            for code, group in grouped:
                group = group.sort_values('æ—¥æœŸ')
                if group.empty: continue
                
                first_row = group.iloc[0]
                last_row = group.iloc[-1]
                
                # æ¨ç®—é¦–æ—¥å¼€ç›˜ä»· = æ”¶ç›˜ / (1 + chg/100)
                # è¿™ç§åæ¨å¦‚æœæ˜¯æ¶¨åœæ¿å¤æƒå¯èƒ½å¾®å°è¯¯å·®ï¼Œä½†å¤Ÿç”¨ã€‚
                # ä¹Ÿå¯ä»¥ç›´æ¥ç”¨ akshare ä¸‹è½½çš„ Openï¼Œä½†è¿™é‡Œåªæœ‰ Close/Chg
                # å‡è®¾ Chg æ˜¯ç²¾ç¡®çš„
                try:
                    start_open = first_row['æ”¶ç›˜'] / (1 + first_row['æ¶¨è·Œå¹…']/100)
                    end_close = last_row['æ”¶ç›˜']
                    
                    period_chg = (end_close - start_open) / start_open * 100
                    period_turnover = group['æˆäº¤é¢'].sum()
                    
                    agg_stats.append({
                        'ä»£ç ': code,
                        'åç§°': first_row['åç§°'], # å‡è®¾æ²¡æ”¹å
                        'åŒºé—´æ¶¨è·Œå¹…': period_chg,
                        'åŒºé—´æ€»æˆäº¤': period_turnover
                    })
                except:
                    pass
            
            agg_df = pd.DataFrame(agg_stats)
            
            if agg_df.empty:
                st.warning("åŒºé—´æ•°æ®è®¡ç®—å¼‚å¸¸")
                st.stop()
                
            median_chg = agg_df['åŒºé—´æ¶¨è·Œå¹…'].median()
            total_turnover = period_df['æˆäº¤é¢'].sum() / 1e8
            top_gainer = agg_df.loc[agg_df['åŒºé—´æ¶¨è·Œå¹…'].idxmax()]
            
            # ä¸ºäº†å…¼å®¹åç»­ daily_df çš„ä½¿ç”¨ (Treemap å’Œ é€‰è‚¡)
            # æˆ‘ä»¬éœ€è¦æ„é€ ä¸€ä¸ª "Proxy Daily DF"
            # è®©åé¢çš„é€‰è‚¡é€»è¾‘åŸºäº "åŒºé—´è¡¨ç°"
            daily_df = agg_df.rename(columns={'åŒºé—´æ¶¨è·Œå¹…': 'æ¶¨è·Œå¹…', 'åŒºé—´æ€»æˆäº¤': 'æˆäº¤é¢'}).copy()
            # è¡¥é½å…¶ä»–å­—æ®µ
            # æ”¶ç›˜ä»·ç”¨æœ€åä¸€å¤©çš„
            # daily_df è¿˜éœ€è¦ 'æ”¶ç›˜' ç”¨äºå±•ç¤º? Treemap hover éœ€è¦
            # æˆ‘ä»¬å¯ä»¥ join å›å»ï¼Œä½† Treemap hover ä¹Ÿå¯ä»¥åªå±•ç¤ºæ¶¨è·Œ
            daily_df['æ”¶ç›˜'] = 0 # Placeholder
            
            metric_label_date = "å½“å‰å›æ”¾åŒºé—´"
            metric_label_chg = "åŒºé—´æ¶¨è·Œå¹…ä¸­ä½æ•°"
            metric_label_to = "åŒºé—´æ€»æˆäº¤"

        # æ˜¾ç¤ºæŒ‡æ ‡è¡Œ
        col1, col2, col3, col4 = st.columns(4)
        col1.metric(metric_label_date, display_date_str)
        col2.metric(metric_label_chg, f"{median_chg:.2f}%", 
                    delta=f"{median_chg:.2f}%", delta_color="normal")
        col3.metric(metric_label_to, f"{total_turnover:.1f} äº¿")
        col4.metric("é¢†æ¶¨é¾™å¤´", f"{top_gainer['åç§°']} ({'æ¶¨è·Œå¹…' in top_gainer and top_gainer['æ¶¨è·Œå¹…'] or top_gainer.get('åŒºé—´æ¶¨è·Œå¹…'):.2f}%)")

        # --- æ–°å¢åŠŸèƒ½ï¼šåˆ†æ—¶èµ°åŠ¿å åŠ  ---
        st.markdown("---")
        st.subheader("ğŸ“ˆ æ ¸å¿ƒèµ„äº§åˆ†æ—¶èµ°åŠ¿å åŠ ")
        
        # æ¨¡å¼é€‰æ‹©
        col_mode, col_num = st.columns([3, 1])
        with col_mode:
            chart_mode = st.radio("é€‰è‚¡æ¨¡å¼", ["æˆäº¤é¢ Top (æ´»è·ƒåº¦)", "æŒ‡æ•°è´¡çŒ® Top (å½±å“å¤§ç›˜)"], horizontal=True)
        with col_num:
            # æ·»åŠ  key é¿å… Bad setIn index é”™è¯¯ï¼Œå¹¶å¼ºåˆ¶é‡ç½®çŠ¶æ€
            top_n = st.number_input("æ ‡çš„æ•°é‡", min_value=5, max_value=50, value=20, step=5, 
                                   help="æ²ª/æ·±å„å– N ä¸ªæ ‡çš„ï¼ˆå³æ€»æ•°ä¸º 2Nï¼‰", key="top_n_stocks_input")

        st.caption(f"æ³¨ï¼šè¿™é‡Œçš„æ’åæ˜¯åŸºäº **{selected_date}** å½“æ—¥çš„æ•°æ®è®¡ç®—çš„ã€‚å¦‚æœæ˜¯å¤šæ—¥æ¨¡å¼ï¼Œåˆ™å±•ç¤ºè¿™äº›è‚¡ç¥¨åœ¨è¿‡å»å‡ å¤©çš„èµ°åŠ¿ã€‚")
        st.caption("æ³¨ï¼šæŒ‡æ•°è´¡çŒ® = æ¶¨è·Œå¹… Ã— æƒé‡(è¿‘ä¼¼ä¸ºæˆäº¤é¢/å¸‚å€¼å æ¯”)ã€‚æ­¤æ¨¡å¼èƒ½çœ‹åˆ°æ˜¯è°åœ¨æ‹‰åŠ¨æˆ–ç ¸ç›˜ã€‚")

        show_intraday = st.checkbox("åŠ è½½åˆ†æ—¶èµ°åŠ¿ (éœ€ä»ç½‘ç»œå®æ—¶æ‹‰å–)", value=False)
        
        if show_intraday:
            # ä½¿ç”¨ placeholder æ”¾ç½®è¿›åº¦æ¡ï¼Œé¿å…ç»„ä»¶é”€æ¯å¯¼è‡´çš„ç´¢å¼•é”™ä¹±
            progress_area = st.empty()
            
            # ç»Ÿä¸€é€‰è‚¡é€»è¾‘ï¼šæ— è®ºæ˜¯æˆäº¤é¢è¿˜æ˜¯æŒ‡æ•°è´¡çŒ®ï¼Œéƒ½æŒ‰æ²ªæ·±åˆ†åˆ«å– Top N
            if "æˆäº¤é¢" in chart_mode:
                sort_col = 'æˆäº¤é¢'
            else:
                daily_df['abs_impact'] = (daily_df['æ¶¨è·Œå¹…'] * daily_df['æˆäº¤é¢']).abs()
                sort_col = 'abs_impact'
                
            sh_pool = daily_df[daily_df['ä»£ç '].astype(str).str.startswith('6')].copy()
            sz_pool = daily_df[~daily_df['ä»£ç '].astype(str).str.startswith('6')].copy()
            
            sh_top = sh_pool.sort_values(sort_col, ascending=False).head(top_n)
            sz_top = sz_pool.sort_values(sort_col, ascending=False).head(top_n)
            
            top_stocks_df = pd.concat([sh_top, sz_top], ignore_index=True)

            target_stocks_list = []
            for _, row in top_stocks_df.iterrows():
                target_stocks_list.append((row['ä»£ç '], row['åç§°'], row['æˆäº¤é¢'])) 
            
            all_intraday_data = [] 
            
            period_to_use = '1'
            
            if len(target_dates) > 5 and playback_mode == "å¤šæ—¥èµ°åŠ¿æ‹¼æ¥":
                if len(target_dates) > 30:
                    period_to_use = '15' # è¶…è¿‡30å¤©ä½¿ç”¨15åˆ†é’Ÿçº¿
                    st.info(f"â„¹ï¸ æ‚¨é€‰æ‹©äº† {len(target_dates)} å¤©ï¼šç³»ç»Ÿè‡ªåŠ¨åˆ‡æ¢è‡³ã€15åˆ†é’Ÿçº§ã€‘æ•°æ®ã€‚")
                else:
                    period_to_use = '5'
                    st.info(f"â„¹ï¸ æ‚¨é€‰æ‹©äº† {len(target_dates)} å¤©ï¼šç³»ç»Ÿè‡ªåŠ¨åˆ‡æ¢è‡³ã€5åˆ†é’Ÿçº§ã€‘æ•°æ®ã€‚")
            elif len(target_dates) > 10:
                 st.toast(f"âš ï¸ æ‚¨é€‰æ‹©äº† {len(target_dates)} å¤©çš„æ•°æ®ï¼ŒåŠ è½½å¯èƒ½è¾ƒæ…¢ï¼Œè¯·è€å¿ƒç­‰å¾…...")
            
            target_dates_to_fetch = target_dates
            total_steps = len(target_dates_to_fetch)

            # æ”¹å›æ‰å¹³åŒ–ç»“æ„ï¼Œä¸å†ä½¿ç”¨ containerï¼Œå‡å°‘ DOM æ“ä½œå±‚çº§
            # å¹¶å‘çº¿ç¨‹ä¸­ç¼“å­˜çš„ show_spinner=False å·²ç»è®¾ç½®ï¼Œè¿™é‡Œåº”è¯¥å®‰å…¨äº†
            status_text = st.empty()
            fetch_progress = st.progress(0)
                 
            for i, d_date in enumerate(target_dates_to_fetch):
                status_text.text(f"ğŸ”„ æ­£åœ¨è·å–: {d_date.strftime('%Y-%m-%d')} ({i+1}/{total_steps})...")
                fetch_progress.progress((i + 1) / total_steps)
                
                d_str = d_date.strftime("%Y-%m-%d")
                day_results = fetch_intraday_data_v2(target_stocks_list, d_str, period=period_to_use)
                
                for res in day_results:
                        res['data']['date_col'] = d_str
                        res['real_date'] = d_date
                
                all_intraday_data.extend(day_results)
            
            # æ•°æ®æ‹‰å–å®Œæ¯•åï¼Œæ¸…é™¤è¿›åº¦ç»„ä»¶
            status_text.empty()
            fetch_progress.empty()
            # ç§»é™¤å¤–å±‚å ä½ç¬¦çš„æ¸…ç†ï¼Œå› ä¸ºå·²ç»ä¸å†ä½¿ç”¨
            progress_area.empty()
                
            if not all_intraday_data:
                st.warning("æœªèƒ½è·å–åˆ°åˆ†æ—¶æ•°æ®")
            else:
                    # --- æ•°æ®é‡ç»„ ---
                    # å°†åˆ†æ•£çš„æ•°æ®åˆå¹¶ä¸ºï¼š { 'code': { 'name':..., 'is_index':..., 'full_data': DataFrame } }
                    combined_series = {}
                    
                    for item in all_intraday_data:
                        code = item['code']
                        if code not in combined_series:
                            # æŸ¥æ‰¾è¯¥è‚¡ç¥¨åœ¨é€‰å®šæ—¥(æœ€åä¸€æ—¥)çš„æˆäº¤é¢ï¼Œç”¨äºå®šçº¿å®½
                            to_val = 0
                            if not item.get('is_index'):
                                matches = daily_df[daily_df['ä»£ç '] == code]
                                if not matches.empty:
                                    to_val = matches.iloc[0]['æˆäº¤é¢']
                            
                            combined_series[code] = {
                                'name': item['name'],
                                'code': code,
                                'is_index': item.get('is_index', False),
                                'turnover': to_val, # ä½¿ç”¨æœ€åä¸€å¤©çš„æˆäº¤é¢
                                'dfs': []
                            }
                        combined_series[code]['dfs'].append(item['data'])
                    
                    # åˆå¹¶ DataFrame å¹¶æ„å»ºè¿ç»­æ—¶é—´è½´
                    # ç­–ç•¥ï¼šä¸ä½¿ç”¨çœŸå®æ—¶é—´è½´ï¼Œè€Œæ˜¯ä½¿ç”¨ "äº¤æ˜“åˆ†é’Ÿåºåˆ—" (Trading Minute Index)
                    # æ¯å¤© 240 åˆ†é’Ÿã€‚Day 1: 0-239, Day 2: 240-479...
                    # éœ€è¦ç”Ÿæˆä¸€ä¸ª Xè½´ Label Map
                    
                    final_plot_data = [] # List of {idx, pct_chg, ...}
                    x_tick_vals = []
                    x_tick_text = []
                    
                    # ä¸ºæ¯ä¸€å¤©ç”Ÿæˆæ ‡å‡†æ—¶é—´åºåˆ— (é¿å…ç¼ºå¤±åˆ†é’Ÿå¯¼è‡´çš„é”™ä½)
                    # 09:30 - 11:30 (121 points usually inclusive? Aè‚¡åˆ†é’Ÿçº¿é€šå¸¸åŒ…å« 11:30 å’Œ 15:00)
                    # åˆ†é’Ÿçº¿é€šå¸¸æ˜¯ 09:31 - 11:30 (120 mins) 13:01 - 15:00 (120 mins). 
                    # AkShare è¿”å›çš„æ•°æ®æ—¶é—´å¦‚æœæ˜¯ 09:30 é€šå¸¸ä»£è¡¨å¼€ç›˜?
                    # è®©æˆ‘ä»¬é€šè¿‡è§‚å¯Ÿç¬¬ä¸€æ¡æ•°æ®æ¥å†³å®šé€»è¾‘ï¼Œé€šå¸¸ç›´æ¥ concat å³å¯
                    # ä¸ºäº†è§£å†³ GAPï¼Œæˆ‘ä»¬åœ¨ UI ç»˜å›¾å±‚å¼ºåˆ¶æŠŠ x æ˜ å°„ä¸º 0, 1, 2...
                    
                    # æå–çœŸæ­£è·å–åˆ°æ•°æ®çš„æ—¥æœŸåˆ—è¡¨ (å»é™¤èŠ‚å‡æ—¥/æ— æ•°æ®æ—¥)
                    valid_dates = set()
                    for item in all_intraday_data:
                         if 'real_date' in item:
                             valid_dates.add(item['real_date'].strftime("%Y-%m-%d"))
                    
                    days_list = sorted(list(valid_dates))
                    
                    if not days_list:
                        # å¦‚æœè¿‡æ»¤åå±…ç„¶æ²¡äº†ï¼ˆç†è®ºä¸Šå¤–å±‚ checked not emptyï¼‰ï¼Œå›é€€å…œåº•
                         days_list = sorted(list(set([x.strftime("%Y-%m-%d") for x in target_dates_to_fetch])))
                    
                    # æ„å»ºåŸºå‡†æ—¶é—´ç½‘æ ¼ (Template) - æ¯å¤© 240/241 ä¸ªç‚¹
                    # 09:30 - 11:30, 13:00 - 15:00
                    dummy_date = "2000-01-01"
                    morning_range = pd.date_range(f"{dummy_date} 09:30", f"{dummy_date} 11:30", freq="1min")
                    afternoon_range = pd.date_range(f"{dummy_date} 13:00", f"{dummy_date} 15:00", freq="1min")
                    daily_time_template = morning_range.union(afternoon_range) # Size approx 242
                    
                    # è®¡ç®—æ€»åç§»é‡
                    points_per_day = len(daily_time_template)
                    
                    for code, info in combined_series.items():
                        # åˆå¹¶ã€æ’åº
                        full_df = pd.concat(info['dfs']).sort_values(['date_col', 'time'])
                        
                        # é‡æ–°æ„é€  X è½´ (Int)
                        # ç®—æ³•ï¼šå¯¹äºæ¯ä¸€è¡Œï¼Œæ‰¾åˆ°å®ƒæ˜¯ ç¬¬å‡ å¤© çš„ ç¬¬å‡ åˆ†é’Ÿ
                        # x_int = day_index * points_per_day + minute_index_in_day
                        
                        full_df['time_str'] = full_df['time'].dt.strftime("%H:%M:%S")
                        
                        x_values = []
                        
                        for idx, row in full_df.iterrows():
                            d_str = row['date_col']
                            t_str = row['time_str'] # å®Œæ•´æ—¶é—´å¯¹è±¡
                            
                            # ç¡®å®šæ˜¯ç¬¬å‡ å¤©
                            day_idx = days_list.index(d_str) if d_str in days_list else 0
                            
                            # ç¡®å®šæ˜¯å½“å¤©çš„ç¬¬å‡ åˆ†é’Ÿ
                            # ç®€å•è½¬æ¢ï¼šå°æ—¶*60 + åˆ†é’Ÿ
                            t_obj = row['time'] # timestamp
                            mins_from_midnight = t_obj.hour * 60 + t_obj.minute
                            
                            # æŠŠä¸­åˆä¼‘å¸‚çš„æ—¶é—´å‹æ‰
                            # 9:30 (570) -> 11:30 (690). Length 120.
                            # 13:00 (780) -> 15:00 (900). 
                            
                            if mins_from_midnight <= 690: # Morning
                                offset = mins_from_midnight - 570 # 09:30 is 0
                            else: # Afternoon
                                offset = 120 + (mins_from_midnight - 780) # 13:00 starts at 120+
                                
                            final_x = day_idx * (240 + 20) + offset # åŠ 20ä¸ªå•ä½çš„é—´éš”è®©å¤©ä¸å¤©ä¹‹é—´æœ‰ç‚¹ç©ºéš™
                            x_values.append(final_x)
                            
                        full_df['x_int'] = x_values
                        
                        # è®¡ç®—ç´¯è®¡æ¶¨è·Œå¹… (å¦‚æœæ˜¯å¤šæ—¥ï¼Œéœ€è¦é“¾å¼è®¡ç®—ï¼Ÿ)
                        # ç®€å•æ–¹æ¡ˆï¼šæ¯å¤©é‡æ–°å½’é›¶ï¼Ÿè¿˜æ˜¯å¤šæ—¥è¿è´¯ï¼Ÿ
                        # ç”¨æˆ·è¯´ "æ‹¼èµ·æ¥å½¢æˆå®Œæ•´çš„å›¾"ï¼Œé€šå¸¸æ„å‘³ç€è¿è´¯è¶‹åŠ¿ã€‚
                        # ä»¥ç¬¬ä¸€å¤©å¼€ç›˜ä»·ä¸ºåŸºå‡†
                        base_price = full_df['close'].iloc[0]
                        # å¦‚æœä¸­é—´æœ‰æ–­ç‚¹ï¼Œç®€å•çš„ (close - base) / base å¯èƒ½å¤±çœŸï¼ˆå› ä¸ºæ˜¨æ”¶...ï¼‰
                        # å‡†ç¡®åšæ³•ï¼šç´¯ä¹˜æ¯å¤©çš„æ¶¨è·Œå¹…ã€‚
                        # ä½†è¿™é‡Œåªè¦å¤§æ¦‚è¶‹åŠ¿ã€‚å¦‚æœç”¨ (Px - P0) / P0ï¼Œé‚£è·¨æ—¥ç¼ºå£ä¼šä½“ç°ä¸ºç›´çº¿çš„è·³è·ƒã€‚
                        # è¿™ç¬¦åˆ "çœŸå®ä»·æ ¼èµ°åŠ¿"ã€‚
                        
                        full_df['cumulative_pct'] = (full_df['close'] - base_price) / base_price * 100
                        
                        info['plot_data'] = full_df
                    
                    # ç”Ÿæˆ X è½´æ ‡ç­¾
                    for i, d_str in enumerate(days_list):
                        base_x = i * (240 + 20)
                        day_label = d_str[5:] # MM-DD
                        
                        if len(days_list) > 1:
                            # å¤šæ—¥æ¨¡å¼ï¼šåªæ˜¾ç¤ºæ—¥æœŸåœ¨ä¸­é—´ æˆ–è€… å¼€å¤´
                            # ä¸ºäº†ç®€æ´ï¼Œåªåœ¨æ¯å¤©çš„ä¸­é—´æ˜¾ç¤ºä¸€ä¸ªæ—¥æœŸ
                            x_tick_vals.append(base_x + 120) 
                            x_tick_text.append(day_label)
                        else:
                            # å•æ—¥æ¨¡å¼ï¼šæ˜¾ç¤ºè¯¦ç»†æ—¶é—´ç‚¹
                            # 09:30
                            x_tick_vals.append(base_x)
                            x_tick_text.append(f"{day_label}\n09:30")
                            # 11:30
                            x_tick_vals.append(base_x + 120)
                            x_tick_text.append("11:30/13:00")
                            # 15:00
                            x_tick_vals.append(base_x + 240)
                            x_tick_text.append("15:00")

                    # åˆ†ç±»
                    idx_data = [v for k,v in combined_series.items() if v['is_index']]
                    stk_data = [v for k,v in combined_series.items() if not v['is_index']]
                    
                    sh_stocks = [v for v in stk_data if v['code'].startswith('6')]
                    sz_stocks = [v for v in stk_data if not v['code'].startswith('6')]
                    
                    sh_index = [v for v in idx_data if v['code'] in ['000001', '000300']]
                    sz_index = [v for v in idx_data if v['code'] in ['399001', '000300']]

                    # ç»˜å›¾å‡½æ•° v3
                    def plot_intraday_v3(stocks, indices, title_suffix):
                        fig = go.Figure()
                        
                        # ä¸ªè‚¡
                        if stocks:
                            max_to = max([s['turnover'] for s in stocks]) if stocks else 1
                            min_to = min([s['turnover'] for s in stocks]) if stocks else 0
                            
                            # ç”Ÿæˆ distinct colors
                            color_palette = px.colors.qualitative.Alphabet + px.colors.qualitative.Dark24
                            
                            for i, s in enumerate(stocks):
                                if max_to == min_to: width=2
                                else: width = 1 + 3*(s['turnover'] - min_to)/(max_to - min_to)
                                
                                df_p = s['plot_data']
                                last_val = df_p['cumulative_pct'].iloc[-1]
                                
                                # ä¹‹å‰çš„çº¢ç»¿é€»è¾‘: color = 'rgba(214, 39, 40, 0.4)' if last_val > 0 else 'rgba(44, 160, 44, 0.4)'
                                # ç°åœ¨æ”¹ä¸ºåŒºåˆ†é¢œè‰²
                                color = color_palette[i % len(color_palette)]
                                
                                fig.add_trace(go.Scatter(
                                    x=df_p['x_int'],
                                    y=df_p['cumulative_pct'],
                                    mode='lines',
                                    name=s['name'],
                                    # line=dict(width=width, color=color),
                                    # ä¸ªè‚¡çº¿å®½ä¸éœ€è¦å¤ªç²—ï¼Œé¢œè‰²è¦æ¸…æ™°
                                    line=dict(width=max(1.5, width), color=color),
                                    hovertemplate=f"<b>{s['name']}</b><br>æ¶¨è·Œ: %{{y:.2f}}%<br>æ—¶é—´: %{{customdata}}",
                                    customdata=df_p['date_col'] + ' ' + df_p['time_str']
                                ))
                                
                        # æŒ‡æ•°
                        idx_colors = {'000300': 'black', '000001': '#d62728', '399001': '#1f77b4'}
                        for idx in indices:
                            df_p = idx['plot_data']
                            c_code = idx.get('code', '000300')
                            
                            fig.add_trace(go.Scatter(
                                x=df_p['x_int'],
                                y=df_p['cumulative_pct'],
                                mode='lines',
                                name=idx['name'],
                                line=dict(width=3, color=idx_colors.get(c_code, 'black')),
                                hovertemplate=f"<b>{idx['name']}</b><br>æ¶¨è·Œ: %{{y:.2f}}%"
                            ))

                        # 3. åˆ†å‰²çº¿ (å¦‚æœæ˜¯å¤šæ—¥)
                        if len(days_list) > 1:
                            for i in range(1, len(days_list)):
                                # åœ¨æ¯ä¸€å¤©å¼€å§‹å‰ç”»ç«–çº¿
                                boundary = i * (240 + 20) - 10
                                fig.add_vline(x=boundary, line_width=1, line_dash="dash", line_color="gray")

                        fig.update_layout(
                            title=f"åˆ†æ—¶èµ°åŠ¿å åŠ  ({'å¤šæ—¥æ‹¼æ¥' if len(days_list)>1 else days_list[0]}) - {title_suffix}",
                            xaxis=dict(
                                tickmode='array',
                                tickvals=x_tick_vals,
                                ticktext=x_tick_text,
                                showgrid=True,
                                showspikes=True, # æ˜¾ç¤ºå‚ç›´è¾…åŠ©çº¿
                                spikemode='across',
                                spikesnap='cursor',
                                showline=True, 
                                linewidth=1, 
                                linecolor='black',
                                mirror=True
                            ),
                            yaxis=dict(
                                showspikes=True # Yè½´ä¹Ÿæ˜¾ç¤ºè¾…åŠ©çº¿ï¼Œæ–¹ä¾¿çœ‹ç‚¹ä½
                            ),
                            yaxis_title="ç´¯è®¡æ¶¨è·Œå¹… (%)",
                            hovermode="x unified", # å¼€å¯ç»Ÿä¸€ Hoverï¼Œæ˜¾ç¤ºè¯¥æ—¶é—´ç‚¹æ‰€æœ‰æ•°æ®
                            height=700, # ç¨å¾®è°ƒé«˜é«˜åº¦ä»¥å®¹çº³æ›´å¤š Hover ä¿¡æ¯
                            legend=dict(orientation="h", y=1.02, x=1, xanchor='right') # ä¿æŒå›¾ä¾‹çš„å¸ƒå±€
                        )
                        
                        # âš ï¸ å…³é”®ä¿®æ­£ï¼šç¡®ä¿ Hover Tooltip çš„æ’åºæŒ‰ç…§ Y è½´æ•°å€¼ (ä»é«˜åˆ°ä½)
                        # "closest" æ¨¡å¼é…åˆ "compare" å¯èƒ½æ— æ³•ç”Ÿæ•ˆï¼Œä½†åœ¨ "x unified" æ¨¡å¼ä¸‹ï¼Œ
                        # é»˜è®¤æ˜¯æŒ‰ç…§ Trace æ·»åŠ é¡ºåºæ’åºçš„ã€‚
                        # Plotly (JSå±‚) åœ¨ x unified ä¸‹æœ‰é»˜è®¤æ’åºé€»è¾‘ (é€šå¸¸æ˜¯ value descending)ï¼Œä½†åœ¨æŸäº›ç‰ˆæœ¬å¯èƒ½ä¸ç¨³å®šã€‚
                        # ä¸ºäº†å¢å¼ºæ’åºä½“éªŒï¼Œæˆ‘ä»¬å¯ä»¥å°è¯•è®¾ç½® layout.hoverlabel.namelength = -1
                        
                        fig.update_layout(hoverlabel=dict(namelength=-1))
                        
                        return fig

                    tab1, tab2 = st.tabs(["æ²ªå¸‚ (SH)", "æ·±å¸‚ (SZ)"])
                    
                    with tab1:
                        st.plotly_chart(plot_intraday_v3(sh_stocks, sh_index, f"æ²ªå¸‚ - {chart_mode}"), use_container_width=True)
                    with tab2:
                        st.plotly_chart(plot_intraday_v3(sz_stocks, sz_index, f"æ·±å¸‚ - {chart_mode}"), use_container_width=True)
        
        # --- å¯è§†åŒ– ---
        st.subheader(f"ğŸ“Š {selected_date.strftime('%Yå¹´%mæœˆ%dæ—¥')} å¸‚åœºå…¨æ™¯çƒ­åŠ›å›¾")
        
        # Aè‚¡ä¸“ç”¨è‰²è°±
        max_limit = 7
        min_limit = -7
        
        fig = px.treemap(
            daily_df,
            path=['åç§°'],
            values='æˆäº¤é¢', # ç”¨æˆäº¤é¢ä»£è¡¨çƒ­åº¦/æƒé‡ (å› ä¸ºå†å²å¸‚å€¼éš¾è·å–)
            color='æ¶¨è·Œå¹…',
            color_continuous_scale=['#00a65a', '#ffffff', '#dd4b39'], # ç»¿ -> ç™½ -> çº¢
            range_color=[min_limit, max_limit],
            hover_data={
                'åç§°': True,
                'ä»£ç ': True,
                'æ”¶ç›˜': True,
                'æ¶¨è·Œå¹…': ':.2f',
                'æˆäº¤é¢': True
            },
            height=650
        )
        
        # ä¼˜åŒ–æ˜¾ç¤º
        fig.update_traces(
            textinfo="label+value+percent entry",
            hovertemplate="<b>%{label}</b><br>æ”¶ç›˜ä»·: %{customdata[2]}<br>æ¶¨è·Œå¹…: %{color:.2f}%<br>æˆäº¤é¢: %{value:.2s}"
        )
        fig.update_layout(
            margin=dict(t=10, l=10, r=10, b=10),
            coloraxis_colorbar=dict(title="æ¶¨è·Œå¹…(%)")
        )
        
        st.plotly_chart(fig, use_container_width=True)
        
        # å¯é€‰ï¼šæ˜¾ç¤ºè¯¦ç»†æ•°æ®è¡¨
        with st.expander("æŸ¥çœ‹å½“æ—¥è¯¦ç»†æ•°æ®"):
            st.dataframe(
                daily_df[['ä»£ç ', 'åç§°', 'æ”¶ç›˜', 'æ¶¨è·Œå¹…', 'æˆäº¤é¢']].style.format({
                    'æ”¶ç›˜': '{:.2f}',
                    'æ¶¨è·Œå¹…': '{:.2f}%',
                    'æˆäº¤é¢': '{:,.0f}'
                }),
                hide_index=True
            )

    elif nav_option == "ğŸŒŠ èµ„é‡‘åç¦»åˆ†æ":
        st.subheader("ğŸŒŠ èµ„é‡‘åç¦»åº¦åˆ†æ (Alpha Divergence)")
        st.info("ğŸ’¡ **é€»è¾‘è¯´æ˜**ï¼šè®¡ç®—é€‰å®šå‘¨æœŸå†…æ¯åªè‚¡ç¥¨ç›¸å¯¹äºã€å¸‚åœºä¸­ä½æ•°ã€‘çš„è¶…é¢æ¶¨è·Œå¹…ï¼ˆåç¦»åº¦ï¼‰ã€‚\n\nå¦‚æœæŸåªè‚¡ç¥¨ **æˆäº¤é¢å·¨å¤§** ä¸” **å‘ä¸‹åç¦»æå¤§**ï¼Œé€šå¸¸æ„å‘³ç€ä¸»åŠ›èµ„é‡‘åœ¨å¤§ä¸¾å‡ºè´§ï¼›åä¹‹åˆ™æ˜¯ä¸»åŠ›æŠ¢ç­¹ã€‚")

        # 1. å‘¨æœŸé€‰æ‹© (Reuse simplified logic)
        available_dates = sorted(df['æ—¥æœŸ'].dt.date.unique())
        col_d1, col_d2 = st.columns(2)
        with col_d1:
            date_range_div = st.date_input(
                "åˆ†æå‘¨æœŸ",
                value=[available_dates[-5] if len(available_dates)>5 else available_dates[0], available_dates[-1]],
                min_value=available_dates[0],
                max_value=available_dates[-1],
                key="divergence_date_input"
            )
        
        target_dates_div = []
        if len(date_range_div) == 2:
            s_d, e_d = date_range_div
            target_dates_div = [d for d in available_dates if s_d <= d <= e_d]
        
        if not target_dates_div:
            st.warning("è¯·é€‰æ‹©æœ‰æ•ˆçš„æ—¶é—´èŒƒå›´")
            st.stop()
            
        st.caption(f"å·²é€‰å– {target_dates_div[0]} è‡³ {target_dates_div[-1]}ï¼Œå…± {len(target_dates_div)} ä¸ªäº¤æ˜“æ—¥ã€‚")
        
        # 2. è®¡ç®—åŒºé—´æ•°æ®
        start_date_ts = pd.Timestamp(target_dates_div[0])
        end_date_ts = pd.Timestamp(target_dates_div[-1])
        
        div_period_df = df[(df['æ—¥æœŸ'] >= start_date_ts) & (df['æ—¥æœŸ'] <= end_date_ts)].copy()
        
        # èšåˆ
        div_stats = []
        grouped = div_period_df.groupby('ä»£ç ')
        
        for code, group in grouped:
            group = group.sort_values('æ—¥æœŸ')
            if group.empty: continue
            
            first_row = group.iloc[0]
            last_row = group.iloc[-1]
            
            try:
                # ä¼°ç®—åŒºé—´æ¶¨å¹…
                s_open = first_row['æ”¶ç›˜'] / (1 + first_row['æ¶¨è·Œå¹…']/100)
                e_close = last_row['æ”¶ç›˜']
                cum_pct = (e_close - s_open) / s_open * 100
                total_to = group['æˆäº¤é¢'].sum()
                
                div_stats.append({
                    'ä»£ç ': code,
                    'åç§°': first_row['åç§°'],
                    'åŒºé—´æ¶¨è·Œå¹…': cum_pct,
                    'åŒºé—´æ€»æˆäº¤': total_to
                })
            except:
                pass
                
        div_df = pd.DataFrame(div_stats)
        if div_df.empty:
            st.stop()
            
        # 3. è®¡ç®—åç¦»åº¦ (Deviation)
        market_median_chg = div_df['åŒºé—´æ¶¨è·Œå¹…'].median()
        div_df['åç¦»åº¦'] = div_df['åŒºé—´æ¶¨è·Œå¹…'] - market_median_chg
        
        # è¾…åŠ©åˆ—
        div_df['æˆäº¤é¢(äº¿)'] = div_df['åŒºé—´æ€»æˆäº¤'] / 1e8
        
        col_m1, col_m2 = st.columns(2)
        col_m1.metric("åŸºå‡†(ä¸­ä½æ•°)æ¶¨è·Œå¹…", f"{market_median_chg:.2f}%")
        col_m2.metric("åˆ†ææ ·æœ¬æ•°", f"{len(div_df)} åª")
        
        st.divider()

        # 4. å¯è§†åŒ– - æ•£ç‚¹å›¾
        # X: æˆäº¤é¢(Log), Y: åç¦»åº¦, Color: åç¦»åº¦
        fig_scatter = px.scatter(
            div_df,
            x='æˆäº¤é¢(äº¿)',
            y='åç¦»åº¦',
            color='åç¦»åº¦',
            text='åç§°', # æ˜¾ç¤ºåå­—
            color_continuous_scale=['#00a65a', '#ffffff', '#dd4b39'],
            range_color=[-20, 20], # é™åˆ¶é¢œè‰²èŒƒå›´é¿å…æå€¼
            log_x=True,
            hover_data=['ä»£ç ', 'åŒºé—´æ¶¨è·Œå¹…'],
            title=f"èµ„é‡‘åç¦»åº¦åˆ†å¸ƒå›¾ (Xè½´ä¸ºæˆäº¤é¢å¯¹æ•°)"
        )
        fig_scatter.update_traces(textposition='top center')
        fig_scatter.update_layout(height=600)
        st.plotly_chart(fig_scatter, use_container_width=True)
        
        # 5. æ¦œå•
        col_list1, col_list2 = st.columns(2)
        
        with col_list1:
            st.subheader("ğŸ”¥ èµ„é‡‘æŠ±å›¢ (æ”¾é‡å‘ä¸Šåç¦»)")
            # é€»è¾‘ï¼šæˆäº¤é¢å¤§ & åç¦»åº¦ > 0
            # æ’åºï¼šç»¼åˆåˆ† = æˆäº¤é¢ * åç¦»åº¦ (ä»…å‚è€ƒ) æˆ–è€…æŒ‰æˆäº¤é¢é™åºçœ‹è°åœ¨æ¶¨
            # ç”¨æˆ·éœ€æ±‚ï¼šæ‰¾å‡ºå‘ä¸Šåç¦»çš„ã€‚é€šå¸¸æƒ³çœ‹â€œå¤§èµ„é‡‘ä¹°è°â€ã€‚æ‰€ä»¥æŒ‰æˆäº¤é¢é™åºï¼Œä¸”åç¦»åº¦>0
            
            buy_df = div_df[div_df['åç¦»åº¦'] > 0].sort_values('åŒºé—´æ€»æˆäº¤', ascending=False).head(20)
            st.dataframe(
                buy_df[['ä»£ç ', 'åç§°', 'åç¦»åº¦', 'æˆäº¤é¢(äº¿)', 'åŒºé—´æ¶¨è·Œå¹…']].style.format({
                    'åç¦»åº¦': '+{:.2f}%',
                    'æˆäº¤é¢(äº¿)': '{:.1f}',
                    'åŒºé—´æ¶¨è·Œå¹…': '{:.2f}%'
                }),
                hide_index=True
            )
            
        with col_list2:
            st.subheader("ğŸ“‰ èµ„é‡‘å‡ºé€ƒ (æ”¾é‡å‘ä¸‹åç¦»)")
            # é€»è¾‘ï¼šæˆäº¤é¢å¤§ & åç¦»åº¦ < 0
            sell_df = div_df[div_df['åç¦»åº¦'] < 0].sort_values('åŒºé—´æ€»æˆäº¤', ascending=False).head(20)
            
            st.dataframe(
                sell_df[['ä»£ç ', 'åç§°', 'åç¦»åº¦', 'æˆäº¤é¢(äº¿)', 'åŒºé—´æ¶¨è·Œå¹…']].style.format({
                    'åç¦»åº¦': '{:.2f}%',
                    'æˆäº¤é¢(äº¿)': '{:.1f}',
                    'åŒºé—´æ¶¨è·Œå¹…': '{:.2f}%'
                }),
                hide_index=True
            )

else:
    st.error("æ•°æ®åŠ è½½å¤±è´¥ï¼Œè¯·åˆ·æ–°é‡è¯•ã€‚")
