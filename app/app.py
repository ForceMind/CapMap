import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
import akshare as ak
from datetime import datetime, timedelta
import time
import random
import os
import concurrent.futures
import threading
import sys
import io
import zipfile
import shutil
import json
import logging
import html

# å°è¯•å¯¼å…¥ Streamlit ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œç”¨äºè§£å†³å¤šçº¿ç¨‹ "missing ScriptRunContext" è­¦å‘Š
try:
    from streamlit.runtime.scriptrunner import add_script_run_ctx, get_script_run_ctx
except ImportError:
    # å…¼å®¹æ—§ç‰ˆæœ¬ Streamlit
    from streamlit.scriptrunner import add_script_run_ctx, get_script_run_ctx

# é…ç½®é¡µé¢ä¿¡æ¯
st.set_page_config(
    page_title="Aè‚¡å†å²ç›˜é¢å›æ”¾ç³»ç»Ÿ",
    page_icon="âª",
    layout="wide"
)

# -----------------------------------------------------------------------------
# 1. æ ¸å¿ƒæ•°æ®é€»è¾‘
# -----------------------------------------------------------------------------

CACHE_FILE = "data/csi300_history_cache.parquet"
MIN_CACHE_DIR = "data/min_cache"
NAME_MAP_FILE = "data/name_map.json"
NAME_REFRESH_FILE = "data/name_refresh.json"
NAME_REFRESH_TTL_HOURS = 24 * 180
NAME_REFRESH_MIN_INTERVAL_MINUTES = 30
NAME_MAP_VERSION = 1
APP_LOG_FILE = "logs/app.log"
INTRADAY_WORKERS = int(os.environ.get("INTRADAY_WORKERS", "1"))
INTRADAY_DELAY_SEC = float(os.environ.get("INTRADAY_DELAY_SEC", "0.5"))
AUTO_PREFETCH_ENABLED = os.environ.get("AUTO_PREFETCH_ENABLED", "1") == "1"
AUTO_PREFETCH_TIME = os.environ.get("AUTO_PREFETCH_TIME", "15:15")
AUTO_PREFETCH_DELAY_SEC = float(os.environ.get("AUTO_PREFETCH_DELAY_SEC", "10"))
AUTO_PREFETCH_RETRY_SLEEP_SEC = float(os.environ.get("AUTO_PREFETCH_RETRY_SLEEP_SEC", "300"))
AUTO_PREFETCH_MAX_RETRIES = int(os.environ.get("AUTO_PREFETCH_MAX_RETRIES", "0"))
AUTO_PREFETCH_STATE_FILE = "data/auto_prefetch_state.json"

def _init_logging():
    log_path = os.path.abspath(APP_LOG_FILE)
    os.makedirs(os.path.dirname(log_path), exist_ok=True)
    formatter = logging.Formatter("%(asctime)s | %(levelname)s | %(name)s | %(message)s")
    logger = logging.getLogger("capmap")
    logger.setLevel(logging.INFO)
    file_handler = None
    for handler in logger.handlers:
        if isinstance(handler, logging.FileHandler) and os.path.abspath(getattr(handler, "baseFilename", "")) == log_path:
            file_handler = handler
            break
    if file_handler is None:
        file_handler = logging.FileHandler(log_path, encoding="utf-8")
        file_handler.setFormatter(formatter)
        logger.addHandler(file_handler)
    logger.propagate = False

    logging.captureWarnings(True)
    for name, level in (("akshare", logging.INFO), ("py.warnings", logging.WARNING)):
        other = logging.getLogger(name)
        other.setLevel(level)
        if not any(isinstance(h, logging.FileHandler) and os.path.abspath(getattr(h, "baseFilename", "")) == log_path for h in other.handlers):
            other.addHandler(file_handler)
        other.propagate = False
    return logger

logger = _init_logging()

def _fmt_kv(kwargs):
    parts = []
    for k, v in kwargs.items():
        try:
            parts.append(f"{k}={v}")
        except Exception:
            parts.append(f"{k}=?")
    return " ".join(parts)

def log_action(action, **kwargs):
    # ä»…ç”¨äºè°ƒè¯•å‰ç«¯æ“ä½œï¼Œé»˜è®¤ä¸è¾“å‡ºåˆ° INFO çº§åˆ«æ—¥å¿—
    if kwargs:
        logger.debug("å‰ç«¯æ“ä½œ: %s | %s", action, _fmt_kv(kwargs))
    else:
        logger.debug("å‰ç«¯æ“ä½œ: %s", action)

CODE_COL_CANDIDATES = [
    "\u4ee3\u7801",
    "\u8bc1\u5238\u4ee3\u7801",
    "\u54c1\u79cd\u4ee3\u7801",
    "variety",
    "symbol",
    "code",
]
NAME_COL_CANDIDATES = [
    "\u540d\u79f0",
    "\u8bc1\u5238\u7b80\u79f0",
    "\u54c1\u79cd\u540d\u79f0",
    "name",
    "\u80a1\u7968\u7b80\u79f0",
    "\u80a1\u7968\u540d\u79f0",
]
NAME_ITEM_CANDIDATES = [
    "\u80a1\u7968\u7b80\u79f0",
    "\u80a1\u7968\u540d\u79f0",
    "\u8bc1\u5238\u7b80\u79f0",
    "\u540d\u79f0",
]

def _normalize_date_str(date_str):
    try:
        dt = pd.to_datetime(date_str)
        return dt.strftime("%Y-%m-%d"), dt.strftime("%Y%m%d")
    except Exception:
        s = str(date_str)
        return s, s.replace("-", "")

def _min_cache_path(symbol, date_key, period, is_index):
    kind = "index" if is_index else "stock"
    return os.path.join(MIN_CACHE_DIR, f"p{period}", kind, str(symbol), f"{date_key}.csv")

def _read_min_cache(path):
    if os.path.exists(path):
        try:
            return pd.read_csv(path, parse_dates=["time"])
        except Exception as e:
            logger.warning("è¯»å–åˆ†æ—¶ç¼“å­˜å¤±è´¥: %s", e)
    return None

def _write_min_cache(path, df):
    try:
        os.makedirs(os.path.dirname(path), exist_ok=True)
        tmp_path = f"{path}.tmp"
        df.to_csv(tmp_path, index=False)
        os.replace(tmp_path, path)
    except Exception as e:
        logger.warning("ä¿å­˜åˆ†æ—¶ç¼“å­˜å¤±è´¥: %s", e)

def _parse_hhmm(value):
    try:
        parts = str(value).split(":")
        if len(parts) == 2:
            return int(parts[0]), int(parts[1])
    except Exception:
        pass
    return 15, 15

def _load_prefetch_state():
    if not os.path.exists(AUTO_PREFETCH_STATE_FILE):
        return {}
    try:
        with open(AUTO_PREFETCH_STATE_FILE, "r", encoding="utf-8") as f:
            data = json.load(f)
        if isinstance(data, dict):
            return data
    except Exception as e:
        logger.warning("\u8bfb\u53d6\u81ea\u52a8\u9884\u53d6\u72b6\u6001\u5931\u8d25: %s", e)
    return {}

def _save_prefetch_state(state):
    try:
        os.makedirs(os.path.dirname(AUTO_PREFETCH_STATE_FILE), exist_ok=True)
        with open(AUTO_PREFETCH_STATE_FILE, "w", encoding="utf-8") as f:
            json.dump(state, f)
    except Exception as e:
        logger.warning("\u4fdd\u5b58\u81ea\u52a8\u9884\u53d6\u72b6\u6001\u5931\u8d25: %s", e)

def _is_trading_day(target_date, origin_df):
    if origin_df is None or origin_df.empty:
        return False
    try:
        return target_date in set(origin_df['æ—¥æœŸ'].dt.date)
    except Exception:
        return False

def _get_daily_codes(origin_df, target_date):
    if origin_df is None or origin_df.empty:
        return [], {}
    daily = origin_df[origin_df['æ—¥æœŸ'].dt.date == target_date]
    if daily.empty:
        return [], {}
    codes = daily['ä»£ç '].astype(str).tolist()
    name_map = dict(zip(daily['ä»£ç '].astype(str), daily['åç§°']))
    return codes, name_map

def _scan_cached_dates(period='1', is_index=False):
    base = os.path.join(MIN_CACHE_DIR, f"p{period}", "index" if is_index else "stock")
    if not os.path.exists(base):
        return []
    dates = set()
    for root, _, files in os.walk(base):
        for f in files:
            if f.endswith('.csv'):
                dates.add(f[:-4])
    return sorted(dates)

def _get_cached_codes_for_date(date_key, codes, period='1', is_index=False):
    cached = set()
    for code in codes:
        path = _min_cache_path(code, date_key, period, is_index)
        if os.path.exists(path):
            cached.add(code)
    return cached

def _serial_fetch_intraday(date_str, codes, name_map, include_indices=True, delay_sec=10, retry_sleep_sec=300, max_retries=3, job_tag="manual"):
    indices_map = {
        "000300": "\ud83d\udcca \u6caa\u6df1300",
        "000001": "\ud83d\udcc8 \u4e0a\u8bc1\u6307\u6570",
        "399001": "\ud83d\udcc9 \u6df1\u8bc1\u6210\u6307",
    }
    tasks = []
    if include_indices:
        for idx_code, idx_name in indices_map.items():
            tasks.append({"code": idx_code, "name": idx_name, "is_index": True})
    for code in codes:
        tasks.append({"code": str(code), "name": name_map.get(str(code), str(code)), "is_index": False})
    logger.info("\u4efb\u52a1\u5f00\u59cb(%s): date=%s total=%s delay=%.1fs retry=%.0fs max_retries=%s", job_tag, date_str, len(tasks), delay_sec, retry_sleep_sec, max_retries)
    success = 0
    failed = 0
    for t in tasks:
        code = t['code']
        name = t['name']
        is_index = t['is_index']
        api_name = "index_zh_a_hist_min_em" if is_index else "stock_zh_a_hist_min_em"
        attempt = 0
        while True:
            try:
                data = fetch_cached_min_data(code, date_str, is_index=is_index, period='1', raise_on_error=True)
                if data is None or data.empty:
                    raise RuntimeError("\u63a5\u53e3\u8fd4\u56de\u7a7a")
                success += 1
                logger.info("\u9884\u53d6\u6210\u529f(%s): code=%s name=%s api=%s", job_tag, code, name, api_name)
                break
            except Exception as e:
                attempt += 1
                logger.warning("\u9884\u53d6\u5931\u8d25(%s): code=%s name=%s api=%s attempt=%s err=%s", job_tag, code, name, api_name, attempt, e)
                if max_retries > 0 and attempt >= max_retries:
                    failed += 1
                    logger.warning("\u9884\u53d6\u653e\u5f03(%s): code=%s name=%s", job_tag, code, name)
                    break
                time.sleep(retry_sleep_sec)
        if delay_sec > 0:
            time.sleep(delay_sec)
    logger.info("\u4efb\u52a1\u5b8c\u6210(%s): date=%s success=%s failed=%s", job_tag, date_str, success, failed)
    return success, failed

def _auto_prefetch_worker(date_str, codes, name_map, ctx=None):
    if ctx:
        add_script_run_ctx(threading.current_thread(), ctx)
    state = {"date": date_str, "status": "running", "updated": int(time.time())}
    _save_prefetch_state(state)
    success, failed = _serial_fetch_intraday(
        date_str,
        codes,
        name_map,
        include_indices=True,
        delay_sec=AUTO_PREFETCH_DELAY_SEC,
        retry_sleep_sec=AUTO_PREFETCH_RETRY_SLEEP_SEC,
        max_retries=AUTO_PREFETCH_MAX_RETRIES,
        job_tag="auto",
    )
    state = {"date": date_str, "status": "done" if failed == 0 else "partial", "success": success, "failed": failed, "updated": int(time.time())}
    _save_prefetch_state(state)

def _start_manual_prefetch(date_str, codes, name_map, include_indices=True):
    if not codes and not include_indices:
        return False
    ctx = get_script_run_ctx()
    def _worker():
        if ctx:
            add_script_run_ctx(threading.current_thread(), ctx)
        _serial_fetch_intraday(
            date_str,
            codes,
            name_map,
            include_indices=include_indices,
            delay_sec=AUTO_PREFETCH_DELAY_SEC,
            retry_sleep_sec=AUTO_PREFETCH_RETRY_SLEEP_SEC,
            max_retries=AUTO_PREFETCH_MAX_RETRIES,
            job_tag="manual",
        )
    t = threading.Thread(target=_worker, daemon=True)
    t.start()
    return True

def _start_auto_prefetch_if_needed(origin_df):
    if not AUTO_PREFETCH_ENABLED:
        return
    now = datetime.now()
    h, m = _parse_hhmm(AUTO_PREFETCH_TIME)
    if now.hour < h or (now.hour == h and now.minute < m):
        return
    today = now.date()
    if not _is_trading_day(today, origin_df):
        return
    today_str = today.strftime("%Y-%m-%d")
    state = _load_prefetch_state()
    if state.get("date") == today_str and state.get("status") in ("running", "done"):
        return
    codes, name_map = _get_daily_codes(origin_df, today)
    if not codes:
        return
    if st.session_state.get("auto_prefetch_started"):
        return
    st.session_state["auto_prefetch_started"] = True
    ctx = get_script_run_ctx()
    t = threading.Thread(target=_auto_prefetch_worker, args=(today_str, codes, name_map, ctx), daemon=True)
    t.start()

def _load_name_refresh_state():
    if not os.path.exists(NAME_REFRESH_FILE):
        return {}
    try:
        with open(NAME_REFRESH_FILE, "r", encoding="utf-8") as f:
            data = json.load(f)
        if isinstance(data, dict):
            return data
    except Exception as e:
        logger.warning("è¯»å–åç§°åˆ·æ–°è®°å½•å¤±è´¥: %s", e)
    return {}

def _save_name_refresh_state(state):
    try:
        os.makedirs(os.path.dirname(NAME_REFRESH_FILE), exist_ok=True)
        with open(NAME_REFRESH_FILE, "w", encoding="utf-8") as f:
            json.dump(state, f)
    except Exception as e:
        logger.warning("ä¿å­˜åç§°åˆ·æ–°è®°å½•å¤±è´¥: %s", e)

def _load_name_map():
    if not os.path.exists(NAME_MAP_FILE):
        return {}
    try:
        with open(NAME_MAP_FILE, "r", encoding="utf-8") as f:
            data = json.load(f)
        if isinstance(data, dict):
            return {str(k): v for k, v in data.items()}
    except Exception as e:
        logger.warning("è¯»å–åç§°æ˜ å°„å¤±è´¥: %s", e)
    return {}

def _save_name_map(name_map):
    try:
        os.makedirs(os.path.dirname(NAME_MAP_FILE), exist_ok=True)
        with open(NAME_MAP_FILE, "w", encoding="utf-8") as f:
            json.dump(name_map, f, ensure_ascii=False)
    except Exception as e:
        logger.warning("ä¿å­˜åç§°æ˜ å°„å¤±è´¥: %s", e)

def _resolve_code_name_columns(df):
    if df is None or df.empty:
        return None, None
    cols = list(df.columns)
    for code_col in CODE_COL_CANDIDATES:
        if code_col in cols:
            for name_col in NAME_COL_CANDIDATES:
                if name_col in cols:
                    return code_col, name_col
    for code_col in cols:
        if "\u4ee3\u7801" in str(code_col):
            for name_col in cols:
                if ("\u540d\u79f0" in str(name_col)) or ("\u7b80\u79f0" in str(name_col)):
                    return code_col, name_col
    if len(cols) >= 2:
        return cols[0], cols[1]
    return None, None

def _name_map_from_df(df):
    if df is None or df.empty:
        return {}
    code_col, name_col = _resolve_code_name_columns(df)
    if not code_col or not name_col:
        return {}
    try:
        sub = df[[code_col, name_col]].copy()
        sub[code_col] = sub[code_col].astype(str)
        sub[name_col] = sub[name_col].astype(str)
        return dict(zip(sub[code_col], sub[name_col]))
    except Exception as e:
        logger.warning("åç§°æ˜ å°„æ„å»ºå¤±è´¥: %s", e)
        return {}

def _extract_name_from_kv_df(df):
    if df is None or df.empty:
        return None
    item_col = None
    value_col = None
    if "item" in df.columns and "value" in df.columns:
        item_col, value_col = "item", "value"
    elif "\u9879\u76ee" in df.columns and "\u503c" in df.columns:
        item_col, value_col = "\u9879\u76ee", "\u503c"
    if not item_col or not value_col:
        return None
    try:
        mapping = dict(zip(df[item_col], df[value_col]))
    except Exception:
        return None
    for key in NAME_ITEM_CANDIDATES:
        if key in mapping and mapping[key]:
            return str(mapping[key]).strip()
    return None

def _fetch_name_for_code(code):
    code = str(code)
    if hasattr(ak, "stock_individual_info_em"):
        try:
            df = ak.stock_individual_info_em(symbol=code)
            name = _extract_name_from_kv_df(df)
            if name:
                return name
        except Exception as e:
            logger.warning("è·å–åç§°å¤±è´¥: code=%s err=%s", code, e)
    return None

def _should_refresh_names(state, now_ts):
    last_attempt = state.get("last_attempt_ts")
    if isinstance(last_attempt, (int, float)):
        if now_ts - last_attempt < NAME_REFRESH_MIN_INTERVAL_MINUTES * 60:
            return False
    last_refresh = state.get("last_refresh_ts")
    if isinstance(last_refresh, (int, float)):
        if now_ts - last_refresh < NAME_REFRESH_TTL_HOURS * 3600:
            return False
    return True

def _refresh_name_map_if_needed(force=False):
    now_ts = int(time.time())
    state = _load_name_refresh_state()
    if state.get("name_map_version") != NAME_MAP_VERSION:
        force = True
    if (not force) and (not _should_refresh_names(state, now_ts)):
        logger.info("åç§°æ˜ å°„æ— éœ€åˆ·æ–°ï¼Œä½¿ç”¨æœ¬åœ°ç¼“å­˜")
        return _load_name_map()
    state["last_attempt_ts"] = now_ts
    _save_name_refresh_state(state)
    logger.info("å¼€å§‹åˆ·æ–°åç§°æ˜ å°„ (force=%s)", force)
    def _try_source(label, fn):
        try:
            df = fn()
        except Exception as e:
            logger.warning("åç§°æºè°ƒç”¨å¤±è´¥: %s err=%s", label, e)
            return {}
        name_map = _name_map_from_df(df)
        if not name_map:
            return {}
        state["last_source"] = label
        return name_map

    sources = [("stock_zh_a_spot_em", lambda: ak.stock_zh_a_spot_em())]
    if hasattr(ak, "stock_info_a_code_name"):
        sources.append(("stock_info_a_code_name", lambda: ak.stock_info_a_code_name()))
    if hasattr(ak, "stock_zh_a_spot"):
        sources.append(("stock_zh_a_spot", lambda: ak.stock_zh_a_spot()))
    sources.append(("index_stock_cons_000300", lambda: ak.index_stock_cons(symbol="000300")))

    for label, fn in sources:
        name_map = _try_source(label, fn)
        if name_map:
            _save_name_map(name_map)
            logger.info("åç§°æ˜ å°„æ›´æ–°æˆåŠŸ: source=%s count=%s", label, len(name_map))
            state["last_refresh_ts"] = now_ts
            state["name_map_version"] = NAME_MAP_VERSION
            _save_name_refresh_state(state)
            return name_map
    logger.warning("åç§°æ˜ å°„åˆ·æ–°å¤±è´¥ï¼Œä½¿ç”¨æœ¬åœ°ç¼“å­˜")
    return _load_name_map()

def _refresh_name_map_for_codes(codes, force=False):
    codes = [str(c) for c in codes if c is not None and str(c).strip()]
    logger.info("åç§°è¡¥é½å¼€å§‹: codes=%s force=%s", len(codes), force)
    if not codes:
        return _refresh_name_map_if_needed(force=force)

    name_map = _refresh_name_map_if_needed(force=force)
    if not name_map:
        name_map = _load_name_map()

    state = _load_name_refresh_state()
    now_ts = int(time.time())
    last_refresh = state.get("last_refresh_ts")
    global_fresh = (
        isinstance(last_refresh, (int, float))
        and now_ts - last_refresh < NAME_REFRESH_TTL_HOURS * 3600
        and bool(name_map)
    )
    if global_fresh and (not force):
        logger.info("åç§°è¡¥é½å®Œæˆ: æ— éœ€æ›´æ–°")
        return name_map

    code_state = state.get("code_refresh_ts")
    if not isinstance(code_state, dict):
        code_state = {}

    updated = False
    updated_count = 0
    for code in codes:
        last_ts = code_state.get(code)
        if (not force) and isinstance(last_ts, (int, float)):
            if now_ts - last_ts < NAME_REFRESH_TTL_HOURS * 3600:
                continue
        name = _fetch_name_for_code(code)
        if name:
            name_map[code] = name
            code_state[code] = now_ts
            updated = True
            updated_count += 1

    if updated:
        _save_name_map(name_map)
        state["code_refresh_ts"] = code_state
        _save_name_refresh_state(state)
        logger.info("åç§°è¡¥é½å®Œæˆ: æ›´æ–° %s æ¡", updated_count)
    else:
        logger.info("åç§°è¡¥é½å®Œæˆ: æ— éœ€æ›´æ–°")
    return name_map

def _refresh_cached_names(cached_df):
    if cached_df is None or cached_df.empty:
        return cached_df
    if 'ä»£ç ' not in cached_df.columns:
        return cached_df
    name_map = _refresh_name_map_if_needed()
    if not name_map:
        return cached_df
    cached_df['ä»£ç '] = cached_df['ä»£ç '].astype(str)
    if 'åç§°' in cached_df.columns:
        cached_df['åç§°'] = cached_df['ä»£ç '].map(name_map).fillna(cached_df['åç§°'])
    else:
        cached_df['åç§°'] = cached_df['ä»£ç '].map(name_map)
    return cached_df

def build_data_backup_zip():
    data_dir = "data"
    if not os.path.isdir(data_dir):
        return None
    buf = io.BytesIO()
    with zipfile.ZipFile(buf, "w", zipfile.ZIP_DEFLATED) as zf:
        for root, _, files in os.walk(data_dir):
            for name in files:
                abs_path = os.path.join(root, name)
                rel_path = os.path.relpath(abs_path, data_dir)
                zf.write(abs_path, os.path.join("data", rel_path))
    buf.seek(0)
    return buf.read()

def restore_data_backup(uploaded_file):
    data_dir = "data"
    os.makedirs(data_dir, exist_ok=True)
    uploaded_file.seek(0)
    restored = 0
    with zipfile.ZipFile(uploaded_file) as zf:
        for member in zf.infolist():
            name = member.filename.replace("\\", "/")
            if name.endswith("/"):
                continue
            if name.startswith("/") or ".." in name.split("/"):
                continue
            parts = name.split("/")
            if parts and parts[0] == "data":
                parts = parts[1:]
            if not parts:
                continue
            dest_path = os.path.join(data_dir, *parts)
            os.makedirs(os.path.dirname(dest_path), exist_ok=True)
            with zf.open(member) as src, open(dest_path, "wb") as dst:
                shutil.copyfileobj(src, dst)
            restored += 1
    return restored

def clear_min_cache():
    if os.path.isdir(MIN_CACHE_DIR):
        shutil.rmtree(MIN_CACHE_DIR, ignore_errors=True)

def get_start_date(years_back=2):
    """è®¡ç®— N å¹´å‰çš„æ—¥æœŸï¼Œè¿”å› YYYYMMDD å­—ç¬¦ä¸²"""
    target = datetime.now() - timedelta(days=365 * years_back)
    return target.strftime("%Y%m%d")

def fetch_history_data():
    """
    è·å–æ²ªæ·±300æˆåˆ†è‚¡è¿‡å»2å¹´çš„æ—¥çº¿æ•°æ®ã€‚
    å¢é‡æ›´æ–°é€»è¾‘ï¼š
    1. å°è¯•è¯»å–æœ¬åœ°ç¼“å­˜ã€‚
    2. å¦‚æœæœ‰ç¼“å­˜ï¼Œæ£€æŸ¥ç¼“å­˜ä¸­æœ€æ–°çš„æ—¥æœŸã€‚
    3. å¦‚æœ æœ€æ–°æ—¥æœŸ < æ˜¨å¤© (æˆ–ä»Šå¤©æ”¶ç›˜å)ï¼Œåˆ™åªä¸‹è½½å¢é‡æ•°æ®ï¼ˆä¸ºäº†ç®€å•å¯é ï¼ŒAkShareæ—¥çº¿æ¥å£é€šå¸¸æ˜¯æŒ‰æ®µä¸‹è½½ï¼Œæˆ–è€…å…¨é‡ä¸‹è½½ï¼‰ã€‚
       * ä¿®æ­£ç­–ç•¥ï¼šç”±äº ak.stock_zh_a_hist æ¥å£å‚æ•°æ˜¯ start_date å’Œ end_dateï¼Œ
         æˆ‘ä»¬å¯ä»¥åªä¸‹è½½ [ç¼“å­˜æœ€æ–°æ—¥æœŸ+1, ä»Šå¤©] çš„æ•°æ®ï¼Œç„¶å append åˆ°ç¼“å­˜ä¸­ã€‚
    """
    logger.info("å¼€å§‹åŠ è½½å†å²æ•°æ®")
    cached_df = pd.DataFrame()
    last_cached_date = None
    logger.info("å·²åŠ è½½æœ¬åœ°ç¼“å­˜ï¼Œæœ€æ–°æ—¥æœŸ=%s", last_cached_date)

    # 1. å°è¯•åŠ è½½æœ¬åœ°ç¼“å­˜
    if os.path.exists(CACHE_FILE):
        try:
            cached_df = pd.read_parquet(CACHE_FILE)
            if not cached_df.empty:
                last_cached_date = cached_df['æ—¥æœŸ'].max().date()
                st.toast(f"âœ… å·²åŠ è½½æœ¬åœ°ç¼“å­˜ï¼Œæœ€æ–°æ—¥æœŸ: {last_cached_date}")
        except Exception as e:
            st.error(f"è¯»å–ç¼“å­˜æ–‡ä»¶å¤±è´¥: {e}")

    # 2. è®¡ç®—éœ€è¦ä¸‹è½½çš„æ—¶é—´èŒƒå›´
    today = datetime.now().date()
    
    # å¦‚æœç¼“å­˜é‡Œçš„æ—¥æœŸå·²ç»æ˜¯ä»Šå¤©ï¼Œä¸”ç°åœ¨æ˜¯ç›˜ä¸­ï¼Œå¯èƒ½ç”¨æˆ·æƒ³åˆ·æ–°
    # ä½†ç®€å•èµ·è§ï¼Œæˆ‘ä»¬è®¾å®šï¼šå¦‚æœç¼“å­˜æœ€æ–°æ—¥æœŸ < ä»Šå¤©ï¼Œè‚¯å®šè¦å°è¯•ä¸‹è½½ã€‚
    # å¦‚æœç¼“å­˜æœ€æ–°æ—¥æœŸ == ä»Šå¤©ï¼Œåªæœ‰å½“å¼ºåˆ¶åˆ·æ–°æ—¶æ‰é€šè¿‡(å¤–éƒ¨æ§åˆ¶)ï¼Œè¿™é‡Œå‡½æ•°å†…éƒ¨å…ˆå‡è®¾"å·²æ˜¯æœ€æ–°"
    # ä½†ä¸ºäº†æ”¯æŒç›˜ä¸­åˆ·æ–°ï¼Œå¦‚æœ last_cached_date == todayï¼Œæˆ‘ä»¬å…¶å®å¯ä»¥é‡æ‹‰ä»Šå¤©çš„ã€‚
    # è¿™é‡Œæˆ‘ä»¬åªå¤„ç† last_cached_date < today çš„è‡ªåŠ¨å¢é‡, æˆ–è€… force refresh (caller clears cache)
    
    if last_cached_date:
        if last_cached_date >= today:
             # å¦‚æœå·²ç»æœ‰ä»Šå¤©çš„æ•°æ®ï¼Œæš‚æ—¶ç›´æ¥è¿”å› (ç”¨æˆ·éœ€ç‚¹å‡»å¼ºåˆ¶åˆ·æ–°æ¥æ›´æ–°ä»Šæ—¥ç›˜ä¸­æ•°æ®)
             # ä½†ä¸ºäº†èƒ½å¤Ÿ"è‡ªåŠ¨"æ‹‰å–ç›˜ä¸­ï¼Œå¦‚æœ last_cached_date == todayï¼Œæˆ‘ä»¬åšä¸ªåˆ¤æ–­ï¼Ÿ
             # ç°åœ¨çš„é€»è¾‘æ˜¯ï¼šå¦‚æœç¼“å­˜æ–‡ä»¶å­˜åœ¨ä¸”æ—¥æœŸ>=ä»Šå¤©ï¼Œå°±ä¸åŠ¨äº†ã€‚
             # è¿™å¯¼è‡´å¦‚æœæ—©ä¸Š9ç‚¹è·‘äº†ä¸€æ¬¡ï¼ˆæœ‰æ•°æ®ï¼‰ï¼Œä¸‹åˆ3ç‚¹å†è·‘ï¼Œè¿˜æ˜¯æ—§çš„ã€‚
             # æ”¹è¿›ï¼šå¦‚æœæ˜¯ä»Šå¤©ï¼Œä¸”ç°åœ¨è¿˜æ²¡æ”¶ç›˜ï¼Œæˆ–è€…åˆšæ”¶ç›˜ï¼Œå…è®¸è¦†ç›–ï¼Ÿ
             # æš‚ä¿ç•™åŸé€»è¾‘é˜²æ­¢é¢‘ç¹è¯·æ±‚ï¼Œä¾é  "å¼ºåˆ¶åˆ·æ–°" æŒ‰é’®æ¥æ¸…ç©ºç¼“å­˜ã€‚
             return _refresh_cached_names(cached_df)
        
        start_date_str = (last_cached_date + timedelta(days=1)).strftime("%Y%m%d")
    else:
        start_date_str = get_start_date(2)
        
    end_date_str = today.strftime("%Y%m%d")

    # å¦‚æœä¸éœ€è¦æ›´æ–°
    if start_date_str > end_date_str:
        return _refresh_cached_names(cached_df)

    # çŠ¶æ€å®¹å™¨
    status_text = st.empty()
    progress_bar = st.progress(0)
    
    try:
        # å¦‚æœæ˜¯å¢é‡æ›´æ–°
        is_incremental = not cached_df.empty
        if not is_incremental:
            status_text.text("æ­£åœ¨åˆå§‹åŒ–å…¨é‡å†å²æ•°æ®...")
        else:
            status_text.text(f"æ­£åœ¨æ£€æŸ¥å¢é‡æ•°æ® ({start_date_str} - {end_date_str})...")

        # è·å–æˆåˆ†è‚¡åˆ—è¡¨
        try:
            logger.info("AKShare è·å–æˆåˆ†è‚¡åˆ—è¡¨: 000300")
            cons_df = ak.index_stock_cons(symbol="000300")
            if cons_df is not None:
                logger.info("æˆåˆ†è‚¡åˆ—è¡¨è·å–æˆåŠŸ: rows=%s", len(cons_df))
        except:
            if not cached_df.empty:
                logger.warning("æˆåˆ†è‚¡åˆ—è¡¨è·å–å¤±è´¥ï¼Œä½¿ç”¨ç¼“å­˜")
                st.warning("æˆåˆ†è‚¡åˆ—è¡¨è·å–å¤±è´¥ï¼Œä½¿ç”¨ç¼“å­˜æ•°æ®")
                return _refresh_cached_names(cached_df)
            return pd.DataFrame()
        
        if cons_df is None or cons_df.empty:
            logger.warning("æˆåˆ†è‚¡åˆ—è¡¨ä¸ºç©ºï¼Œä½¿ç”¨ç¼“å­˜")
            return _refresh_cached_names(cached_df) if not cached_df.empty else pd.DataFrame()

        if 'variety' in cons_df.columns:
            code_col, name_col = 'variety', 'name'
        elif 'å“ç§ä»£ç ' in cons_df.columns:
            code_col, name_col = 'å“ç§ä»£ç ', 'å“ç§åç§°'
        else:
            code_col = cons_df.columns[0]
            name_col = cons_df.columns[1]
            
        stock_list = cons_df[code_col].tolist()
        stock_names = dict(zip(cons_df[code_col], cons_df[name_col]))
        
        # Update name map (refresh cadence)
        name_map = _refresh_name_map_if_needed()
        if name_map:
            stock_names.update(name_map)

        new_data_list = []
        total_stocks = len(stock_list)
        
        # --- å°è¯•è·å–ä»Šæ—¥å®æ—¶æ•°æ® (Spot) ä½œä¸ºè¡¥å…… ---
        # å¾ˆå¤šæ—¶å€™ stock_zh_a_hist åœ¨ç›˜ä¸­ä¸è¿”å›å½“æ—¥æ•°æ®ï¼Œæˆ–è€…æœ‰äº›æºä¸è¿”å›ã€‚
        # æˆ‘ä»¬å¯ä»¥æ‹‰å– ak.stock_zh_a_spot_em() è·å–æ‰€æœ‰Aè‚¡å®æ—¶è¡Œæƒ…ï¼Œç„¶åè¿‡æ»¤å‡º CSI300
        # ä»…å½“æˆ‘ä»¬éœ€è¦ "ä»Šå¤©" çš„æ•°æ®æ—¶ (start_date_str <= today_str)
        today_spot_map = {}
        has_today_hist = False # æ ‡è®°æ˜¯å¦é€šè¿‡ hist æ¥å£æ‹¿åˆ°äº†ä»Šå¤©æ•°æ®
        
        if end_date_str >= start_date_str:
             try:
                 logger.info("AKShare è·å–å®æ—¶è¡Œæƒ…ï¼Œç”¨äºè¡¥é½ä»Šæ—¥æ•°æ®")
                 logger.info("è°ƒç”¨æ¥å£: stock_zh_a_spot_em")
                 spot_df = ak.stock_zh_a_spot_em()
                 if spot_df is not None and not spot_df.empty:
                     # spot_df columns: ä»£ç , åç§°, æœ€æ–°ä»·, æ¶¨è·Œå¹…, æˆäº¤é¢ ...
                     # å»ºç«‹æ˜ å°„: code -> row
                     spot_df['ä»£ç '] = spot_df['ä»£ç '].astype(str)
                     today_spot_map = spot_df.set_index('ä»£ç ').to_dict('index')
             except Exception as e:
                 logger.warning("å®æ—¶æ•°æ®æ‹‰å–å¤±è´¥: %s", e)

        # å¾ªç¯è·å–å†å²
        # ä½¿ç”¨ ThreadPoolExecutor åŠ é€Ÿå¢é‡å†å²ä¸‹è½½ (å¦‚æœéœ€è¦ä¸‹è½½å¾ˆå¤šå¤©)
        # ä½† akshare æ¥å£é¢‘ç¹è°ƒç”¨å¯èƒ½å—é™ï¼Œé€‚åº¦å¹¶å‘
        
        def fetch_one_stock(code, name):
            try:
                # è·å–æ—¥çº¿
                logger.info("è°ƒç”¨æ¥å£: stock_zh_a_hist code=%s start=%s end=%s", code, start_date_str, end_date_str)
                df_hist = ak.stock_zh_a_hist(symbol=code, start_date=start_date_str, end_date=end_date_str, adjust="qfq")
                # æ£€æŸ¥æ˜¯å¦åŒ…å«ä»Šå¤©
                # å¦‚æœ df_hist ä¸åŒ…å«ä»Šå¤©ï¼Œä½†æˆ‘ä»¬æœ‰ today_spot_mapï¼Œåˆ™äººå·¥è¡¥ä¸€è¡Œ
                fetched_today = False
                if df_hist is not None and not df_hist.empty:
                    logger.info("æ—¥çº¿æ‹‰å–æˆåŠŸ: code=%s rows=%s", code, len(df_hist))
                    df_hist['æ—¥æœŸ'] = pd.to_datetime(df_hist['æ—¥æœŸ'])
                    if end_date_str in df_hist['æ—¥æœŸ'].dt.strftime("%Y%m%d").values:
                        fetched_today = True
                else:
                    logger.warning("æ—¥çº¿æ¥å£è¿”å›ç©º: code=%s", code)
                    df_hist = pd.DataFrame()

                # å¦‚æœæ²¡æœ‰æ‹‰åˆ°ä»Šå¤©çš„æ•°æ®ï¼Œä¸”æˆ‘ä»¬éœ€è¦ä»Šå¤© (end_date_str == today)ï¼Œè¡¥å…¨
                if (not fetched_today) and (end_date_str == datetime.now().strftime("%Y%m%d")):
                    if code in today_spot_map:
                        row = today_spot_map[code]
                        # æ„é€ ä¸€è¡Œ
                        # å¿…é¡»å­—æ®µ: æ—¥æœŸ, æ”¶ç›˜, æ¶¨è·Œå¹…, æˆäº¤é¢, ä»£ç , åç§°
                        # spot row keys: 'æœ€æ–°ä»·', 'æ¶¨è·Œå¹…', 'æˆäº¤é¢'
                        try:
                             new_row = pd.DataFrame([{
                                 'æ—¥æœŸ': pd.to_datetime(end_date_str),
                                 'æ”¶ç›˜': row['æœ€æ–°ä»·'],
                                 'æ¶¨è·Œå¹…': row['æ¶¨è·Œå¹…'],
                                 'æˆäº¤é¢': row['æˆäº¤é¢'],
                                 'ä»£ç ': code,
                                 'åç§°': name
                             }])
                             df_hist = pd.concat([df_hist, new_row], ignore_index=True)
                        except:
                            pass
                
                if df_hist is not None and not df_hist.empty:
                    # ç¡®ä¿åˆ—å­˜åœ¨
                    if 'æ—¥æœŸ' not in df_hist.columns: return None
                    cols_needed = ['æ—¥æœŸ', 'æ”¶ç›˜', 'æ¶¨è·Œå¹…', 'æˆäº¤é¢']
                    for c in cols_needed:
                        if c not in df_hist.columns: return None
                    
                    df_hist = df_hist[cols_needed].copy()
                    df_hist['ä»£ç '] = code
                    df_hist['åç§°'] = name
                    return df_hist
            except Exception as e:
                logger.warning("æ—¥çº¿æ‹‰å–å¤±è´¥: code=%s err=%s", code, e)
                pass
            return None

        # å¦‚æœæ˜¯å¢é‡åªå·®1å¤©ï¼Œå…¶å®å•çº¿ç¨‹ä¹Ÿå¿«ã€‚å¦‚æœæ˜¯åˆå§‹åŒ–ï¼Œå¹¶å‘ã€‚
        # Use concurrency
        ctx = get_script_run_ctx()
        def fetch_one_stock_wrapper(code, name):
            if ctx:
                add_script_run_ctx(threading.current_thread(), ctx)
            return fetch_one_stock(code, name)

        logger.info("AKShare æ‹‰å–æ—¥çº¿: è‚¡ç¥¨æ•°=%s åŒºé—´=%s~%s", len(stock_list), start_date_str, end_date_str)
        with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
             future_map = {executor.submit(fetch_one_stock_wrapper, c, stock_names.get(c, c)): c for c in stock_list}
             
             for i, future in enumerate(concurrent.futures.as_completed(future_map)):
                 # Update progress
                 if i % 10 == 0:
                     progress_bar.progress((i + 1) / total_stocks)
                     status_text.text(f"æ­£åœ¨åŒæ­¥æ•°æ®: {i+1}/{total_stocks}")
                 
                 res = future.result()
                 if res is not None:
                     new_data_list.append(res)
                
        status_text.empty()
        progress_bar.empty()
        
        # åˆå¹¶é€»è¾‘
        if new_data_list:
            new_df = pd.concat(new_data_list, ignore_index=True)
            # ç±»å‹è½¬æ¢
            new_df['æ—¥æœŸ'] = pd.to_datetime(new_df['æ—¥æœŸ'])
            new_df['æ¶¨è·Œå¹…'] = pd.to_numeric(new_df['æ¶¨è·Œå¹…'], errors='coerce')
            new_df['æˆäº¤é¢'] = pd.to_numeric(new_df['æˆäº¤é¢'], errors='coerce')
            new_df['æ”¶ç›˜'] = pd.to_numeric(new_df['æ”¶ç›˜'], errors='coerce')
            
            if cached_df.empty:
                final_df = new_df
            else:
                # åˆå¹¶æ—§æ•°æ®å’Œæ–°æ•°æ®ï¼Œå¹¶å»é‡
                st.toast(f"ğŸ“¥ æˆåŠŸè·å– {len(new_df)} æ¡æ–°è®°å½•")
                final_df = pd.concat([cached_df, new_df], ignore_index=True)
                # æŒ‰ 'æ—¥æœŸ' + 'ä»£ç ' å»é‡ï¼Œä¿ç•™æ–°çš„ï¼ˆå¦‚æœé‡å ï¼‰
                final_df.drop_duplicates(subset=['æ—¥æœŸ', 'ä»£ç '], keep='last', inplace=True)
        else:
            # æ²¡ä¸‹è½½åˆ°æ–°æ•°æ®ï¼ˆå¯èƒ½æ˜¯å‡æœŸï¼‰
            final_df = cached_df
            
        if final_df.empty:
            return pd.DataFrame()

        final_df = final_df.sort_values('æ—¥æœŸ')
        
        # ä½¿ç”¨æœ€æ–°çš„ stock_names æ›´æ–° DataFrame ä¸­çš„åç§°åˆ—
        if final_df is not None and not final_df.empty:
            # åªæ›´æ–°å­˜åœ¨çš„ä»£ç 
            final_df['åç§°'] = final_df['ä»£ç '].map(stock_names).fillna(final_df['åç§°'])
        
        # åªæœ‰å½“æœ‰æ–°æ•°æ® æˆ–è€… æ˜¯é¦–æ¬¡ä¸‹è½½æ—¶ï¼Œæ‰ä¿å­˜
        if new_data_list or cached_df.empty:
            try:
                if not os.path.exists("data"):
                    os.makedirs("data")
                final_df.to_parquet(CACHE_FILE)
                if not cached_df.empty:
                    st.toast("ğŸ’¾ å¢é‡æ•°æ®å·²åˆå¹¶å¹¶ä¿å­˜")
                else:
                    st.success("ğŸ’¾ å…¨é‡æ•°æ®å·²åˆå§‹åŒ–")
            except Exception as e:
                st.warning(f"æ— æ³•ä¿å­˜ç¼“å­˜: {e}")

        logger.info("å†å²æ•°æ®åŠ è½½å®Œæˆ: è¡Œæ•°=%s", len(final_df))
        return final_df

    except Exception as e:
        logger.exception("å…¨å±€æ•°æ®é”™è¯¯: %s", e)
        st.error(f"å…¨å±€æ•°æ®é”™è¯¯: {e}")
        status_text.empty()
        progress_bar.empty()
        return pd.DataFrame()

# -----------------------------------------------------------------------------
@st.cache_data(ttl=3600*24, show_spinner=False)
def fetch_cached_min_data(symbol, date_str, is_index=False, period='1', raise_on_error=False):
    """
    åŸå­åŒ–è·å–å•ä¸ªæ ‡çš„çš„åˆ†æ—¶æ•°æ®ï¼Œç‹¬ç«‹ç¼“å­˜ã€‚
    é¿å…å› è‚¡ç¥¨åˆ—è¡¨ç»„åˆå˜åŒ–å¯¼è‡´æ•´ä¸ªç¼“å­˜å¤±æ•ˆã€‚
    params:
    period: '1', '5', '15', '30', '60'
    """
    date_str_norm, date_key = _normalize_date_str(date_str)
    cache_path = _min_cache_path(symbol, date_key, period, is_index)
    cached_df = _read_min_cache(cache_path)
    if cached_df is not None and not cached_df.empty:
        logger.info("åˆ†æ—¶ç¼“å­˜å‘½ä¸­: code=%s date=%s period=%s index=%s path=%s", symbol, date_str_norm, period, is_index, cache_path)
        return cached_df
    logger.info("åˆ†æ—¶ç¼“å­˜æœªå‘½ä¸­ï¼Œå‡†å¤‡ç½‘ç»œæ‹‰å–: code=%s date=%s period=%s index=%s", symbol, date_str_norm, period, is_index)
    logger.info("AKShare åˆ†æ—¶æ‹‰å–: code=%s date=%s period=%s index=%s", symbol, date_str_norm, period, is_index)


    start_time = f"{date_str_norm} 09:30:00"
    end_time = f"{date_str_norm} 15:00:00"
    
    # æŒ‡æ•°é€€é¿ç­–ç•¥å…¨å±€å˜é‡ (ç®€å•æ¨¡æ‹Ÿï¼Œå®é™…ç¯å¢ƒåº”ç”¨ç±»å°è£…)
    # ä½¿ç”¨å‡½æ•°å±æ€§æš‚å­˜çŠ¶æ€
    if not hasattr(fetch_cached_min_data, "current_backoff"):
        fetch_cached_min_data.current_backoff = 0
            
    # ç®€å•çš„é‡è¯•æœºåˆ¶
    max_retries = 3
    last_err = None
    api_name = "index_zh_a_hist_min_em" if is_index else "stock_zh_a_hist_min_em"
    
    # å¦‚æœå¤„äº"å†·å´æœŸ"å†…? è¿™é‡Œç®€åŒ–ä¸ºï¼šæ¯æ¬¡å¤±è´¥åå¢åŠ ç­‰å¾…æ—¶é—´ï¼ŒæˆåŠŸåˆ™é‡ç½®
    
    for attempt in range(max_retries):
        try:
            logger.info("è°ƒç”¨æ¥å£: %s code=%s date=%s period=%s", api_name, symbol, date_str_norm, period)
            if is_index:
                # æŒ‡æ•°æ¥å£
                df = ak.index_zh_a_hist_min_em(symbol=symbol, period=period, start_date=start_time, end_date=end_time)
            else:
                # ä¸ªè‚¡æ¥å£
                df = ak.stock_zh_a_hist_min_em(symbol=symbol, start_date=start_time, end_date=end_time, period=period, adjust='qfq')
            
            if df is not None and not df.empty:
                logger.info("åˆ†æ—¶æ‹‰å–æˆåŠŸ: code=%s date=%s period=%s rows=%s", symbol, date_str_norm, period, len(df))
                # æˆåŠŸ - é‡ç½®é€€é¿
                if fetch_cached_min_data.current_backoff > 0:
                     logger.info("API æ¢å¤ï¼Œé‡ç½®é€€é¿æ—¶é—´")
                     fetch_cached_min_data.current_backoff = 0

                # ç»Ÿä¸€åˆ—å
                if 'æ—¶é—´' in df.columns:
                    df.rename(columns={'æ—¶é—´': 'time', 'å¼€ç›˜': 'open', 'æ”¶ç›˜': 'close'}, inplace=True)
                
                # ç®€å•æ¸…æ´—
                df['time'] = pd.to_datetime(df['time'])
                
                # è®¡ç®—æ¶¨è·Œå¹… (ç›¸å¯¹äºå½“æ—¥å¼€ç›˜)
                base_price = df['open'].iloc[0]
                df['pct_chg'] = (df['close'] - base_price) / base_price * 100
                
                result = df[['time', 'pct_chg', 'close']].copy()
                _write_min_cache(cache_path, result)
                return result
            else:
                logger.warning("åˆ†æ—¶æ¥å£è¿”å›ç©º: code=%s date=%s period=%s api=%s", symbol, date_str_norm, period, api_name)
                
        except Exception as e:
            last_err = e
            logger.warning("åˆ†æ—¶æ‹‰å–å¤±è´¥: code=%s date=%s period=%s api=%s err=%s", symbol, date_str_norm, period, api_name, e)
            # å¤±è´¥å¤„ç†é€»è¾‘
            # å¦‚æœæ˜¯ç‰¹å®šçš„ API é™åˆ¶é”™è¯¯ (éœ€åˆ†æ eï¼Œè¿™é‡Œç®€å•å‡è®¾æ‰€æœ‰å¼‚å¸¸éƒ½å¯èƒ½ç”±é¢‘ç‡å¯¼è‡´)
            # å¢åŠ é€€é¿æ—¶é—´
            if fetch_cached_min_data.current_backoff == 0:
                fetch_cached_min_data.current_backoff = 60 # åˆå§‹ 1 åˆ†é’Ÿ
            else:
                fetch_cached_min_data.current_backoff *= 2 # ç¿»å€
            
            wait_time = fetch_cached_min_data.current_backoff
            
            # åªæœ‰å½“è¿™æ˜¯åå°é¢„å–ä»»åŠ¡æ—¶æ‰è¿›è¡Œé•¿æ—¶é—´ç­‰å¾…? 
            # å‰å°å®æ—¶æ‹‰å–ä¸å®œç­‰å¾…å¤ªä¹…ã€‚è¿™é‡Œæˆ‘ä»¬æ·»åŠ ä¸€ä¸ªä¸Šä¸‹æ–‡åˆ¤æ–­æ˜¯ä¸ç°å®çš„ã€‚
            # ä½†æ—¢ç„¶ç”¨æˆ·æåˆ°äº†"ç¿»å€ç­‰å¾…"ï¼Œè¿™é€šå¸¸æ˜¯é’ˆå¯¹åå°çˆ¬è™«ã€‚
            # å¯¹äºå‰å°äº¤äº’ï¼Œç­‰å¾…1åˆ†é’Ÿç”¨æˆ·æ—©è·‘äº†ã€‚
            # ä¸ºäº†å…¼å®¹ï¼Œæˆ‘ä»¬åªåœ¨ "é¢„å–/çˆ¬è™«" æ¨¡å¼ä¸‹å¯ç”¨æ­¤é€»è¾‘ï¼Ÿ 
            # ä½† fetch_cached_min_data æ˜¯é€šç”¨å‡½æ•°ã€‚
            # å¦¥åï¼šå¦‚æœç­‰å¾…æ—¶é—´å¾ˆé•¿ (>5s)ï¼Œåˆ™å¯ä»¥è®¤ä¸ºè¿™æ˜¯ä¸€ä¸ªéœ€è¦é•¿æ—¶é—´æ¢å¤çš„é”™è¯¯ï¼Œ
            # åœ¨å‰å°ç›´æ¥å¤±è´¥æ¯”è¾ƒå¥½ã€‚åœ¨åå°åˆ™ sleepã€‚
            # ä½†è¿™é‡Œæ— æ³•åŒºåˆ†ã€‚æˆ‘ä»¬å‡è®¾æ­¤ä¸¥æ ¼çš„é€€é¿ç­–ç•¥åªåœ¨å¤–éƒ¨æ§åˆ¶å¾ªç¯ä¸­ç”Ÿæ•ˆæ¯”è¾ƒå¥½ã€‚
            # ä¿®æ”¹ï¼šå°†ä¸¥æ ¼çš„é€€é¿é€»è¾‘ç§»åˆ°è°ƒç”¨æ–¹çš„ loop ä¸­ (Task Worker)ï¼Œ
            # è¿™é‡Œçš„ fetch_cached_min_data åªè´Ÿè´£å•æ¬¡å°è¯•ã€‚
            pass

    return None

# --- æ–°å¢ï¼šåå°é¢„å–çº¿ç¨‹é€»è¾‘ ---
def background_prefetch_task(date_list, origin_df):
    """
    åå°çº¿ç¨‹ï¼šæ‰§è¡Œæ•°æ®é¢„å–ã€‚
    """
    total_dates = len(date_list)
    logger.info("åå°ä»»åŠ¡å¼€å§‹é¢„å– %s å¤©æ•°æ®", total_dates)
    
    current_backoff = 0 # ç§’
    
    indices_codes = ["000300", "000001", "399001"]
    
    for i, d in enumerate(date_list):
        d_str = d.strftime("%Y-%m-%d")
        logger.info("åå°ä»»åŠ¡å¤„ç†ä¸­: %s (%s/%s)", d_str, i + 1, total_dates)
        
        # ç­›é€‰
        daily = origin_df[origin_df['æ—¥æœŸ'].dt.date == d]
        if daily.empty: continue
        
        # Top 25
        top_stocks = daily.sort_values('æˆäº¤é¢', ascending=False).head(25)['ä»£ç '].tolist()
        
        # ä»»åŠ¡åˆ—è¡¨
        tasks = []
        for code in indices_codes: tasks.append((code, d_str, True))
        for code in top_stocks: tasks.append((code, d_str, False))
        
        # å†…å±‚é€ä¸ªæ‰§è¡Œ (ä¸ºäº†æ–¹ä¾¿æ§åˆ¶é€€é¿ï¼Œä¸”åå°ä»»åŠ¡ä¸æ€¥äºä¸€æ—¶çš„å¹¶å‘ï¼Œç¨³å®šç¬¬ä¸€)
        # å¦‚æœè¦å¹¶å‘ï¼Œä¹Ÿå¿…é¡»åœ¨å¹¶å‘å‘ç”Ÿå¼‚å¸¸æ—¶æ•è·å¹¶è§¦å‘é€€é¿ã€‚
        # ç®€å•èµ·è§ï¼Œè¿™é‡ŒæŒ‰é¡ºåºæˆ–å°æ‰¹æ¬¡æ‰§è¡Œã€‚
        
        for t_code, t_date, t_is_index in tasks:
            
            # Indefinite retry loop with backoff
            while True:
                try:
                    # æ£€æŸ¥é€€é¿
                    if current_backoff > 0:
                        logger.info("åå°ä»»åŠ¡å†·å´ä¸­ï¼Œç­‰å¾… %s ç§’", current_backoff)
                        time.sleep(current_backoff)
                        
                    # å°è¯•æ‹‰å– (fetch_cached_min_data å†…éƒ¨æœ‰ç¼“å­˜ï¼Œå¦‚æœå·²å­˜åœ¨ä¼šç›´æ¥è¿”å›)
                    # ä¸ºäº†æµ‹è¯• API è¿æ¥ï¼Œå¦‚æœç¼“å­˜å·²å­˜åœ¨ï¼Œå…¶å®ä¸ä¼šè§¦å‘ç½‘ç»œè¯·æ±‚ã€‚
                    # æˆ‘ä»¬éœ€è¦å‡è®¾ fetch_cached_min_data ä¼šå¤„ç†ç½‘ç»œã€‚
                    # æ³¨æ„ï¼šfetch_cached_min_data è¢« @st.cache_data è£…é¥°ã€‚
                    # åœ¨åå°çº¿ç¨‹è°ƒç”¨ st.cache_data è£…é¥°çš„å‡½æ•°é€šå¸¸æ˜¯æ²¡é—®é¢˜çš„ã€‚
                    
                    fetch_cached_min_data(t_code, t_date, is_index=t_is_index, period='1')
                    # åªæœ‰å½“æˆ‘ä»¬éœ€è¦æ›´å¤šæ•°æ®æ—¶æ‰æ‹‰5åˆ†é’Ÿ
                    # fetch_cached_min_data(t_code, t_date, is_index=t_is_index, period='5') 
                    
                    # Success
                    if current_backoff > 0:
                        logger.info("åå°ä»»åŠ¡å·²æ¢å¤ï¼Œé‡ç½®é€€é¿æ—¶é—´")
                        current_backoff = 0
                    
                    # æ‹‰å–æˆåŠŸåç¨å¾® sleep ä¸€ä¸‹é¿å…è¿‡äºé¢‘ç¹ (0.1s)
                    time.sleep(0.1)
                    break # è·³å‡º whileï¼Œå¤„ç†ä¸‹ä¸€ä¸ª task

                except Exception as e:
                    logger.warning("åå°ä»»åŠ¡è·å–å¤±è´¥: code=%s date=%s err=%s", t_code, t_date, e)
                    # è§¦å‘é€€é¿æœºåˆ¶
                    if current_backoff == 0:
                        current_backoff = 60
                    else:
                        current_backoff *= 2
                    
                    logger.warning("åå°ä»»åŠ¡é€€é¿æ—¶é—´å¢åŠ åˆ° %s ç§’ï¼Œé‡è¯•åŒä¸€ä»»åŠ¡", current_backoff)
                    # Loop continues, will sleep at start of next iteration
    
    logger.info("åå°ä»»åŠ¡å·²å®Œæˆ")


def fetch_intraday_data_v2(stock_codes, target_date_str, period='1'):
    """
    è·å–æŒ‡å®šè‚¡ç¥¨åˆ—è¡¨ + ä¸‰å¤§æŒ‡æ•° çš„åˆ†é’Ÿçº§æ•°æ® (å¹¶å‘ç‰ˆ)ã€‚
    v2: å¢åŠ ä¸Šè¯ã€æ·±è¯æŒ‡æ•°ï¼Œä¼˜åŒ–ç¼“å­˜ï¼ŒåŸå­åŒ–è°ƒç”¨ã€‚
    v3: å¼•å…¥å¤šçº¿ç¨‹å¹¶å‘åŠ é€Ÿ
    """
    results = [] 
    failures = [] 
    
    # å®šä¹‰éœ€è¦è·å–çš„æŒ‡æ•°
    indices_map = {
        "000300": "ğŸ“Š æ²ªæ·±300",
        "000001": "ğŸ“ˆ ä¸Šè¯æŒ‡æ•°",
        "399001": "ğŸ“‰ æ·±è¯æˆæŒ‡"
    }

    # ä»»åŠ¡åˆ—è¡¨
    tasks = []

    # 1. æäº¤æŒ‡æ•°ä»»åŠ¡
    for idx_code, idx_name in indices_map.items():
        tasks.append({
            'type': 'index',
            'code': idx_code,
            'name': idx_name,
            'to_val': 99999999999
        })

    # 2. æäº¤ä¸ªè‚¡ä»»åŠ¡
    for code, name, to_val in stock_codes:
        tasks.append({
            'type': 'stock',
            'code': code,
            'name': name,
            'to_val': to_val
        })
        
    stats = {'total': len(tasks), 'success': 0, 'failed': 0, 'cache': 0, 'network': 0}

    def _worker(task):
        is_index = (task['type'] == 'index')
        api_name = "index_zh_a_hist_min_em" if is_index else "stock_zh_a_hist_min_em"
        _, date_key = _normalize_date_str(target_date_str)
        cache_path = _min_cache_path(task['code'], date_key, period, is_index)
        cached_df = _read_min_cache(cache_path)
        if cached_df is not None and not cached_df.empty:
            item = {
                'code': task['code'],
                'name': task['name'],
                'data': cached_df,
                'turnover': task['to_val'],
                'is_index': is_index
            }
            return item, None, 'cache'
        try:
            data = fetch_cached_min_data(task['code'], target_date_str, is_index=is_index, period=period, raise_on_error=True)
            if data is not None and not data.empty:
                item = {
                    'code': task['code'],
                    'name': task['name'],
                    'data': data,
                    'turnover': task['to_val'],
                    'is_index': is_index
                }
                return item, None, 'network'
            err = {
                'code': task['code'],
                'name': task['name'],
                'date': target_date_str,
                'period': period,
                'api': api_name,
                'reason': '\u63a5\u53e3\u8fd4\u56de\u7a7a',
                'is_index': is_index,
                'source': 'network'
            }
            return None, err, 'network'
        except Exception as e:
            err = {
                'code': task['code'],
                'name': task['name'],
                'date': target_date_str,
                'period': period,
                'api': api_name,
                'reason': str(e),
                'is_index': is_index,
                'source': 'network'
            }
            return None, err, 'network'

    # å¹¶å‘æ‰§è¡Œ
    # çº¿ç¨‹æ•°ä¸å®œè¿‡å¤šï¼Œä»¥å…è§¦å‘åçˆ¬é™åˆ¶ï¼Œ10-20å·¦å³è¾ƒä¸ºå®‰å…¨
    if INTRADAY_WORKERS <= 1:
        logger.info("\u5206\u65f6\u62c9\u53d6\u6a21\u5f0f: \u4e32\u884c delay=%.2fs", INTRADAY_DELAY_SEC)
        for t in tasks:
            item, err, source = _worker(t)
            if item:
                results.append(item)
                stats['success'] += 1
            if err:
                failures.append(err)
                stats['failed'] += 1
            if source in stats:
                stats[source] += 1
            if source == 'network' and INTRADAY_DELAY_SEC > 0:
                time.sleep(INTRADAY_DELAY_SEC)
    else:
        logger.info("\u5206\u65f6\u62c9\u53d6\u6a21\u5f0f: \u5e76\u53d1 workers=%s", INTRADAY_WORKERS)
        # ????????????????
        ctx = get_script_run_ctx()
        def _worker_wrapper(t):
            if ctx:
                add_script_run_ctx(threading.current_thread(), ctx)
            return _worker(t)

        with concurrent.futures.ThreadPoolExecutor(max_workers=INTRADAY_WORKERS) as executor:
            future_to_task = {executor.submit(_worker_wrapper, t): t for t in tasks}
            
            for future in concurrent.futures.as_completed(future_to_task):
                item, err, source = future.result()
                if item:
                    results.append(item)
                    stats['success'] += 1
                if err:
                    failures.append(err)
                    stats['failed'] += 1
                if source in stats:
                    stats[source] += 1

    return results, failures, stats

# 2. UI å¸ƒå±€
# -----------------------------------------------------------------------------

st.title("Aè‚¡å†å²ç›˜é¢å›æ”¾ç³»ç»Ÿ (æ²ªæ·±300 Market Replay)")

st.markdown("""
> ğŸ•¹ï¸ **æ“ä½œæŒ‡å—**ï¼š
> 1. ç­‰å¾…æ•°æ®åˆå§‹åŒ–å®Œæˆï¼ˆåˆæ¬¡è¿è¡Œå¯èƒ½éœ€è¦ 2-3 åˆ†é’Ÿï¼‰ã€‚
> 2. æ‹–åŠ¨ä¸‹æ–¹æ»‘å—é€‰æ‹©å†å²æ—¥æœŸã€‚
> 3. è§‚å¯Ÿå½“æ—¥ç›˜é¢çš„èµ„é‡‘æµå‘ä¸çƒ­åº¦ã€‚
""")

# ä¾§è¾¹æ 
with st.sidebar:
    st.header("âš™ï¸ æ•°æ®ç®¡ç†")
    
    with st.expander("æ•°æ®åˆ·æ–°ä¸ç»´æŠ¤", expanded=True):
        st.write("å¦‚æœæ•°æ®æ˜¾ç¤ºä¸æ­£ç¡®ï¼Œè¯·å°è¯•ä»¥ä¸‹æ“ä½œï¼š")
        
        # 1. ????
        if st.button("ğŸŸ¢ åˆ·æ–°ä»Šæ—¥è¡Œæƒ… (ç›˜ä¸­)"):
            log_action("åˆ·æ–°ä»Šæ—¥è¡Œæƒ…(ç›˜ä¸­)")
            try:
                if os.path.exists(CACHE_FILE):
                    _df = pd.read_parquet(CACHE_FILE)
                    _today = datetime.now().date()
                    _df_new = _df[_df["æ—¥æœŸ"].dt.date < _today]
                    _df_new.to_parquet(CACHE_FILE)
                    st.toast("å·²æ¸…é™¤ä»Šæ—¥ç¼“å­˜ï¼Œæ­£åœ¨é‡æ–°æ‹‰å–å®æ—¶æ•°æ®...")
                st.cache_data.clear()
                st.rerun()
            except Exception as e:
                st.error(f"æ“ä½œå¤±è´¥: {e}")

        # 2. ??????????
        if st.button("ğŸ§¹ æ¸…ç©ºåˆ†æ—¶å›¾å†…å­˜ç¼“å­˜"):
            log_action("æ¸…ç©ºåˆ†æ—¶å›¾å†…å­˜ç¼“å­˜")
            st.cache_data.clear()
            st.toast("âœ… å†…å­˜ç¼“å­˜å·²æ¸…ç©ºï¼Œç£ç›˜ç¼“å­˜ä¿ç•™ã€‚")

        # 3. ????????
        if st.button("ğŸ”„ æ‰‹åŠ¨æ›´æ–°è‚¡ç¥¨åç§°"):
            codes_hint = st.session_state.get("last_top_codes", [])
            log_action("æ‰‹åŠ¨æ›´æ–°è‚¡ç¥¨åç§°", codes=len(codes_hint))
            name_map = _refresh_name_map_for_codes(codes_hint, force=True)
            if name_map:
                st.toast(f"âœ… å·²æ›´æ–°åç§°æ˜ å°„ï¼š{len(name_map)} æ¡")
            else:
                st.warning("æœªè·å–åˆ°æœ€æ–°åç§°æ˜ å°„ã€‚")

        # 4. ????????
        if st.button("ğŸ—‘ï¸ åˆ é™¤æœ¬åœ°åˆ†æ—¶ç¼“å­˜"):
            log_action("åˆ é™¤æœ¬åœ°åˆ†æ—¶ç¼“å­˜")
            clear_min_cache()
            st.cache_data.clear()
            st.toast("âœ… æœ¬åœ°åˆ†æ—¶ç¼“å­˜å·²åˆ é™¤ã€‚")

        # 5. ????
        if st.button("ğŸš¨ å½»åº•é‡ç½® (åˆ é™¤æ‰€æœ‰)"):
            log_action("å½»åº•é‡ç½®")
            if os.path.exists(CACHE_FILE):
                os.remove(CACHE_FILE)
                st.toast("å·²åˆ é™¤å†å²æ—¥çº¿ç¼“å­˜ã€‚")
            clear_min_cache()
            st.cache_data.clear()
            st.rerun()

    with st.expander("ğŸ’¾ æ•°æ®å¤‡ä»½ä¸æ¢å¤", expanded=False):
        st.caption("å¤‡ä»½ data ç›®å½•ï¼ˆå†å²æ—¥çº¿ + åˆ†æ—¶ç¼“å­˜ï¼‰")
        if st.button("ğŸ“¦ ç”Ÿæˆå¤‡ä»½", key="backup_build"):
            log_action("ç”Ÿæˆå¤‡ä»½")
            data_bytes = build_data_backup_zip()
            if data_bytes:
                st.session_state["backup_zip"] = data_bytes
                st.session_state["backup_name"] = f"capmap_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.zip"
                st.toast("âœ… å¤‡ä»½å·²ç”Ÿæˆ")
            else:
                st.warning("æ²¡æœ‰å¯å¤‡ä»½çš„æ•°æ®ã€‚")
        if "backup_zip" in st.session_state:
            download_clicked = st.download_button(
                "â¬‡ï¸ ä¸‹è½½å¤‡ä»½",
                data=st.session_state["backup_zip"],
                file_name=st.session_state.get("backup_name", "capmap_data_backup.zip"),
                mime="application/zip",
                key="backup_download",
            )
            if download_clicked:
                log_action("ä¸‹è½½å¤‡ä»½")
        uploaded = st.file_uploader("æ¢å¤å¤‡ä»½ï¼ˆ.zipï¼‰", type=["zip"], key="backup_upload")
        if uploaded is not None and st.button("â™»ï¸ æ¢å¤å¤‡ä»½", key="backup_restore"):
            log_action("æ¢å¤å¤‡ä»½", file=getattr(uploaded, "name", ""))
            try:
                restored = restore_data_backup(uploaded)
                st.cache_data.clear()
                log_action("æ¢å¤å¤‡ä»½å®Œæˆ", files=restored)
                st.toast(f"âœ… å·²æ¢å¤ {restored} ä¸ªæ–‡ä»¶")
                st.rerun()
            except Exception as e:
                st.error(f"æ¢å¤å¤±è´¥: {e}")
    st.info("æ•°æ®æºï¼šæ²ªæ·±300æˆåˆ†è‚¡ (AkShare)")
    st.caption("æ³¨ï¼šæ–¹å—å¤§å°ä½¿ç”¨'æˆäº¤é¢'ä»£æ›¿'å¸‚å€¼'ï¼Œ\nåæ˜ å½“æ—¥äº¤æ˜“çƒ­åº¦ã€‚")

    st.markdown("---")
    st.markdown("### ğŸ› ï¸ æ¿å—è¿‡æ»¤")
    filter_cyb = st.checkbox("å±è”½åˆ›ä¸šæ¿ (300å¼€å¤´)", value=False)
    filter_kcb = st.checkbox("å±è”½ç§‘åˆ›æ¿ (688å¼€å¤´)", value=False)
    filter_state = (filter_cyb, filter_kcb)
    if st.session_state.get("filter_state") != filter_state:
        st.session_state["filter_state"] = filter_state
        log_action("ç­›é€‰æ¡ä»¶å˜æ›´", cyb=filter_cyb, kcb=filter_kcb)
    
# åŠ è½½æ•°æ®
with st.spinner("æ­£åœ¨åˆå§‹åŒ–å†å²æ•°æ®ä»“åº“..."):
    origin_df = fetch_history_data()
    _start_auto_prefetch_if_needed(origin_df)

# --- åå°ä»»åŠ¡æ£€æµ‹ä¸æ§åˆ¶ ---
# æ£€æŸ¥æ˜¯å¦æœ‰åä¸º "PrefetchWorker" çš„åå°çº¿ç¨‹
bg_thread = None
for t in threading.enumerate():
    if t.name == "PrefetchWorker":
        bg_thread = t
        break

# æ›´æ–° Sidebar UI
with st.sidebar:
    st.markdown("---")
    
    # å¯¼èˆªæ 
    nav_option = st.radio("ğŸ“¡ åŠŸèƒ½å¯¼èˆª", ["âª å†å²ç›˜é¢å›æ”¾", "ğŸŒŠ èµ„é‡‘åç¦»åˆ†æ", "ğŸ—‚ï¸ æ•°æ®ç®¡ç†"], index=0)
    prev_nav = st.session_state.get("nav_option_prev")
    if prev_nav != nav_option:
        st.session_state["nav_option_prev"] = nav_option
        log_action("åŠŸèƒ½å¯¼èˆªåˆ‡æ¢", nav=nav_option)
    
    with st.expander("ğŸ“¥ åå°æ•°æ®é¢„å–", expanded=False):
        st.caption("åå°é™é»˜ä¸‹è½½æœ€è¿‘ N å¤©åˆ†æ—¶æ•°æ®")
        prefetch_days = st.number_input("é¢„å–å¤©æ•°", min_value=5, max_value=200, value=30, step=10)
        
        if bg_thread and bg_thread.is_alive():
            st.info(f"ğŸŸ¢ åå°ä»»åŠ¡è¿è¡Œä¸­...\nè¯·å…³æ³¨æ§åˆ¶å°(Console)æ—¥å¿—")
            # æ— æ³•é€šè¿‡ Button åœæ­¢çº¿ç¨‹ï¼Œé™¤éä½¿ç”¨ Eventã€‚æš‚ä¸å®ç°åœæ­¢ã€‚
        else:
            if st.button("ğŸš€ å¯åŠ¨åå°ä¸‹è½½"):
                log_action("å¯åŠ¨åå°é¢„å–", days=prefetch_days)
                if not origin_df.empty:
                    # è·å–æ—¥æœŸåˆ—è¡¨
                    all_dates = sorted(origin_df['æ—¥æœŸ'].dt.date.unique())
                    target_prefetch_dates = all_dates[-prefetch_days:]
                    
                    # å¯åŠ¨çº¿ç¨‹
                    t = threading.Thread(
                        target=background_prefetch_task,
                        args=(target_prefetch_dates, origin_df),
                        name="PrefetchWorker",
                        daemon=True
                    )
                    add_script_run_ctx(t)
                    t.start()
                    st.rerun()
                else:
                    st.error("å†å²æ•°æ®å°šæœªå°±ç»ª")

if not origin_df.empty:
    # --- å…¨å±€è¿‡æ»¤é€»è¾‘ ---
    df = origin_df.copy()
    if filter_cyb:
        df = df[~df['ä»£ç '].astype(str).str.startswith('300')]
    if filter_kcb:
        df = df[~df['ä»£ç '].astype(str).str.startswith('688')]

    if df.empty:
        st.warning("è¿‡æ»¤åæ²¡æœ‰å‰©ä½™è‚¡ç¥¨æ•°æ®ï¼Œè¯·å–æ¶ˆå‹¾é€‰è¿‡æ»¤é€‰é¡¹ã€‚")
        st.stop()

    # --- æ—¶é—´é€‰æ‹©å™¨é€»è¾‘ (Session State ç®¡ç†) ---
    available_dates = sorted(df['æ—¥æœŸ'].dt.date.unique())
    
    if nav_option == "âª å†å²ç›˜é¢å›æ”¾":
        if 'selected_date_idx' not in st.session_state:
            st.session_state.selected_date_idx = len(available_dates) - 1

        #ç¡®ä¿ç´¢å¼•ä¸è¶Šç•Œ
        if st.session_state.selected_date_idx >= len(available_dates):
            st.session_state.selected_date_idx = len(available_dates) - 1
        if st.session_state.selected_date_idx < 0:
            st.session_state.selected_date_idx = 0

        # å¸ƒå±€ï¼šå‰ä¸€å¤© | æ»‘å— | åä¸€å¤©
        st.markdown("### ğŸ“… é€‰æ‹©å›æ”¾æ—¥æœŸ")
        
        # æ¨¡å¼é€‰æ‹©
        mode_col1, mode_col2 = st.columns([1, 3])
        with mode_col1:
            playback_mode = st.radio("å›æ”¾æ¨¡å¼", ["å•æ—¥å¤ç›˜", "å¤šæ—¥èµ°åŠ¿æ‹¼æ¥"], horizontal=True)

        if playback_mode == "å•æ—¥å¤ç›˜":
            col_prev, col_slider, col_next = st.columns([1, 6, 1])
            
            with col_prev:
                st.write("") 
                st.write("")
                if st.button("â¬…ï¸ å‰ä¸€å¤©"):
                    if st.session_state.selected_date_idx > 0:
                        st.session_state.selected_date_idx -= 1
                        st.rerun()

            with col_next:
                st.write("")
                st.write("")
                if st.button("åä¸€å¤© â¡ï¸"):
                    if st.session_state.selected_date_idx < len(available_dates) - 1:
                        st.session_state.selected_date_idx += 1
                        st.rerun()

            with col_slider:
                # åŸ select_slider æ›¿æ¢ä¸º date_input ä»¥æ”¯æŒå¿«é€Ÿå¹´ä»½é€‰æ‹©
                current_date_val = available_dates[st.session_state.selected_date_idx]
                
                picked_date = st.date_input(
                    "æ—¥æœŸ",
                    value=current_date_val,
                    min_value=available_dates[0],
                    max_value=available_dates[-1],
                    label_visibility="collapsed"
                )
                
                # å¦‚æœæ—¥æœŸå‘ç”Ÿå˜åŒ–
                if picked_date != current_date_val:
                    if picked_date in available_dates:
                        st.session_state.selected_date_idx = available_dates.index(picked_date)
                    else:
                        # å¦‚æœé€‰ä¸­çš„æ˜¯éäº¤æ˜“æ—¥ï¼Œå¯»æ‰¾æœ€è¿‘çš„äº¤æ˜“æ—¥
                        closest_date = min(available_dates, key=lambda d: abs(d - picked_date))
                        st.session_state.selected_date_idx = available_dates.index(closest_date)
                        st.toast(f"ğŸ“… ä¼‘å¸‚æ—¥ï¼Œå·²è‡ªåŠ¨å®šä½åˆ°æœ€è¿‘äº¤æ˜“æ—¥: {closest_date}")
                    st.rerun()
            
            target_dates = [available_dates[st.session_state.selected_date_idx]] # ä½¿ç”¨ state ä¸­çš„æ—¥æœŸ
            selected_date = target_dates[0]
            display_date_str = selected_date.strftime("%Y-%m-%d")
            
        else: # å¤šæ—¥èµ°åŠ¿æ‹¼æ¥
            with mode_col2:
                date_range = st.date_input(
                    "é€‰æ‹©æ—¶é—´èŒƒå›´ (å»ºè®®ä¸è¶…è¿‡5å¤©ï¼Œå¦åˆ™åŠ è½½è¾ƒæ…¢)",
                    value=[available_dates[-5] if len(available_dates)>5 else available_dates[0], available_dates[-1]],
                    min_value=available_dates[0],
                    max_value=available_dates[-1]
                )
            
            if len(date_range) == 2:
                start_d, end_d = date_range
                # ç­›é€‰å‡ºèŒƒå›´å†…çš„äº¤æ˜“æ—¥
                target_dates = [d for d in available_dates if start_d <= d <= end_d]
                if not target_dates: # å¦‚æœé€‰å®šçš„èŒƒå›´å†…æ²¡æœ‰äº¤æ˜“æ—¥ (ä¾‹å¦‚å…¨é€‰äº†å‡æœŸ)
                    st.warning("âš ï¸ é€‰å®šèŒƒå›´å†…æ— äº¤æ˜“æ•°æ®ï¼Œå·²è‡ªåŠ¨é‡ç½®ä¸ºæœ€è¿‘äº¤æ˜“æ—¥")
                    target_dates = [available_dates[-1]]
                
                st.info(f"å·²é€‰æ‹© {len(target_dates)} ä¸ªäº¤æ˜“æ—¥è¿›è¡Œæ‹¼æ¥å±•ç¤º")
                selected_date = target_dates[-1] # ç”¨äºä¸‹æ–¹æ˜¾ç¤ºç»Ÿè®¡é¢æ¿çš„åŸºå‡† (å…¼å®¹æ—§ä»£ç å˜é‡å)
                display_date_str = f"{target_dates[0].strftime('%Y%m%d')} ~ {target_dates[-1].strftime('%Y%m%d')}"
            else:
                st.warning("è¯·é€‰æ‹©å®Œæ•´çš„å¼€å§‹å’Œç»“æŸæ—¥æœŸ")
                target_dates = [available_dates[-1]]
                selected_date = available_dates[-1]
                display_date_str = selected_date.strftime("%Y-%m-%d")

        # --- æ•°æ®åˆ‡ç‰‡ä¸ç»Ÿè®¡ (å…¼å®¹å•æ—¥ä¸å¤šæ—¥) ---
        if len(target_dates) == 1:
            # å•æ—¥é€»è¾‘
            daily_df = df[df['æ—¥æœŸ'].dt.date == selected_date].copy()
            if daily_df.empty:
                st.warning(f"{selected_date} å½“æ—¥æ— äº¤æ˜“æ•°æ®ã€‚")
                st.stop()
                
            median_chg = daily_df['æ¶¨è·Œå¹…'].median()
            total_turnover = daily_df['æˆäº¤é¢'].sum() / 1e8 
            top_gainer = daily_df.loc[daily_df['æ¶¨è·Œå¹…'].idxmax()]
            
            metric_label_date = "å½“å‰å›æ”¾æ—¥æœŸ"
            metric_label_chg = "æˆåˆ†è‚¡ä¸­ä½æ•°æ¶¨è·Œ"
            metric_label_to = "æˆåˆ†è‚¡æ€»æˆäº¤"
            
        else:
            # å¤šæ—¥é€»è¾‘ (è®¡ç®—ç´¯è®¡)
            # 1. ç­›é€‰å‡ºèŒƒå›´å†…æ‰€æœ‰æ•°æ®
            start_date_ts = pd.Timestamp(target_dates[0])
            end_date_ts = pd.Timestamp(target_dates[-1])
            
            period_df = df[(df['æ—¥æœŸ'] >= start_date_ts) & (df['æ—¥æœŸ'] <= end_date_ts)].copy()
            
            if period_df.empty:
                st.stop()
                
            # 2. è®¡ç®—åŒºé—´ç´¯è®¡æ¶¨è·Œå¹…
            # æ–¹æ³•: å¯¹æ¯ä¸ªä»£ç ï¼Œæ‰¾åˆ°é¦–å°¾ä»·æ ¼
            # æ³¨æ„: å¦‚æœåªç”¨ period_dfï¼Œé¦–æ—¥çš„æ•°æ®é‡Œçš„ 'æ”¶ç›˜' æ˜¯é¦–æ—¥çš„æ”¶ç›˜ä»·ã€‚
            # åŒºé—´æ¶¨å¹… = (End_Close - Start_Close) / Start_Close ? 
            # æˆ–è€…æ›´ç²¾ç¡®ï¼šStart_Close åº”è¯¥æ˜¯ Start_Date çš„ å‰ä¸€æ—¥æ”¶ç›˜ä»· (å³ Start_Open / (1+Start_Chg))?
            # ç®€åŒ–èµ·è§ï¼Œæˆ‘ä»¬ç”¨ (End_Date Close - Start_Date Open) / Start_Date Open
            # è¿™æ ·èƒ½åŒ…å« Start_Date å½“å¤©çš„æ¶¨è·Œ
            
            agg_stats = []
            
            # ä½¿ç”¨ groupby åŠ é€Ÿ
            grouped = period_df.groupby('ä»£ç ')
            
            for code, group in grouped:
                group = group.sort_values('æ—¥æœŸ')
                if group.empty: continue
                
                first_row = group.iloc[0]
                last_row = group.iloc[-1]
                
                # æ¨ç®—é¦–æ—¥å¼€ç›˜ä»· = æ”¶ç›˜ / (1 + chg/100)
                # è¿™ç§åæ¨å¦‚æœæ˜¯æ¶¨åœæ¿å¤æƒå¯èƒ½å¾®å°è¯¯å·®ï¼Œä½†å¤Ÿç”¨ã€‚
                # ä¹Ÿå¯ä»¥ç›´æ¥ç”¨ akshare ä¸‹è½½çš„ Openï¼Œä½†è¿™é‡Œåªæœ‰ Close/Chg
                # å‡è®¾ Chg æ˜¯ç²¾ç¡®çš„
                try:
                    start_open = first_row['æ”¶ç›˜'] / (1 + first_row['æ¶¨è·Œå¹…']/100)
                    end_close = last_row['æ”¶ç›˜']
                    
                    period_chg = (end_close - start_open) / start_open * 100
                    period_turnover = group['æˆäº¤é¢'].sum()
                    
                    agg_stats.append({
                        'ä»£ç ': code,
                        'åç§°': first_row['åç§°'], # å‡è®¾æ²¡æ”¹å
                        'åŒºé—´æ¶¨è·Œå¹…': period_chg,
                        'åŒºé—´æ€»æˆäº¤': period_turnover
                    })
                except:
                    pass
            
            agg_df = pd.DataFrame(agg_stats)
            
            if agg_df.empty:
                st.warning("åŒºé—´æ•°æ®è®¡ç®—å¼‚å¸¸")
                st.stop()
                
            median_chg = agg_df['åŒºé—´æ¶¨è·Œå¹…'].median()
            total_turnover = period_df['æˆäº¤é¢'].sum() / 1e8
            top_gainer = agg_df.loc[agg_df['åŒºé—´æ¶¨è·Œå¹…'].idxmax()]
            
            # ä¸ºäº†å…¼å®¹åç»­ daily_df çš„ä½¿ç”¨ (Treemap å’Œ é€‰è‚¡)
            # æˆ‘ä»¬éœ€è¦æ„é€ ä¸€ä¸ª "Proxy Daily DF"
            # è®©åé¢çš„é€‰è‚¡é€»è¾‘åŸºäº "åŒºé—´è¡¨ç°"
            daily_df = agg_df.rename(columns={'åŒºé—´æ¶¨è·Œå¹…': 'æ¶¨è·Œå¹…', 'åŒºé—´æ€»æˆäº¤': 'æˆäº¤é¢'}).copy()
            # è¡¥é½å…¶ä»–å­—æ®µ
            # æ”¶ç›˜ä»·ç”¨æœ€åä¸€å¤©çš„
            # daily_df è¿˜éœ€è¦ 'æ”¶ç›˜' ç”¨äºå±•ç¤º? Treemap hover éœ€è¦
            # æˆ‘ä»¬å¯ä»¥ join å›å»ï¼Œä½† Treemap hover ä¹Ÿå¯ä»¥åªå±•ç¤ºæ¶¨è·Œ
            daily_df['æ”¶ç›˜'] = 0 # Placeholder
            
            metric_label_date = "å½“å‰å›æ”¾åŒºé—´"
            metric_label_chg = "åŒºé—´æ¶¨è·Œå¹…ä¸­ä½æ•°"
            metric_label_to = "åŒºé—´æ€»æˆäº¤"

        # æ˜¾ç¤ºæŒ‡æ ‡è¡Œ
        col1, col2, col3, col4 = st.columns(4)
        col1.metric(metric_label_date, display_date_str)
        col2.metric(metric_label_chg, f"{median_chg:.2f}%", 
                    delta=f"{median_chg:.2f}%", delta_color="normal")
        col3.metric(metric_label_to, f"{total_turnover:.1f} äº¿")
        col4.metric("é¢†æ¶¨é¾™å¤´", f"{top_gainer['åç§°']} ({'æ¶¨è·Œå¹…' in top_gainer and top_gainer['æ¶¨è·Œå¹…'] or top_gainer.get('åŒºé—´æ¶¨è·Œå¹…'):.2f}%)")

        # --- æ–°å¢åŠŸèƒ½ï¼šåˆ†æ—¶èµ°åŠ¿å åŠ  ---
        st.markdown("---")
        st.subheader("ğŸ“ˆ æ ¸å¿ƒèµ„äº§åˆ†æ—¶èµ°åŠ¿å åŠ ")
        
        # æ¨¡å¼é€‰æ‹©
        col_mode, col_num = st.columns([3, 1])
        with col_mode:
            chart_mode = st.radio("é€‰è‚¡æ¨¡å¼", ["æˆäº¤é¢ Top (æ´»è·ƒåº¦)", "æŒ‡æ•°è´¡çŒ® Top (å½±å“å¤§ç›˜)"], horizontal=True)
        with col_num:
            # æ·»åŠ  key é¿å… Bad setIn index é”™è¯¯ï¼Œå¹¶å¼ºåˆ¶é‡ç½®çŠ¶æ€
            top_n = st.number_input("æ ‡çš„æ•°é‡", min_value=5, max_value=50, value=20, step=5, 
                                   help="æ²ª/æ·±å„å– N ä¸ªæ ‡çš„ï¼ˆå³æ€»æ•°ä¸º 2Nï¼‰", key="top_n_stocks_input")

        st.caption(f"æ³¨ï¼šè¿™é‡Œçš„æ’åæ˜¯åŸºäº **{selected_date}** å½“æ—¥çš„æ•°æ®è®¡ç®—çš„ã€‚å¦‚æœæ˜¯å¤šæ—¥æ¨¡å¼ï¼Œåˆ™å±•ç¤ºè¿™äº›è‚¡ç¥¨åœ¨è¿‡å»å‡ å¤©çš„èµ°åŠ¿ã€‚")
        st.caption("æ³¨ï¼šæŒ‡æ•°è´¡çŒ® = æ¶¨è·Œå¹… Ã— æƒé‡(è¿‘ä¼¼ä¸ºæˆäº¤é¢/å¸‚å€¼å æ¯”)ã€‚æ­¤æ¨¡å¼èƒ½çœ‹åˆ°æ˜¯è°åœ¨æ‹‰åŠ¨æˆ–ç ¸ç›˜ã€‚")

        dates_sig = ("", "", 0)
        if target_dates:
            dates_sig = (
                target_dates[0].strftime("%Y-%m-%d"),
                target_dates[-1].strftime("%Y-%m-%d"),
                len(target_dates),
            )
        intraday_sig = (playback_mode, chart_mode, int(top_n), dates_sig)
        if st.session_state.get("intraday_sig") != intraday_sig:
            st.session_state["intraday_sig"] = intraday_sig
            log_action("\u5206\u65f6\u9009\u9879\u53d8\u66f4", playback=playback_mode, chart=chart_mode, top_n=top_n, dates=dates_sig)
            st.session_state["show_intraday"] = False

        show_intraday = st.checkbox("åŠ è½½åˆ†æ—¶èµ°åŠ¿ (æœ¬åœ°ä¼˜å…ˆï¼Œæ— åˆ™ç½‘ç»œæ‹‰å–)", key="show_intraday")
        prev_show = st.session_state.get("show_intraday_prev", False)
        if show_intraday and not prev_show:
            log_action("\u52fe\u9009\u5206\u65f6\u52a0\u8f7d")
        st.session_state["show_intraday_prev"] = show_intraday
        
        if show_intraday:
            # ä½¿ç”¨ placeholder æ”¾ç½®è¿›åº¦æ¡ï¼Œé¿å…ç»„ä»¶é”€æ¯å¯¼è‡´çš„ç´¢å¼•é”™ä¹±
            progress_area = st.empty()
            
            # ç»Ÿä¸€é€‰è‚¡é€»è¾‘ï¼šæ— è®ºæ˜¯æˆäº¤é¢è¿˜æ˜¯æŒ‡æ•°è´¡çŒ®ï¼Œéƒ½æŒ‰æ²ªæ·±åˆ†åˆ«å– Top N
            if "æˆäº¤é¢" in chart_mode:
                sort_col = 'æˆäº¤é¢'
            else:
                daily_df['abs_impact'] = (daily_df['æ¶¨è·Œå¹…'] * daily_df['æˆäº¤é¢']).abs()
                sort_col = 'abs_impact'
                
            sh_pool = daily_df[daily_df['ä»£ç '].astype(str).str.startswith('6')].copy()
            sz_pool = daily_df[~daily_df['ä»£ç '].astype(str).str.startswith('6')].copy()
            
            sh_top = sh_pool.sort_values(sort_col, ascending=False).head(top_n)
            sz_top = sz_pool.sort_values(sort_col, ascending=False).head(top_n)
            
            top_stocks_df = pd.concat([sh_top, sz_top], ignore_index=True)
            top_codes = top_stocks_df['ä»£ç '].astype(str).tolist()
            st.session_state["last_top_codes"] = top_codes
            name_map = _refresh_name_map_for_codes(top_codes, force=False)
            if name_map:
                top_stocks_df['åç§°'] = top_stocks_df['ä»£ç '].astype(str).map(name_map).fillna(top_stocks_df['åç§°'])

            target_stocks_list = []
            for _, row in top_stocks_df.iterrows():
                target_stocks_list.append((row['ä»£ç '], row['åç§°'], row['æˆäº¤é¢'])) 
            
            all_intraday_data = [] 
            
            period_to_use = '1'
            
            if len(target_dates) > 5 and playback_mode == "å¤šæ—¥èµ°åŠ¿æ‹¼æ¥":
                if len(target_dates) > 30:
                    period_to_use = '15' # è¶…è¿‡30å¤©ä½¿ç”¨15åˆ†é’Ÿçº¿
                    st.info(f"â„¹ï¸ æ‚¨é€‰æ‹©äº† {len(target_dates)} å¤©ï¼šç³»ç»Ÿè‡ªåŠ¨åˆ‡æ¢è‡³ã€15åˆ†é’Ÿçº§ã€‘æ•°æ®ã€‚")
                else:
                    period_to_use = '5'
                    st.info(f"â„¹ï¸ æ‚¨é€‰æ‹©äº† {len(target_dates)} å¤©ï¼šç³»ç»Ÿè‡ªåŠ¨åˆ‡æ¢è‡³ã€5åˆ†é’Ÿçº§ã€‘æ•°æ®ã€‚")
            elif len(target_dates) > 10:
                 st.toast(f"âš ï¸ æ‚¨é€‰æ‹©äº† {len(target_dates)} å¤©çš„æ•°æ®ï¼ŒåŠ è½½å¯èƒ½è¾ƒæ…¢ï¼Œè¯·è€å¿ƒç­‰å¾…...")
            
            target_dates_to_fetch = target_dates
            total_steps = len(target_dates_to_fetch)
            logger.info("åˆ†æ—¶åŠ è½½å¼€å§‹: æ¨¡å¼=%s æ—¥æœŸæ•°=%s æ ‡çš„æ•°=%s å‘¨æœŸ=%s", chart_mode, len(target_dates_to_fetch), len(target_stocks_list), period_to_use)

            # æ”¹å›æ‰å¹³åŒ–ç»“æ„ï¼Œä¸å†ä½¿ç”¨ containerï¼Œå‡å°‘ DOM æ“ä½œå±‚çº§
            # å¹¶å‘çº¿ç¨‹ä¸­ç¼“å­˜çš„ show_spinner=False å·²ç»è®¾ç½®ï¼Œè¿™é‡Œåº”è¯¥å®‰å…¨äº†
            status_text = st.empty()
            fetch_progress = st.progress(0)
            for i, d_date in enumerate(target_dates_to_fetch):
                status_text.text(f"ğŸ”„ \u6b63\u5728\u83b7\u53d6: {d_date.strftime('%Y-%m-%d')} | \u5468\u671f={period_to_use}\u5206\u949f | \u76ee\u6807={len(target_stocks_list)}+\u6307\u65703 ({i+1}/{total_steps})")
                fetch_progress.progress((i + 1) / total_steps)
                
                d_str = d_date.strftime("%Y-%m-%d")
                day_results, day_failures, day_stats = fetch_intraday_data_v2(target_stocks_list, d_str, period=period_to_use)

                success = day_stats.get('success', 0)
                failed = day_stats.get('failed', 0)
                total_req = day_stats.get('total', 0)
                cache_hits = day_stats.get('cache', 0)
                network_calls = day_stats.get('network', 0)

                logger.info("\u5206\u65f6\u65e5\u6c47\u603b: date=%s total=%s success=%s failed=%s cache=%s network=%s", d_str, total_req, success, failed, cache_hits, network_calls)
                if day_failures:
                    for err in day_failures[:3]:
                        logger.warning("\u5206\u65f6\u5931\u8d25: code=%s name=%s api=%s reason=%s", err.get('code'), err.get('name'), err.get('api'), err.get('reason'))

                for res in day_results:
                    res["data"]["date_col"] = d_str
                    res["real_date"] = d_date
                
                all_intraday_data.extend(day_results)
            
            logger.info("åˆ†æ—¶åŠ è½½å®Œæˆ: ç»“æœæ•°=%s", len(all_intraday_data))
            # æ•°æ®æ‹‰å–å®Œæ¯•åï¼Œæ¸…é™¤è¿›åº¦ç»„ä»¶
            status_text.empty()
            fetch_progress.empty()
            # ç§»é™¤å¤–å±‚å ä½ç¬¦çš„æ¸…ç†ï¼Œå› ä¸ºå·²ç»ä¸å†ä½¿ç”¨
            progress_area.empty()
                
            if not all_intraday_data:
                st.warning("æœªèƒ½è·å–åˆ°åˆ†æ—¶æ•°æ®")
            else:
                    # --- æ•°æ®é‡ç»„ ---
                    # å°†åˆ†æ•£çš„æ•°æ®åˆå¹¶ä¸ºï¼š { 'code': { 'name':..., 'is_index':..., 'full_data': DataFrame } }
                    combined_series = {}
                    
                    for item in all_intraday_data:
                        code = item['code']
                        if code not in combined_series:
                            # æŸ¥æ‰¾è¯¥è‚¡ç¥¨åœ¨é€‰å®šæ—¥(æœ€åä¸€æ—¥)çš„æˆäº¤é¢ï¼Œç”¨äºå®šçº¿å®½
                            to_val = 0
                            if not item.get('is_index'):
                                matches = daily_df[daily_df['ä»£ç '] == code]
                                if not matches.empty:
                                    to_val = matches.iloc[0]['æˆäº¤é¢']
                            
                            combined_series[code] = {
                                'name': item['name'],
                                'code': code,
                                'is_index': item.get('is_index', False),
                                'turnover': to_val, # ä½¿ç”¨æœ€åä¸€å¤©çš„æˆäº¤é¢
                                'dfs': []
                            }
                        combined_series[code]['dfs'].append(item['data'])
                    
                    # åˆå¹¶ DataFrame å¹¶æ„å»ºè¿ç»­æ—¶é—´è½´
                    # ç­–ç•¥ï¼šä¸ä½¿ç”¨çœŸå®æ—¶é—´è½´ï¼Œè€Œæ˜¯ä½¿ç”¨ "äº¤æ˜“åˆ†é’Ÿåºåˆ—" (Trading Minute Index)
                    # æ¯å¤© 240 åˆ†é’Ÿã€‚Day 1: 0-239, Day 2: 240-479...
                    # éœ€è¦ç”Ÿæˆä¸€ä¸ª Xè½´ Label Map
                    
                    final_plot_data = [] # List of {idx, pct_chg, ...}
                    x_tick_vals = []
                    x_tick_text = []
                    
                    # ä¸ºæ¯ä¸€å¤©ç”Ÿæˆæ ‡å‡†æ—¶é—´åºåˆ— (é¿å…ç¼ºå¤±åˆ†é’Ÿå¯¼è‡´çš„é”™ä½)
                    # 09:30 - 11:30 (121 points usually inclusive? Aè‚¡åˆ†é’Ÿçº¿é€šå¸¸åŒ…å« 11:30 å’Œ 15:00)
                    # åˆ†é’Ÿçº¿é€šå¸¸æ˜¯ 09:31 - 11:30 (120 mins) 13:01 - 15:00 (120 mins). 
                    # AkShare è¿”å›çš„æ•°æ®æ—¶é—´å¦‚æœæ˜¯ 09:30 é€šå¸¸ä»£è¡¨å¼€ç›˜?
                    # è®©æˆ‘ä»¬é€šè¿‡è§‚å¯Ÿç¬¬ä¸€æ¡æ•°æ®æ¥å†³å®šé€»è¾‘ï¼Œé€šå¸¸ç›´æ¥ concat å³å¯
                    # ä¸ºäº†è§£å†³ GAPï¼Œæˆ‘ä»¬åœ¨ UI ç»˜å›¾å±‚å¼ºåˆ¶æŠŠ x æ˜ å°„ä¸º 0, 1, 2...
                    
                    # æå–çœŸæ­£è·å–åˆ°æ•°æ®çš„æ—¥æœŸåˆ—è¡¨ (å»é™¤èŠ‚å‡æ—¥/æ— æ•°æ®æ—¥)
                    valid_dates = set()
                    for item in all_intraday_data:
                         if 'real_date' in item:
                             valid_dates.add(item['real_date'].strftime("%Y-%m-%d"))
                    
                    days_list = sorted(list(valid_dates))
                    
                    if not days_list:
                        # å¦‚æœè¿‡æ»¤åå±…ç„¶æ²¡äº†ï¼ˆç†è®ºä¸Šå¤–å±‚ checked not emptyï¼‰ï¼Œå›é€€å…œåº•
                         days_list = sorted(list(set([x.strftime("%Y-%m-%d") for x in target_dates_to_fetch])))
                    
                    # æ„å»ºåŸºå‡†æ—¶é—´ç½‘æ ¼ (Template) - æ¯å¤© 240/241 ä¸ªç‚¹
                    # 09:30 - 11:30, 13:00 - 15:00
                    dummy_date = "2000-01-01"
                    morning_range = pd.date_range(f"{dummy_date} 09:30", f"{dummy_date} 11:30", freq="1min")
                    afternoon_range = pd.date_range(f"{dummy_date} 13:00", f"{dummy_date} 15:00", freq="1min")
                    daily_time_template = morning_range.union(afternoon_range) # Size approx 242
                    
                    # è®¡ç®—æ€»åç§»é‡
                    points_per_day = len(daily_time_template)
                    
                    for code, info in combined_series.items():
                        # åˆå¹¶ã€æ’åº
                        full_df = pd.concat(info['dfs']).sort_values(['date_col', 'time'])
                        
                        # é‡æ–°æ„é€  X è½´ (Int)
                        # ç®—æ³•ï¼šå¯¹äºæ¯ä¸€è¡Œï¼Œæ‰¾åˆ°å®ƒæ˜¯ ç¬¬å‡ å¤© çš„ ç¬¬å‡ åˆ†é’Ÿ
                        # x_int = day_index * points_per_day + minute_index_in_day
                        
                        full_df['time_str'] = full_df['time'].dt.strftime("%H:%M:%S")
                        
                        x_values = []
                        
                        for idx, row in full_df.iterrows():
                            d_str = row['date_col']
                            t_str = row['time_str'] # å®Œæ•´æ—¶é—´å¯¹è±¡
                            
                            # ç¡®å®šæ˜¯ç¬¬å‡ å¤©
                            day_idx = days_list.index(d_str) if d_str in days_list else 0
                            
                            # ç¡®å®šæ˜¯å½“å¤©çš„ç¬¬å‡ åˆ†é’Ÿ
                            # ç®€å•è½¬æ¢ï¼šå°æ—¶*60 + åˆ†é’Ÿ
                            t_obj = row['time'] # timestamp
                            mins_from_midnight = t_obj.hour * 60 + t_obj.minute
                            
                            # æŠŠä¸­åˆä¼‘å¸‚çš„æ—¶é—´å‹æ‰
                            # 9:30 (570) -> 11:30 (690). Length 120.
                            # 13:00 (780) -> 15:00 (900). 
                            
                            if mins_from_midnight <= 690: # Morning
                                offset = mins_from_midnight - 570 # 09:30 is 0
                            else: # Afternoon
                                offset = 120 + (mins_from_midnight - 780) # 13:00 starts at 120+
                                
                            final_x = day_idx * (240 + 20) + offset # åŠ 20ä¸ªå•ä½çš„é—´éš”è®©å¤©ä¸å¤©ä¹‹é—´æœ‰ç‚¹ç©ºéš™
                            x_values.append(final_x)
                            
                        full_df['x_int'] = x_values
                        
                        # è®¡ç®—ç´¯è®¡æ¶¨è·Œå¹… (å¦‚æœæ˜¯å¤šæ—¥ï¼Œéœ€è¦é“¾å¼è®¡ç®—ï¼Ÿ)
                        # ç®€å•æ–¹æ¡ˆï¼šæ¯å¤©é‡æ–°å½’é›¶ï¼Ÿè¿˜æ˜¯å¤šæ—¥è¿è´¯ï¼Ÿ
                        # ç”¨æˆ·è¯´ "æ‹¼èµ·æ¥å½¢æˆå®Œæ•´çš„å›¾"ï¼Œé€šå¸¸æ„å‘³ç€è¿è´¯è¶‹åŠ¿ã€‚
                        # ä»¥ç¬¬ä¸€å¤©å¼€ç›˜ä»·ä¸ºåŸºå‡†
                        base_price = full_df['close'].iloc[0]
                        # å¦‚æœä¸­é—´æœ‰æ–­ç‚¹ï¼Œç®€å•çš„ (close - base) / base å¯èƒ½å¤±çœŸï¼ˆå› ä¸ºæ˜¨æ”¶...ï¼‰
                        # å‡†ç¡®åšæ³•ï¼šç´¯ä¹˜æ¯å¤©çš„æ¶¨è·Œå¹…ã€‚
                        # ä½†è¿™é‡Œåªè¦å¤§æ¦‚è¶‹åŠ¿ã€‚å¦‚æœç”¨ (Px - P0) / P0ï¼Œé‚£è·¨æ—¥ç¼ºå£ä¼šä½“ç°ä¸ºç›´çº¿çš„è·³è·ƒã€‚
                        # è¿™ç¬¦åˆ "çœŸå®ä»·æ ¼èµ°åŠ¿"ã€‚
                        
                        full_df['cumulative_pct'] = (full_df['close'] - base_price) / base_price * 100
                        
                        info['plot_data'] = full_df
                    
                    # ç”Ÿæˆ X è½´æ ‡ç­¾
                    for i, d_str in enumerate(days_list):
                        base_x = i * (240 + 20)
                        day_label = d_str[5:] # MM-DD
                        
                        if len(days_list) > 1:
                            # å¤šæ—¥æ¨¡å¼ï¼šåªæ˜¾ç¤ºæ—¥æœŸåœ¨ä¸­é—´ æˆ–è€… å¼€å¤´
                            # ä¸ºäº†ç®€æ´ï¼Œåªåœ¨æ¯å¤©çš„ä¸­é—´æ˜¾ç¤ºä¸€ä¸ªæ—¥æœŸ
                            x_tick_vals.append(base_x + 120) 
                            x_tick_text.append(day_label)
                        else:
                            # å•æ—¥æ¨¡å¼ï¼šæ˜¾ç¤ºè¯¦ç»†æ—¶é—´ç‚¹
                            # 09:30
                            x_tick_vals.append(base_x)
                            x_tick_text.append(f"{day_label}\n09:30")
                            # 11:30
                            x_tick_vals.append(base_x + 120)
                            x_tick_text.append("11:30/13:00")
                            # 15:00
                            x_tick_vals.append(base_x + 240)
                            x_tick_text.append("15:00")

                    # åˆ†ç±»
                    idx_data = [v for k,v in combined_series.items() if v['is_index']]
                    stk_data = [v for k,v in combined_series.items() if not v['is_index']]
                    
                    sh_stocks = [v for v in stk_data if v['code'].startswith('6')]
                    sz_stocks = [v for v in stk_data if not v['code'].startswith('6')]
                    
                    sh_index = [v for v in idx_data if v['code'] in ['000001', '000300']]
                    sz_index = [v for v in idx_data if v['code'] in ['399001', '000300']]

                    # ç»˜å›¾å‡½æ•° v3
                    def plot_intraday_v3(stocks, indices, title_suffix):
                        fig = go.Figure()
                        
                        # ä¸ªè‚¡
                        if stocks:
                            max_to = max([s['turnover'] for s in stocks]) if stocks else 1
                            min_to = min([s['turnover'] for s in stocks]) if stocks else 0
                            
                            # ç”Ÿæˆ distinct colors
                            color_palette = px.colors.qualitative.Alphabet + px.colors.qualitative.Dark24
                            
                            for i, s in enumerate(stocks):
                                if max_to == min_to: width=2
                                else: width = 1 + 3*(s['turnover'] - min_to)/(max_to - min_to)
                                
                                df_p = s['plot_data']
                                last_val = df_p['cumulative_pct'].iloc[-1]
                                
                                # ä¹‹å‰çš„çº¢ç»¿é€»è¾‘: color = 'rgba(214, 39, 40, 0.4)' if last_val > 0 else 'rgba(44, 160, 44, 0.4)'
                                # ç°åœ¨æ”¹ä¸ºåŒºåˆ†é¢œè‰²
                                color = color_palette[i % len(color_palette)]
                                
                                fig.add_trace(go.Scatter(
                                    x=df_p['x_int'],
                                    y=df_p['cumulative_pct'],
                                    mode='lines',
                                    name=s['name'],
                                    # line=dict(width=width, color=color),
                                    # ä¸ªè‚¡çº¿å®½ä¸éœ€è¦å¤ªç²—ï¼Œé¢œè‰²è¦æ¸…æ™°
                                    line=dict(width=max(1.5, width), color=color),
                                    hovertemplate=f"<b>{s['name']}</b><br>æ¶¨è·Œ: %{{y:.2f}}%<br>æ—¶é—´: %{{customdata}}",
                                    customdata=df_p['date_col'] + ' ' + df_p['time_str']
                                ))
                                
                        # æŒ‡æ•°
                        idx_colors = {'000300': 'black', '000001': '#d62728', '399001': '#1f77b4'}
                        for idx in indices:
                            df_p = idx['plot_data']
                            c_code = idx.get('code', '000300')
                            
                            fig.add_trace(go.Scatter(
                                x=df_p['x_int'],
                                y=df_p['cumulative_pct'],
                                mode='lines',
                                name=idx['name'],
                                line=dict(width=3, color=idx_colors.get(c_code, 'black')),
                                hovertemplate=f"<b>{idx['name']}</b><br>æ¶¨è·Œ: %{{y:.2f}}%"
                            ))

                        # 3. åˆ†å‰²çº¿ (å¦‚æœæ˜¯å¤šæ—¥)
                        if len(days_list) > 1:
                            for i in range(1, len(days_list)):
                                # åœ¨æ¯ä¸€å¤©å¼€å§‹å‰ç”»ç«–çº¿
                                boundary = i * (240 + 20) - 10
                                fig.add_vline(x=boundary, line_width=1, line_dash="dash", line_color="gray")

                        fig.update_layout(
                            title=f"åˆ†æ—¶èµ°åŠ¿å åŠ  ({'å¤šæ—¥æ‹¼æ¥' if len(days_list)>1 else days_list[0]}) - {title_suffix}",
                            xaxis=dict(
                                tickmode='array',
                                tickvals=x_tick_vals,
                                ticktext=x_tick_text,
                                showgrid=True,
                                showspikes=True, # æ˜¾ç¤ºå‚ç›´è¾…åŠ©çº¿
                                spikemode='across',
                                spikesnap='cursor',
                                showline=True, 
                                linewidth=1, 
                                linecolor='black',
                                mirror=True
                            ),
                            yaxis=dict(
                                showspikes=True # Yè½´ä¹Ÿæ˜¾ç¤ºè¾…åŠ©çº¿ï¼Œæ–¹ä¾¿çœ‹ç‚¹ä½
                            ),
                            yaxis_title="ç´¯è®¡æ¶¨è·Œå¹… (%)",
                            hovermode="x unified", # å¼€å¯ç»Ÿä¸€ Hoverï¼Œæ˜¾ç¤ºè¯¥æ—¶é—´ç‚¹æ‰€æœ‰æ•°æ®
                            height=700, # ç¨å¾®è°ƒé«˜é«˜åº¦ä»¥å®¹çº³æ›´å¤š Hover ä¿¡æ¯
                            legend=dict(orientation="h", y=1.02, x=1, xanchor='right') # ä¿æŒå›¾ä¾‹çš„å¸ƒå±€
                        )
                        
                        # âš ï¸ å…³é”®ä¿®æ­£ï¼šç¡®ä¿ Hover Tooltip çš„æ’åºæŒ‰ç…§ Y è½´æ•°å€¼ (ä»é«˜åˆ°ä½)
                        # "closest" æ¨¡å¼é…åˆ "compare" å¯èƒ½æ— æ³•ç”Ÿæ•ˆï¼Œä½†åœ¨ "x unified" æ¨¡å¼ä¸‹ï¼Œ
                        # é»˜è®¤æ˜¯æŒ‰ç…§ Trace æ·»åŠ é¡ºåºæ’åºçš„ã€‚
                        # Plotly (JSå±‚) åœ¨ x unified ä¸‹æœ‰é»˜è®¤æ’åºé€»è¾‘ (é€šå¸¸æ˜¯ value descending)ï¼Œä½†åœ¨æŸäº›ç‰ˆæœ¬å¯èƒ½ä¸ç¨³å®šã€‚
                        # ä¸ºäº†å¢å¼ºæ’åºä½“éªŒï¼Œæˆ‘ä»¬å¯ä»¥å°è¯•è®¾ç½® layout.hoverlabel.namelength = -1
                        
                        fig.update_layout(hoverlabel=dict(namelength=-1))
                        
                        return fig

                    tab1, tab2 = st.tabs(["æ²ªå¸‚ (SH)", "æ·±å¸‚ (SZ)"])
                    
                    with tab1:
                        st.plotly_chart(plot_intraday_v3(sh_stocks, sh_index, f"æ²ªå¸‚ - {chart_mode}"), width="stretch")
                    with tab2:
                        st.plotly_chart(plot_intraday_v3(sz_stocks, sz_index, f"æ·±å¸‚ - {chart_mode}"), width="stretch")
        
        # --- å¯è§†åŒ– ---
        st.subheader(f"ğŸ“Š {selected_date.strftime('%Yå¹´%mæœˆ%dæ—¥')} å¸‚åœºå…¨æ™¯çƒ­åŠ›å›¾")
        
        # Aè‚¡ä¸“ç”¨è‰²è°±
        max_limit = 7
        min_limit = -7
        
        fig = px.treemap(
            daily_df,
            path=['åç§°'],
            values='æˆäº¤é¢', # ç”¨æˆäº¤é¢ä»£è¡¨çƒ­åº¦/æƒé‡ (å› ä¸ºå†å²å¸‚å€¼éš¾è·å–)
            color='æ¶¨è·Œå¹…',
            color_continuous_scale=['#00a65a', '#ffffff', '#dd4b39'], # ç»¿ -> ç™½ -> çº¢
            range_color=[min_limit, max_limit],
            hover_data={
                'åç§°': True,
                'ä»£ç ': True,
                'æ”¶ç›˜': True,
                'æ¶¨è·Œå¹…': ':.2f',
                'æˆäº¤é¢': True
            },
            height=650
        )
        
        # ä¼˜åŒ–æ˜¾ç¤º
        fig.update_traces(
            textinfo="label+value+percent entry",
            hovertemplate="<b>%{label}</b><br>æ”¶ç›˜ä»·: %{customdata[2]}<br>æ¶¨è·Œå¹…: %{color:.2f}%<br>æˆäº¤é¢: %{value:.2s}"
        )
        fig.update_layout(
            margin=dict(t=10, l=10, r=10, b=10),
            coloraxis_colorbar=dict(title="æ¶¨è·Œå¹…(%)")
        )
        
        st.plotly_chart(fig, width="stretch")
        
        # å¯é€‰ï¼šæ˜¾ç¤ºè¯¦ç»†æ•°æ®è¡¨
        with st.expander("æŸ¥çœ‹å½“æ—¥è¯¦ç»†æ•°æ®"):
            st.dataframe(
                daily_df[['ä»£ç ', 'åç§°', 'æ”¶ç›˜', 'æ¶¨è·Œå¹…', 'æˆäº¤é¢']].style.format({
                    'æ”¶ç›˜': '{:.2f}',
                    'æ¶¨è·Œå¹…': '{:.2f}%',
                    'æˆäº¤é¢': '{:,.0f}'
                }),
                hide_index=True
            )

    elif nav_option == "ğŸ—‚ï¸ æ•°æ®ç®¡ç†":
        st.subheader("ğŸ“¦ æœ¬åœ°åˆ†æ—¶æ•°æ®ç®¡ç†")
        st.caption("è¯´æ˜ï¼šåªæ˜¾ç¤º 1åˆ†é’Ÿåˆ†æ—¶ç¼“å­˜ï¼Œå¯æ‰‹åŠ¨è¡¥é½ç¼ºå¤±ã€‚")
        if origin_df is None or origin_df.empty:
            st.warning("æš‚æ— å†å²æ•°æ®ï¼Œè¯·å…ˆåˆå§‹åŒ–ã€‚")
        else:
            trading_dates = sorted(origin_df['æ—¥æœŸ'].dt.date.unique())
            date_strs = [d.strftime("%Y-%m-%d") for d in trading_dates]
            selected_date_str = st.selectbox("é€‰æ‹©äº¤æ˜“æ—¥æœŸ", date_strs, index=len(date_strs) - 1)
            selected_date = datetime.strptime(selected_date_str, "%Y-%m-%d").date()
            date_key = selected_date_str.replace("-", "")
            codes, name_map = _get_daily_codes(origin_df, selected_date)
            st.write(f"å½“æ—¥æˆåˆ†è‚¡æ•°é‡: {len(codes)}")

            cached_codes = _get_cached_codes_for_date(date_key, codes, period='1', is_index=False)
            missing_codes = [c for c in codes if c not in cached_codes]
            st.write(f"è‚¡ç¥¨åˆ†æ—¶ç¼“å­˜: {len(cached_codes)}/{len(codes)}")
            if missing_codes:
                missing_lines = [f"{c} {name_map.get(c, c)}" for c in missing_codes]
                st.text_area("ç¼ºå¤±è‚¡ç¥¨", "\n".join(missing_lines), height=160)
            else:
                st.success("è‚¡ç¥¨åˆ†æ—¶å·²å®Œæ•´ã€‚")

            index_codes = ["000300", "000001", "399001"]
            cached_idx = _get_cached_codes_for_date(date_key, index_codes, period='1', is_index=True)
            missing_idx = [c for c in index_codes if c not in cached_idx]
            st.write(f"æŒ‡æ•°åˆ†æ—¶ç¼“å­˜: {len(cached_idx)}/{len(index_codes)}")
            if missing_idx:
                st.warning("ç¼ºå¤±æŒ‡æ•°: " + ", ".join(missing_idx))
            else:
                st.success("æŒ‡æ•°åˆ†æ—¶å·²å®Œæ•´ã€‚")

            st.markdown("### æ‰‹åŠ¨è¡¥é½")
            include_indices = st.checkbox("åŒ…å«æŒ‡æ•°", value=True)
            default_codes = missing_codes[:20]
            select_codes = st.multiselect("é€‰æ‹©è¦è¡¥é½çš„è‚¡ç¥¨(ç¼ºå¤±)", options=missing_codes, default=default_codes)
            if st.button("å¯åŠ¨åå°è¡¥é½"):
                started = _start_manual_prefetch(selected_date_str, select_codes, name_map, include_indices=include_indices)
                if started:
                    st.info("å·²å¯åŠ¨åå°è¡¥é½ï¼Œè¯·æŸ¥çœ‹ logs/app.logã€‚")
                else:
                    st.warning("æ²¡æœ‰å¯è¡¥é½çš„ä»»åŠ¡ã€‚")

            with st.expander("æœ¬åœ°åˆ†æ—¶ç¼“å­˜æ—¥æœŸ"):
                cached_dates = _scan_cached_dates(period='1', is_index=False)
                if cached_dates:
                    readable = [d[:4] + '-' + d[4:6] + '-' + d[6:] for d in cached_dates]
                    st.text_area("ç¼“å­˜æ—¥æœŸ", "\n".join(readable), height=120)
                else:
                    st.write("æš‚æ— æœ¬åœ°åˆ†æ—¶ç¼“å­˜ã€‚")

    elif nav_option == "ğŸŒŠ èµ„é‡‘åç¦»åˆ†æ":
        st.subheader("ğŸŒŠ èµ„é‡‘åç¦»åº¦åˆ†æ (Alpha Divergence)")
        st.info("ğŸ’¡ **é€»è¾‘è¯´æ˜**ï¼šè®¡ç®—é€‰å®šå‘¨æœŸå†…æ¯åªè‚¡ç¥¨ç›¸å¯¹äºã€å¸‚åœºä¸­ä½æ•°ã€‘çš„è¶…é¢æ¶¨è·Œå¹…ï¼ˆåç¦»åº¦ï¼‰ã€‚\n\nå¦‚æœæŸåªè‚¡ç¥¨ **æˆäº¤é¢å·¨å¤§** ä¸” **å‘ä¸‹åç¦»æå¤§**ï¼Œé€šå¸¸æ„å‘³ç€ä¸»åŠ›èµ„é‡‘åœ¨å¤§ä¸¾å‡ºè´§ï¼›åä¹‹åˆ™æ˜¯ä¸»åŠ›æŠ¢ç­¹ã€‚")

        # 1. å‘¨æœŸé€‰æ‹© (Reuse simplified logic)
        available_dates = sorted(df['æ—¥æœŸ'].dt.date.unique())
        col_d1, col_d2 = st.columns(2)
        with col_d1:
            date_range_div = st.date_input(
                "åˆ†æå‘¨æœŸ",
                value=[available_dates[-5] if len(available_dates)>5 else available_dates[0], available_dates[-1]],
                min_value=available_dates[0],
                max_value=available_dates[-1],
                key="divergence_date_input"
            )
        
        target_dates_div = []
        if len(date_range_div) == 2:
            s_d, e_d = date_range_div
            target_dates_div = [d for d in available_dates if s_d <= d <= e_d]
        
        if not target_dates_div:
            st.warning("è¯·é€‰æ‹©æœ‰æ•ˆçš„æ—¶é—´èŒƒå›´")
            st.stop()
            
        st.caption(f"å·²é€‰å– {target_dates_div[0]} è‡³ {target_dates_div[-1]}ï¼Œå…± {len(target_dates_div)} ä¸ªäº¤æ˜“æ—¥ã€‚")
        
        # 2. è®¡ç®—åŒºé—´æ•°æ®
        start_date_ts = pd.Timestamp(target_dates_div[0])
        end_date_ts = pd.Timestamp(target_dates_div[-1])
        
        div_period_df = df[(df['æ—¥æœŸ'] >= start_date_ts) & (df['æ—¥æœŸ'] <= end_date_ts)].copy()
        
        # èšåˆ
        div_stats = []
        grouped = div_period_df.groupby('ä»£ç ')
        
        for code, group in grouped:
            group = group.sort_values('æ—¥æœŸ')
            if group.empty: continue
            
            first_row = group.iloc[0]
            last_row = group.iloc[-1]
            
            try:
                # ä¼°ç®—åŒºé—´æ¶¨å¹…
                s_open = first_row['æ”¶ç›˜'] / (1 + first_row['æ¶¨è·Œå¹…']/100)
                e_close = last_row['æ”¶ç›˜']
                cum_pct = (e_close - s_open) / s_open * 100
                total_to = group['æˆäº¤é¢'].sum()
                
                div_stats.append({
                    'ä»£ç ': code,
                    'åç§°': first_row['åç§°'],
                    'åŒºé—´æ¶¨è·Œå¹…': cum_pct,
                    'åŒºé—´æ€»æˆäº¤': total_to
                })
            except:
                pass
                
        div_df = pd.DataFrame(div_stats)
        if div_df.empty:
            st.stop()
            
        # 3. è®¡ç®—åç¦»åº¦ (Deviation)
        market_median_chg = div_df['åŒºé—´æ¶¨è·Œå¹…'].median()
        div_df['åç¦»åº¦'] = div_df['åŒºé—´æ¶¨è·Œå¹…'] - market_median_chg
        
        # è¾…åŠ©åˆ—
        div_df['æˆäº¤é¢(äº¿)'] = div_df['åŒºé—´æ€»æˆäº¤'] / 1e8
        
        col_m1, col_m2 = st.columns(2)
        col_m1.metric("åŸºå‡†(ä¸­ä½æ•°)æ¶¨è·Œå¹…", f"{market_median_chg:.2f}%")
        col_m2.metric("åˆ†ææ ·æœ¬æ•°", f"{len(div_df)} åª")
        
        st.divider()

        # 4. å¯è§†åŒ– - æ•£ç‚¹å›¾
        # X: æˆäº¤é¢(Log), Y: åç¦»åº¦, Color: åç¦»åº¦
        fig_scatter = px.scatter(
            div_df,
            x='æˆäº¤é¢(äº¿)',
            y='åç¦»åº¦',
            color='åç¦»åº¦',
            text='åç§°', # æ˜¾ç¤ºåå­—
            color_continuous_scale=['#00a65a', '#ffffff', '#dd4b39'],
            range_color=[-20, 20], # é™åˆ¶é¢œè‰²èŒƒå›´é¿å…æå€¼
            log_x=True,
            hover_data=['ä»£ç ', 'åŒºé—´æ¶¨è·Œå¹…'],
            title=f"èµ„é‡‘åç¦»åº¦åˆ†å¸ƒå›¾ (Xè½´ä¸ºæˆäº¤é¢å¯¹æ•°)"
        )
        fig_scatter.update_traces(textposition='top center')
        fig_scatter.update_layout(height=600)
        st.plotly_chart(fig_scatter, width="stretch")
        
        # 5. æ¦œå•
        col_list1, col_list2 = st.columns(2)
        
        with col_list1:
            st.subheader("ğŸ”¥ èµ„é‡‘æŠ±å›¢ (æ”¾é‡å‘ä¸Šåç¦»)")
            # é€»è¾‘ï¼šæˆäº¤é¢å¤§ & åç¦»åº¦ > 0
            # æ’åºï¼šç»¼åˆåˆ† = æˆäº¤é¢ * åç¦»åº¦ (ä»…å‚è€ƒ) æˆ–è€…æŒ‰æˆäº¤é¢é™åºçœ‹è°åœ¨æ¶¨
            # ç”¨æˆ·éœ€æ±‚ï¼šæ‰¾å‡ºå‘ä¸Šåç¦»çš„ã€‚é€šå¸¸æƒ³çœ‹â€œå¤§èµ„é‡‘ä¹°è°â€ã€‚æ‰€ä»¥æŒ‰æˆäº¤é¢é™åºï¼Œä¸”åç¦»åº¦>0
            
            buy_df = div_df[div_df['åç¦»åº¦'] > 0].sort_values('åŒºé—´æ€»æˆäº¤', ascending=False).head(20)
            st.dataframe(
                buy_df[['ä»£ç ', 'åç§°', 'åç¦»åº¦', 'æˆäº¤é¢(äº¿)', 'åŒºé—´æ¶¨è·Œå¹…']].style.format({
                    'åç¦»åº¦': '+{:.2f}%',
                    'æˆäº¤é¢(äº¿)': '{:.1f}',
                    'åŒºé—´æ¶¨è·Œå¹…': '{:.2f}%'
                }),
                hide_index=True
            )
            
        with col_list2:
            st.subheader("ğŸ“‰ èµ„é‡‘å‡ºé€ƒ (æ”¾é‡å‘ä¸‹åç¦»)")
            # é€»è¾‘ï¼šæˆäº¤é¢å¤§ & åç¦»åº¦ < 0
            sell_df = div_df[div_df['åç¦»åº¦'] < 0].sort_values('åŒºé—´æ€»æˆäº¤', ascending=False).head(20)
            
            st.dataframe(
                sell_df[['ä»£ç ', 'åç§°', 'åç¦»åº¦', 'æˆäº¤é¢(äº¿)', 'åŒºé—´æ¶¨è·Œå¹…']].style.format({
                    'åç¦»åº¦': '{:.2f}%',
                    'æˆäº¤é¢(äº¿)': '{:.1f}',
                    'åŒºé—´æ¶¨è·Œå¹…': '{:.2f}%'
                }),
                hide_index=True
            )

else:
    st.error("æ•°æ®åŠ è½½å¤±è´¥ï¼Œè¯·åˆ·æ–°é‡è¯•ã€‚")
